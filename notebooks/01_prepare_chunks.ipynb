{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafc8e61",
   "metadata": {},
   "source": [
    "# Prepare Chunks (Syllabus, Sluides and Labs)\n",
    "\n",
    "The **goal of this notebook** is to turn all of the course files into small chunks of text that we can search later with embeddings/ FAISS. \n",
    "\n",
    "**Inputs** *(from `data/raw/`)\n",
    "- `syllabus/`: syllabus pdf \n",
    "- `slides/`  : lecture slide PDFs (convert PPTX : PDF if needed)\n",
    "- `labs/`    : Jupyter notebooks (`.ipynb`) organized by Phase/Week/etc.\n",
    "\n",
    "**What this notebook does**\n",
    "1. Finds all PDFs (`syllabus/`, `slides/`) and all `.ipynb` files (`labs/`) recursively.\n",
    "2. Extracts text:\n",
    "   - PDFs → per-page text\n",
    "   - Notebooks → Markdown + Code cells (kept as text)\n",
    "3. Splits long text into ~900-char chunks (600 for code cells).\n",
    "4. Writes a single table with chunk metadata.\n",
    "\n",
    "**Outputs (written to `data/processed/`)**\n",
    "- `chunk_meta.parquet` — canonical table of chunks + metadata\n",
    "- `chunks.csv` — same as parquet, human-readable\n",
    "\n",
    "**Schema (columns)**\n",
    "- `chunk_id`  — stable id like `<doc>#p<page>#c<chunk>` or `<nb>#cell<idx>#c<chunk>`\n",
    "- `doc_id`    — source document/notebook stem\n",
    "- `source_type` — `syllabus | slides | lab`\n",
    "- `page` / `cell_index` — where the chunk came from\n",
    "- `topic_hint` — (blank for now) we can fill later\n",
    "- `text` — the chunk text\n",
    "- `citation_url` — file:// link with page anchor when available\n",
    "\n",
    "**Run checklist**\n",
    "- [ ] Files are local \n",
    "- [ ] Slide files are pdf, not pptx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 9.7/18.7 MB 67.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 47.4 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyMuPDF\n",
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\julmo\\miniconda3\\envs\\ds\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\julmo\\miniconda3\\envs\\ds\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20250506->pdfplumber)\n",
      "  Downloading cryptography-45.0.6-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\julmo\\miniconda3\\envs\\ds\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\julmo\\miniconda3\\envs\\ds\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 38.3 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 56.0 MB/s eta 0:00:00\n",
      "Downloading cryptography-45.0.6-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.4/3.4 MB 66.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed cryptography-45.0.6 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Library \n",
    "from pathlib import Path # For handling file paths\n",
    "import re # This is for regular expressions is used for pattern matching in strings used for text cleaning,processing\n",
    "import fitz # For reading PDF files\n",
    "import pdfplumber # For reading PDF files\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655123f3",
   "metadata": {},
   "source": [
    "## Find all the necessary data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ba534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides dir: C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\slides | exists: True\n",
      "Syllabus : C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\syllabus | exists: True\n",
      "Labs     : C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\labs | exists: True\n"
     ]
    }
   ],
   "source": [
    "# Access raw data folders\n",
    "raw = Path(\"../data/raw\").resolve()\n",
    "slides_folder = raw/\"slides\"\n",
    "syllabus_folder = raw/\"syllabus\"\n",
    "labs_folder = raw/\"labs\"\n",
    "\n",
    "print(\"Slides dir:\", slides_folder,   \"| exists:\", slides_folder.exists())\n",
    "print(\"Syllabus :\", syllabus_folder, \"| exists:\", syllabus_folder.exists())\n",
    "print(\"Labs     :\", labs_folder,     \"| exists:\", labs_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0844c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 slide PDFs\n",
      "Found 1 syllabus PDFs\n",
      "Found 39 lab notebooks\n"
     ]
    }
   ],
   "source": [
    "# Make a list of the files\n",
    "slide_files = []\n",
    "for file in slides_folder.rglob(\"*.pdf\"): # look for all pdf files\n",
    "    slide_files.append(file) # add them to the slide_files list\n",
    "\n",
    "syllabus_files = []\n",
    "for file in syllabus_folder.rglob(\"*.pdf\"): # look for all pdf files\n",
    "    syllabus_files.append(file) # add them to the syllabus_files list\n",
    "    \n",
    "lab_files = []\n",
    "for file in labs_folder.rglob(\"*.ipynb\"): # look for all ipynb files\n",
    "    if \"checkpoint\" not in str(file): # ignore autosave checkpoints Jupyter creates\n",
    "        lab_files.append(file) # add the rest to the lab_files list\n",
    "\n",
    "# Show the files found\n",
    "print(\"Found\", len(slide_files), \"slide PDFs\")\n",
    "print(\"Found\", len(syllabus_files), \"syllabus PDFs\")\n",
    "print(\"Found\", len(lab_files), \"lab notebooks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9d99e",
   "metadata": {},
   "source": [
    "# Extract Text and Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c55be1",
   "metadata": {},
   "source": [
    "### Chunking logic\n",
    "\n",
    "Some slide pages are very short (just a few bullets), while others may contain lots of text.  \n",
    "To make retrieval effective, we use a simple rule:\n",
    "\n",
    "- If a page is short (≤700 characters) → keep the page as one chunk.  \n",
    "- If a page is long (>700 characters) → split into multiple 700-character chunks.  \n",
    "\n",
    "This way:\n",
    "- Sparse slides are not split unnecessarily.\n",
    "- Dense slides are broken into smaller, more searchable pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e96a9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text, max_characters=700):\n",
    "    \"\"\"If text is short, keep it as is. If it's long, split into 700 char chunks.\"\"\"\n",
    "    if len(text) <= max_characters:\n",
    "        return [text] # return a list with the original text if it's short enough\n",
    "    else:\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), max_characters):\n",
    "            chunk = text[i:i + max_characters]\n",
    "            chunks.append(chunk)  \n",
    "        return chunks      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42809343",
   "metadata": {},
   "source": [
    "**Slides**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7361fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 40 chunks from 39 slide PDFs\n",
      "Saved to data/processed/slides_chunks.csv and slides_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# Extract from slides\n",
    "# Each slide pdf is read and loop through its pages. For each page, the text is extracted and stored in a list of dictionaries with metadata (source, file name, page number, and text content).\n",
    "# Each row  = one slide page\n",
    "# Each column = source, file, page, text\n",
    "\n",
    "\n",
    "# Hold the extracted text chunk \n",
    "slides_rows = []\n",
    "\n",
    "# Loop through is slide pdf \n",
    "for file in slide_files:\n",
    "    pdf = fitz.open(file.as_posix()) # open the PDF file, convert Path to string with as_posix()\n",
    "    # Iterate over each page and extract text\n",
    "    for page_num in range(len(pdf)): # iterate over each page\n",
    "        page_text = pdf[page_num].get_text(\"text\").strip() # extract text and add a newline for separation\n",
    "        if not page_text: # skip empty pages\n",
    "            continue\n",
    "\n",
    "    # Apply chunking\n",
    "    chunks = create_chunks(page_text, max_characters=700)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        slides_rows.append({\"source\": \"slide\", \n",
    "                            \"file\": file.name,\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"chunk\": i + 1,\n",
    "                            \"text\": chunk\n",
    "        }) # store the extracted text with metadata\n",
    "\n",
    "print(f\"Extracted {len(slides_rows)} chunks from {len(slide_files)} slide PDFs\")\n",
    "            \n",
    "# Convert to DataFrame and show sample\n",
    "slides_df = pd.DataFrame(slides_rows)\n",
    "slides_df.head()\n",
    "\n",
    "# Save to data/processed\n",
    "slides_df.to_csv(\"../data/processed/slides_chunks.csv\", index=False)\n",
    "slides_df.to_parquet(\"../data/processed/slides_chunks.parquet\", index=False)\n",
    "\n",
    "print (\"Saved to data/processed/slides_chunks.csv and slides_chunks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d27e362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     40.000000\n",
      "mean      60.675000\n",
      "std       23.432814\n",
      "min       15.000000\n",
      "25%       43.750000\n",
      "50%       62.000000\n",
      "75%       77.500000\n",
      "max      104.000000\n",
      "Name: page, dtype: float64\n",
      "count     40.000000\n",
      "mean     213.975000\n",
      "std      121.045126\n",
      "min        0.000000\n",
      "25%      162.500000\n",
      "50%      213.500000\n",
      "75%      238.250000\n",
      "max      700.000000\n",
      "Name: text, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slide</td>\n",
       "      <td>(Re)-Introduction to Data Science &amp; Control Fl...</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slide</td>\n",
       "      <td>AB Testing.pdf</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>Thursday\\nOn Thursday we will be meeting for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced Abstraction.pptx.pdf</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday\\nTomorrow will entail:\\n●\\nFurther exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced Control Flow.pptx.pdf</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced Data Processing.pptx.pdf</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday\\nWednesday will entail:\\n●\\nA review...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               file  page  chunk  \\\n",
       "0  slide  (Re)-Introduction to Data Science & Control Fl...   102      1   \n",
       "1  slide                                     AB Testing.pdf    49      1   \n",
       "2  slide                      Advanced Abstraction.pptx.pdf    66      1   \n",
       "3  slide                     Advanced Control Flow.pptx.pdf    93      1   \n",
       "4  slide                  Advanced Data Processing.pptx.pdf    61      1   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1  Thursday\\nOn Thursday we will be meeting for o...  \n",
       "2  Tuesday\\nTomorrow will entail:\\n●\\nFurther exp...  \n",
       "3                                                     \n",
       "4  Wednesday\\nWednesday will entail:\\n●\\nA review...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(slides_rows)\n",
    "print(df['page'].describe())      # sanity check page numbers\n",
    "print(df['text'].str.len().describe())  # avg text length\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11cb5fb",
   "metadata": {},
   "source": [
    "**Labs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84e42387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty file: C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\labs\\phase2\\week09\\transformers.ipynb\n",
      "Extracted 1410 chunks from labs\n",
      "Saved labs to ../data/processed/labs_chunks.csv and labs_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Extract text from code and markdown cells in lab notebooks\n",
    "\n",
    "labs_rows = [] # hold the extracted text chunks\n",
    "\n",
    "for file in lab_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f: # r allows reading the file\n",
    "            content = f.read() # read the entire file content\n",
    "            if not content.strip(): # skip empty files\n",
    "                print(f\"Skipping empty file: {file}\")\n",
    "                continue\n",
    "            notebook = json.loads(content)  # load the notebook as a dict\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    cells = notebook.get('cells', [])  # access the cells section\n",
    "    for cell in cells:  # loop through each cell\n",
    "        cell_type = cell.get('cell_type')  # get the type of cell (code or markdown)\n",
    "        if cell_type in ['code', 'markdown']:  # only want code and markdown cells\n",
    "            source = ''.join(cell.get('source', [])).strip()  # join the list of strings into one string and strip whitespace\n",
    "            if not source:  # skip empty cells\n",
    "                continue\n",
    "\n",
    "            # Apply chunking\n",
    "            chunks = create_chunks(source, max_characters=700)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                labs_rows.append({\n",
    "                    \"source\": \"lab\",\n",
    "                    \"file\": file.name,\n",
    "                    \"cell_type\": cell_type,\n",
    "                    \"chunk\": i + 1,\n",
    "                    \"text\": chunk\n",
    "                })  # store the extracted text with metadata\n",
    "\n",
    "print(\"Extracted\", len(labs_rows), \"chunks from labs\")\n",
    "\n",
    "# Convert to DataFrame and show sample\n",
    "labs_df = pd.DataFrame(labs_rows)\n",
    "labs_df.to_csv(\"../data/processed/labs_chunks.csv\", index=False)\n",
    "labs_df.to_parquet(\"../data/processed/labs_chunks.parquet\", index=False)\n",
    "\n",
    "labs_df.head()\n",
    "print(\"Saved labs to ../data/processed/labs_chunks.csv and labs_chunks.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6465d24",
   "metadata": {},
   "source": [
    "**Syllabus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96121e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 tables in total\n"
     ]
    }
   ],
   "source": [
    "# path to syllabus\n",
    "pdf_path = Path(\"../data/raw/syllabus/IF Data Science 2025 Syllabus.pdf\")\n",
    "\n",
    "with pdfplumber.open(pdf_path.as_posix()) as pdf:  # open the PDF file, convert Path to string with as_posix()\n",
    "    tables = []\n",
    "    for page in pdf.pages:  # loop through each page\n",
    "        page_tables = page.extract_tables()  # extract tables from the page\n",
    "        tables.extend(page_tables)\n",
    "    print(\"Found\", len(tables), \"tables in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c368ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found potential headers on page 1: ['', '']\n",
      "Found potential headers on page 2: ['', '', '', '']\n",
      "Found potential headers on page 3: ['', '', '']\n",
      "Found potential headers on page 4: ['', '', '', '']\n",
      "Found potential headers on page 5: ['', '', '']\n",
      "Found potential headers on page 6: ['', '', '', '']\n",
      "Found potential headers on page 7: ['', '', '', '', '']\n",
      "Found potential headers on page 8: ['', '', '']\n",
      "Found potential headers on page 9: ['', '', '', '']\n",
      "Found potential headers on page 10: ['', '', '']\n",
      "Found potential headers on page 11: ['', '', '']\n",
      "Found potential headers on page 12: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Found potential headers on page 13: ['', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "def normalize_cell_text(cell_text):\n",
    "    \"\"\"Normalize cell text by removing extra whitespace and newlines, clean text from a pdf table cell\n",
    "\n",
    "    Args:\n",
    "    cell(str or none): The raw data content from a pdf table cell. This could be a string, dict, or None if the cell is empty.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned and normalized text from the cell. If the input is None or not a string, returns an empty string.\n",
    "    Ex.:\n",
    "    normalize_cell_text(\"  This is   a sample \\n text.  \") -> \"This is a sample text.\"\n",
    "    normalize_cell_text(None) -> \"\"\n",
    "    \"\"\"\n",
    "    if isinstance(cell_text, str):\n",
    "        return cell_text.strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Open the syll pdf\n",
    "pdf_path = Path(\"../data/raw/syllabus/IF Data Science 2025 Syllabus.pdf\")\n",
    "\n",
    "table_headers = []\n",
    "with pdfplumber.open(pdf_path.as_posix()) as pdf:  # open the PDF file, convert Path to string with as_posix()\n",
    "    for page_num, page in enumerate(pdf.pages):  # loop through each page\n",
    "        page_tables = page.extract_tables()  # extract tables from the page\n",
    "        for table in page_tables:  # loop through each table\n",
    "            if not table:  # skip empty tables\n",
    "                continue\n",
    "            # Look at the first few rows of each table to see if it has headers\n",
    "            headers = [normalize_cell_text(cell) for cell in table]  # first row as headers\n",
    "            if not headers: # skip empty rows \n",
    "                continue\n",
    "\n",
    "            # Convert everything to lowercase\n",
    "            lowercase_headers = [header.lower() for header in headers]\n",
    "\n",
    "            # If row contains words like pre-class, pre-class content\n",
    "            if any(re.search(r' pre-class' in cell or \"pre-class content\", cell) for cell in lowercase_headers):\n",
    "                table_headers.append({\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"headers\": headers\n",
    "                })\n",
    "        print(f\"Found potential headers on page {page_num + 1}: {headers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
