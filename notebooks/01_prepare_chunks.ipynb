{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafc8e61",
   "metadata": {},
   "source": [
    "# Prepare Chunks (Slides and Labs) + Build Resource Catalog (Syllabus)\n",
    "\n",
    "This notebook prepares our course materials so they can be used later for search and plan generation.\n",
    "\n",
    "We treat each source differently based on its role:\n",
    "\n",
    "- **Slides & Labs:** broken into small text “chunks” so we can embed them, store them in FAISS, and retrieve them when a student asks about a topic.\n",
    "\n",
    "- **Syllabus:** not chunked; instead, we extract its links (such as pre-class readings, Kaggle, DataCamp, YouTube) and use them to build a resource catalog for supplemental study material.\n",
    "\n",
    "**What happens here**\n",
    "\n",
    "1. Collect files from `data/raw/`\n",
    "\n",
    "- Slides (PDFs)\n",
    "- Labs (Jupyter notebooks)\n",
    "- Syllabus (PDF with resource links)\n",
    "\n",
    "2. Extract and process text\n",
    "\n",
    "- Slides -> page-level text\n",
    "- Labs -> Markdown + Code cells as text\n",
    "- Syllabus -> links only (added to catalog, not chunked)\n",
    "\n",
    "3. Chunk text for slides/labs (~700 chars text).\n",
    "\n",
    "4. Save outputs into structured tables.\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "- `slides_chunks.parquet` / `slides_chunks.csv`\n",
    "- `labs_chunks.parquet` / `labs_chunks.csv`\n",
    "- `resources_catalog.csv` (built from syllabus links)\n",
    "\n",
    "**Schema: Chunk Files (slides/labs)**\n",
    "\n",
    "- `chunk_id` -> stable id (such as `<doc>#p<page>#c<chunk>` or `<nb>#cell<idx>#c<chunk>`)\n",
    "\n",
    "- `doc_id` -> source document/notebook name\n",
    "\n",
    "- `source_type` -> `slide | lab`\n",
    "\n",
    "- `page` / `cell_index` -> where the chunk came from\n",
    "\n",
    "- `topic_hint` -> (blank for now) placeholder for future tagging\n",
    "\n",
    "- `text` -> the chunk text\n",
    "\n",
    "- `citation_url` -> file path with page anchor\n",
    "\n",
    "**Schema: Resource Catalog (syllabus)**\n",
    "\n",
    "- `id` ->  unique numeric identifier\n",
    "\n",
    "- `title` -> \n",
    "\n",
    "- `url` -> the resource link\n",
    "\n",
    "- `category` -> course or external (such as Kaggle, DataCamp, YouTube)\n",
    "\n",
    "- `topic_tags` -> \n",
    "\n",
    "- `description` -> line snippet or short description from syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library \n",
    "from pathlib import Path # For handling file paths\n",
    "import re # This is for regular expressions is used for pattern matching in strings used for text cleaning,processing\n",
    "import fitz # For reading PDF files\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655123f3",
   "metadata": {},
   "source": [
    "## Find all the necessary data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ba534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides dir: C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\slides | exists: True\n",
      "Syllabus : C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\syllabus | exists: True\n",
      "Labs     : C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\labs | exists: True\n"
     ]
    }
   ],
   "source": [
    "# Access raw data folders\n",
    "raw = Path(\"../data/raw\").resolve()\n",
    "slides_folder = raw/\"slides\"\n",
    "syllabus_folder = raw/\"syllabus\"\n",
    "labs_folder = raw/\"labs\"\n",
    "\n",
    "print(\"Slides dir:\", slides_folder,   \"| exists:\", slides_folder.exists())\n",
    "print(\"Syllabus :\", syllabus_folder, \"| exists:\", syllabus_folder.exists())\n",
    "print(\"Labs     :\", labs_folder,     \"| exists:\", labs_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0844c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 slide PDFs\n",
      "Found 1 syllabus PDFs\n",
      "Found 39 lab notebooks\n"
     ]
    }
   ],
   "source": [
    "# Make a list of the files\n",
    "slide_files = []\n",
    "for file in slides_folder.rglob(\"*.pdf\"): # look for all pdf files\n",
    "    slide_files.append(file) # add them to the slide_files list\n",
    "\n",
    "syllabus_files = []\n",
    "for file in syllabus_folder.rglob(\"*.pdf\"): # look for all pdf files\n",
    "    syllabus_files.append(file) # add them to the syllabus_files list\n",
    "    \n",
    "lab_files = []\n",
    "for file in labs_folder.rglob(\"*.ipynb\"): # look for all ipynb files\n",
    "    if \"checkpoint\" not in str(file): # ignore autosave checkpoints Jupyter creates\n",
    "        lab_files.append(file) # add the rest to the lab_files list\n",
    "\n",
    "# Show the files found\n",
    "print(\"Found\", len(slide_files), \"slide PDFs\")\n",
    "print(\"Found\", len(syllabus_files), \"syllabus PDFs\")\n",
    "print(\"Found\", len(lab_files), \"lab notebooks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9d99e",
   "metadata": {},
   "source": [
    "# Extract Text and Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c55be1",
   "metadata": {},
   "source": [
    "### Chunking logic\n",
    "\n",
    "Some slide pages are very short (just a few bullets), while others may contain lots of text.  \n",
    "To make retrieval effective, we use a simple rule:\n",
    "\n",
    "- If a page is short (≤700 characters) → keep the page as one chunk.  \n",
    "- If a page is long (>700 characters) → split into multiple 700-character chunks.  \n",
    "\n",
    "This way:\n",
    "- Sparse slides are not split unnecessarily.\n",
    "- Dense slides are broken into smaller, more searchable pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text, max_characters=700):\n",
    "    \"\"\"If text is short, keep it as is. If it's long, split into 700 char chunks.\"\"\"\n",
    "    if len(text) <= max_characters:\n",
    "        return [text] # return a list with the original text if it's short enough\n",
    "    else:\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), max_characters):\n",
    "            chunk = text[i:i + max_characters] # get a chunk of max_characters\n",
    "            chunks.append(chunk)  # add the chunk to the list\n",
    "        return chunks      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42809343",
   "metadata": {},
   "source": [
    "**Slides**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 40 chunks from 39 slide PDFs\n",
      "Saved to data/processed/slides_chunks.csv and slides_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# Extract from slides\n",
    "# Each slide pdf is read and loop through its pages. For each page, the text is extracted and stored in a list of dictionaries with metadata (source, file name, page number, and text content).\n",
    "# Each row  = one slide page\n",
    "# Each column = source, file, page, text\n",
    "\n",
    "\n",
    "# Hold the extracted text chunk \n",
    "slides_rows = []\n",
    "\n",
    "# Loop through is slide pdf \n",
    "for file in slide_files:\n",
    "    pdf = fitz.open(file.as_posix()) # open the PDF file, convert Path to string with as_posix()\n",
    "    # Iterate over each page and extract text\n",
    "    for page_num in range(len(pdf)): # iterate over each page\n",
    "        page_text = pdf[page_num].get_text(\"text\").strip() # extract text and add a newline for separation\n",
    "        if not page_text: # skip empty pages\n",
    "            continue\n",
    "\n",
    "    # Apply chunking\n",
    "    chunks = create_chunks(page_text, max_characters=700)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        slides_rows.append({\"source\": \"slide\", \n",
    "                            \"file\": file.name, # the file name \n",
    "                            \"page\": page_num + 1, # page number \n",
    "                            \"chunk\": i + 1, # chunk number\n",
    "                            \"text\": chunk # the chunked text\n",
    "        }) # store the extracted text with metadata\n",
    "\n",
    "print(f\"Extracted {len(slides_rows)} chunks from {len(slide_files)} slide PDFs\") \n",
    "            \n",
    "# Convert to DataFrame and show sample\n",
    "slides_df = pd.DataFrame(slides_rows)\n",
    "slides_df.head()\n",
    "\n",
    "# Save to data/processed\n",
    "slides_df.to_csv(\"../data/processed/slides_chunks.csv\", index=False)\n",
    "slides_df.to_parquet(\"../data/processed/slides_chunks.parquet\", index=False)\n",
    "\n",
    "print (\"Saved to data/processed/slides_chunks.csv and slides_chunks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     40.000000\n",
      "mean      60.675000\n",
      "std       23.432814\n",
      "min       15.000000\n",
      "25%       43.750000\n",
      "50%       62.000000\n",
      "75%       77.500000\n",
      "max      104.000000\n",
      "Name: page, dtype: float64\n",
      "count     40.000000\n",
      "mean     213.975000\n",
      "std      121.045126\n",
      "min        0.000000\n",
      "25%      162.500000\n",
      "50%      213.500000\n",
      "75%      238.250000\n",
      "max      700.000000\n",
      "Name: text, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slide</td>\n",
       "      <td>(Re)-Introduction to Data Science &amp; Control Fl...</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slide</td>\n",
       "      <td>AB Testing.pdf</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>Thursday\\nOn Thursday we will be meeting for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced Abstraction.pptx.pdf</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday\\nTomorrow will entail:\\n●\\nFurther exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced Control Flow.pptx.pdf</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced Data Processing.pptx.pdf</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday\\nWednesday will entail:\\n●\\nA review...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               file  page  chunk  \\\n",
       "0  slide  (Re)-Introduction to Data Science & Control Fl...   102      1   \n",
       "1  slide                                     AB Testing.pdf    49      1   \n",
       "2  slide                      Advanced Abstraction.pptx.pdf    66      1   \n",
       "3  slide                     Advanced Control Flow.pptx.pdf    93      1   \n",
       "4  slide                  Advanced Data Processing.pptx.pdf    61      1   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1  Thursday\\nOn Thursday we will be meeting for o...  \n",
       "2  Tuesday\\nTomorrow will entail:\\n●\\nFurther exp...  \n",
       "3                                                     \n",
       "4  Wednesday\\nWednesday will entail:\\n●\\nA review...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(slides_rows)\n",
    "print(df['page'].describe())      # sanity check page numbers\n",
    "print(df['text'].str.len().describe())  # avg text length\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11cb5fb",
   "metadata": {},
   "source": [
    "**Labs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e42387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty file: C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\raw\\labs\\phase2\\week09\\transformers.ipynb\n",
      "Extracted 1410 chunks from labs\n",
      "Saved labs to ../data/processed/labs_chunks.csv and labs_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Extract text from code and markdown cells in lab notebooks\n",
    "\n",
    "labs_rows = [] # hold the extracted text chunks\n",
    "\n",
    "for file in lab_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f: # r allows reading the file\n",
    "            content = f.read() # read the entire file content\n",
    "            if not content.strip(): # skip empty files\n",
    "                print(f\"Skipping empty file: {file}\")\n",
    "                continue\n",
    "            notebook = json.loads(content)  # load the notebook as a dict\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    cells = notebook.get('cells', [])  # access the cells section\n",
    "    for cell in cells:  # loop through each cell\n",
    "        cell_type = cell.get('cell_type')  # get the type of cell (code or markdown)\n",
    "        if cell_type in ['code', 'markdown']:  # only want code and markdown cells\n",
    "            source = ''.join(cell.get('source', [])).strip()  # join the list of strings into one string and strip whitespace\n",
    "            if not source:  # skip empty cells\n",
    "                continue\n",
    "\n",
    "            # Apply chunking\n",
    "            chunks = create_chunks(source, max_characters=700)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                labs_rows.append({\n",
    "                    \"source\": \"lab\", # source type\n",
    "                    \"file\": file.name, # the file name\n",
    "                    \"cell_type\": cell_type, # code or markdown\n",
    "                    \"chunk\": i + 1, # chunk number\n",
    "                    \"text\": chunk # the chunked text\n",
    "                })  # store the extracted text with metadata\n",
    "\n",
    "print(\"Extracted\", len(labs_rows), \"chunks from labs\")\n",
    "\n",
    "# Convert to DataFrame and show sample\n",
    "labs_df = pd.DataFrame(labs_rows)\n",
    "labs_df.to_csv(\"../data/processed/labs_chunks.csv\", index=False)\n",
    "labs_df.to_parquet(\"../data/processed/labs_chunks.parquet\", index=False)\n",
    "\n",
    "labs_df.head()\n",
    "print(\"Saved labs to ../data/processed/labs_chunks.csv and labs_chunks.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6465d24",
   "metadata": {},
   "source": [
    "**Syllabus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28dceeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYLL_PDF_PATH = Path(\"../data/raw/syllabus/IF Data Science 2025 Syllabus.pdf\")\n",
    "OUT_ALL_LINKS  = Path(\"../data/processed/syllabus_all_links.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e476d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 89 total links from IF Data Science 2025 Syllabus.pdf and saved to ..\\data\\processed\\syllabus_all_links.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_links_from_syll(syll_pdf_path):\n",
    "    \"\"\"Extract all links and pre-class links from the syllabus PDF.\n",
    "    Extracts:\n",
    "      A) Link annotations (true clickable links)\n",
    "      B) URLs found in page text via regex\n",
    "    Returns a DataFrame with: page, url, source (\"annotation\" or \"text\"), line_snippet\n",
    "    \"\"\"\n",
    "    syll_pdf = fitz.open(syll_pdf_path.as_posix())  # open the PDF file\n",
    "\n",
    "    url_regex = re.compile(r\"https?://[^\\s]+\")  # regex to find URLs starting with http or https\n",
    "\n",
    "    rows = []  # hold extracted links\n",
    "\n",
    "    for page_num in range(len(syll_pdf)):  # iterate over each page\n",
    "        page = syll_pdf[page_num] # get the page\n",
    "        page_number = page_num + 1 # page number\n",
    "\n",
    "        # A) Extract link annotations\n",
    "        for link in page.get_links():\n",
    "            uri = (link.get(\"uri\") or \"\").strip() # get the URL from the link annotation\n",
    "            if uri:\n",
    "                rows.append({\n",
    "                    \"page\": page_number, \n",
    "                    \"url\": uri, # the URL; uri is used for web links\n",
    "                    \"source\": \"annotation\",\n",
    "                    \"line_snippet\": \"\"  # no line snippet for annotations\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Get all links from the pdf \n",
    "all_links_df = extract_links_from_syll(SYLL_PDF_PATH)\n",
    "all_links_df.to_csv(OUT_ALL_LINKS, index=False)\n",
    "\n",
    "print(f\"Extracted {len(all_links_df)} total links from {SYLL_PDF_PATH.name} and saved to {OUT_ALL_LINKS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf6b622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved resource catalog with 89 resources to ..\\catalog\\resources_catalog.csv\n"
     ]
    }
   ],
   "source": [
    "ALL_LINKS = Path(\"../data/processed/syllabus_all_links.csv\")\n",
    "OUT_LINKS = Path(\"../catalog/resources_catalog.csv\")\n",
    "\n",
    "syll_df = pd.read_csv(ALL_LINKS)\n",
    "\n",
    "# External resources categories (Kaggle/DataCamp/YouTube)\n",
    "\n",
    "def categorize_link(url):\n",
    "    url = str(url).lower()\n",
    "    if any (keyword in url for keyword in [\"kaggle.com\", \"datacamp.com\", \"youtube.com\"]):\n",
    "        return \"external\"\n",
    "    return \"course\"\n",
    "\n",
    "# Create catalog category col\n",
    "\n",
    "catalog_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": range(1, len(syll_df) + 1), # unique ID\n",
    "        \"title\": \"\",  # \n",
    "        \"url\": syll_df[\"url\"].fillna(\"\"),  # the URL\n",
    "        \"category\": syll_df[\"url\"].apply(categorize_link),  # categorize as external or course\n",
    "        \"topic_tags\": \"\",  \n",
    "        \"description\": syll_df.get(\"line_snippet\").fillna(\"\"),  # use line snippet as description if available\n",
    "    }\n",
    ")\n",
    "\n",
    "catalog_df.to_csv(OUT_LINKS, index=False)\n",
    "print(f\"Saved resource catalog with {len(catalog_df)} resources to {OUT_LINKS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbededb",
   "metadata": {},
   "source": [
    "**Clean Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c08de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 empty rows from slides_df\n",
      "Before filtering: 38\n",
      "Empty DataFrame\n",
      "Columns: [source, file, page, chunk, text]\n",
      "Index: []\n",
      "After filtering: 38\n"
     ]
    }
   ],
   "source": [
    "# Remove white spaces in Slides\n",
    "# slides_df[\"text\"] = (\n",
    "#     slides_df[\"text\"]. astype(str)  # ensure it's a string\n",
    "#     .str.replace(r'\\s+', ' ', regex=True)  # replace multiple whitespace with single space\n",
    "#     .str.strip()  # remove leading/trailing whitespace\n",
    "# )\n",
    "\n",
    "# # Drop empty rows\n",
    "# slides_before = len(slides_df)\n",
    "# slides_df = slides_df[slides_df[\"text\"] != \"\"].copy()\n",
    "# print(f\"Dropped {slides_before - len(slides_df)} empty rows from slides_df\")\n",
    "\n",
    "# # Remove Agenda slides\n",
    "# # Count how many rows mention \"agenda\"\n",
    "# print(\"Before filtering:\", len(slides_df))\n",
    "# print(slides_df[slides_df[\"text\"].str.contains(\"agenda\", case=False, na=False)].head(5))\n",
    "\n",
    "\n",
    "# # Drop rows that contain the word \"agenda\" (case-insensitive)\n",
    "# slides_df = slides_df[~slides_df[\"text\"].str.contains(\"agenda\", case=False, na=False)].copy()\n",
    "\n",
    "# print(\"After filtering:\", len(slides_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0158e",
   "metadata": {},
   "source": [
    "## Note for Future Work \n",
    "\n",
    "Currently, the slide data is going to be used as is after chunking. Some extra processing is needed such as removing the agendas from the slides to improve retreiving data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
