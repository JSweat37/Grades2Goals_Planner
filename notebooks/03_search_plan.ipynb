{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af13f703",
   "metadata": {},
   "source": [
    "the goal of thois notebook is to \n",
    "pull from faiss and connect to llm to output 7 day plan \n",
    "\n",
    "Implement semantic search (topk) and a simple 7-day plan composer that attaches citations from chunks.\n",
    "\n",
    "LLM planner\n",
    "\n",
    "use openai that can\n",
    "1. take a query (student's input)\n",
    "2. search indicies using (search_slides and search_labs)\n",
    "3. format results with citations \n",
    "4. call the chat model using api key\n",
    "5. gen 7day  plan \n",
    "6. save the plan to JSON/CSV \n",
    "\n",
    "**Inputs**\n",
    "- `../data/processed/slides_chunks.parquet`\n",
    "- `../data/processed/labs_chunks.parquet`\n",
    "- `../data/processed/faiss_slides.index`\n",
    "- `../data/processed/faiss_labs.index`\n",
    "\n",
    "\n",
    "- `../data/processed/faiss_labs.index`\n",
    "\n",
    "**Outputs**\n",
    "- Printed study plan in the notebook\n",
    "- JSON file of the plan for saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84587bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "583ea6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides chunks: True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\slides_chunks.parquet\n",
      "Labs chunks:   True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\labs_chunks.parquet\n",
      "Slides index:  True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\faiss_slides.index\n",
      "Labs index:    True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\faiss_labs.index\n"
     ]
    }
   ],
   "source": [
    "# Load DAta\n",
    "# Folders\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "PROCESSED_FOLDER = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "SLIDES_CHUNKS_PATH = PROCESSED_FOLDER / \"slides_chunks.parquet\"\n",
    "LABS_CHUNKS_PATH   = PROCESSED_FOLDER / \"labs_chunks.parquet\"\n",
    "FAISS_SLIDES_PATH  = PROCESSED_FOLDER / \"faiss_slides.index\"\n",
    "FAISS_LABS_PATH    = PROCESSED_FOLDER / \"faiss_labs.index\"\n",
    "\n",
    "\n",
    "print(\"Slides chunks:\", SLIDES_CHUNKS_PATH.exists(), SLIDES_CHUNKS_PATH)\n",
    "print(\"Labs chunks:  \", LABS_CHUNKS_PATH.exists(),   LABS_CHUNKS_PATH)\n",
    "print(\"Slides index: \", FAISS_SLIDES_PATH.exists(),  FAISS_SLIDES_PATH)\n",
    "print(\"Labs index:   \", FAISS_LABS_PATH.exists(),    FAISS_LABS_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d41d4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides rows: 40\n",
      "Labs rows:   1410\n"
     ]
    }
   ],
   "source": [
    "# Load chunks \n",
    "slides_df = pd.read_parquet(\"../data/processed/slides_chunks.parquet\")\n",
    "labs_df   = pd.read_parquet(\"../data/processed/labs_chunks.parquet\")\n",
    "\n",
    "# Drop empty rows\n",
    "slides_df = slides_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "labs_df   = labs_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "print(\"Slides rows:\", len(slides_df))\n",
    "print(\"Labs rows:  \", len(labs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b814a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS indexes from disk\n",
    "slides_index = faiss.read_index(FAISS_SLIDES_PATH.as_posix())\n",
    "labs_index   = faiss.read_index(FAISS_LABS_PATH.as_posix())# LLM call (OpenAI) to build a 7-day plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a059f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "\n",
    "def encode_normalized(texts):\n",
    "    \"\"\"Convert a single query string (what the student types) into a normalized float32 vector.\n",
    "    - normalize_embeddings=True ensures vectors have length 1, so FAISS inner product ≈ cosine similarity.\n",
    "    - We return a NumPy array with dtype float32 because FAISS expects float32 vectors.\"\"\"\n",
    "    embeddings = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(\"Embedding model loaded:\", embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87de2254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5e03896a314fd1bd3374ca0628093f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf19373283c44d38b61630a384bbb2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides index size: 40\n",
      "Labs index size: 1410\n"
     ]
    }
   ],
   "source": [
    "# Create 2 FAISS indices, one for slides and one for labs\n",
    "\n",
    "# Slides index\n",
    "slides_embeddings = encode_normalized(slides_df['text'].tolist())\n",
    "slides_index = faiss.IndexFlatIP(slides_embeddings.shape[1])\n",
    "slides_index.add(slides_embeddings)\n",
    "faiss.write_index(slides_index, FAISS_SLIDES_PATH.as_posix())\n",
    "\n",
    "# Labs index\n",
    "labs_embeddings = encode_normalized(labs_df['text'].tolist())\n",
    "labs_index = faiss.IndexFlatIP(labs_embeddings.shape[1])\n",
    "labs_index.add(labs_embeddings)\n",
    "faiss.write_index(labs_index, FAISS_LABS_PATH.as_posix())\n",
    "\n",
    "print(\"Slides index size:\", slides_index.ntotal)\n",
    "print(\"Labs index size:\", labs_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44485042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_slides(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the slides index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the slides index.\n",
    "    3) For each match, look up the original row in slides_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = slides_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from slides_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = slides_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"slide\",\n",
    "            \"file\": row['file'], # slide filename\n",
    "            \"page\": row['page'], # slide page number\n",
    "            \"text\": row['text'], # slide text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdbca334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_labs(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the labs index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the labs index.\n",
    "    3) For each match, look up the original row in labs_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = labs_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from labs_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = labs_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"lab\",\n",
    "            \"file\": row['file'], # lab filename\n",
    "            \"text\": row['text'], # lab text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c29779",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)  # take environment variables from .env file\n",
    "\n",
    "# Create a single OpenAI client (reads key from environment)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def generate_7_day_plan(feedback_text, top_k_slides: 5, top_k_labs: 5, model_name: \"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    End-to-end:\n",
    "    1) Search slides and labs separately.\n",
    "    2) Concatenate the results (labs first by default since they are practical).\n",
    "    3) Build a single context block with simple citations.\n",
    "    4) Call OpenAI to write a 7-day plan with spaced review.\n",
    "    5) Return the generated text.\n",
    "\n",
    "    Args:\n",
    "        feedback_text (str): Student's feedback, \"I lost points on SQL joins and confusion matrix.\"\n",
    "        top_k_slides (int): Number of slide chunks to include.\n",
    "        top_k_labs   (int): Number of lab chunks to include.\n",
    "        model_name   (str): OpenAI chat model.\n",
    "\n",
    "    Returns:\n",
    "        str: The study plan text generated by the LLM.\n",
    "    \"\"\"\n",
    "    # Search slides and labs\n",
    "    slides_results = search_slides(feedback_text, top_k=top_k_slides)\n",
    "    labs_results   = search_labs(feedback_text, top_k=top_k_labs)\n",
    "\n",
    "    # Combine results \n",
    "    combined_results = pd.concat([labs_results, slides_results], ignore_index=True)\n",
    "\n",
    "    # Build context block with citations\n",
    "    context_blocks = []\n",
    "    for i, row in combined_results.iterrows():\n",
    "        citation = f\"[{i+1}]\"\n",
    "        if row['source_type'] == 'slide':\n",
    "            context_blocks.append(f\"{citation} (Slide: {row['file']} Page: {row['page']}) {row['text']}\")\n",
    "        else:\n",
    "            context_blocks.append(f\"{citation} (Lab: {row['file']}) {row['text']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks) # double newline for readability\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are an academic coach for a data science course. \"\n",
    "        \"You must create a concrete 7-day micro-task plan using ONLY the provided context. \"\n",
    "        \"Ensure tasks alternate between review (reading/notes), application (coding exercises), and reflection.\"\n",
    "        \"Each day should include 2–4 actionable tasks, with estimated time, and a citation line that points back to the source. \"\n",
    "        \"Use spaced review on Day 1, Day 3, and Day 6. \"\n",
    "        \"If context is insufficient for any part, state that clearly.\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Student feedback: {feedback_text}\\n\\n\"\n",
    "        f\"Context from course materials (slides and labs):\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"Now write the 7-day plan in this structure:\\n\"\n",
    "        \"Day 1 — Understand\\n\"\n",
    "        \"- Task 1 (est. 15–25 min) — description [CITATION]\\n\"\n",
    "        \"- Task 2 (est. 10–20 min) — description [CITATION]\\n\"\n",
    "        \"Day 2 — Apply\\n\"\n",
    "        \"- Task 1 ...\\n\"\n",
    "        \"...\\n\"\n",
    "        \"Day 7 — Checkpoint\\n\"\n",
    "        \"- Mini-quiz or small coding task ...\\n\"\n",
    "        \"\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Use only facts available in the context above.\\n\"\n",
    "        \"- Each task line should end with a [CITATION] using the [SOURCE: ...] entry from context.\\n\"\n",
    "        \"- If something is unclear or missing, say so.\\n\"\n",
    "    )\n",
    "\n",
    "    # Call OpenAI chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2,  # low temperature for focused output\n",
    "        max_tokens=1000   # adjust as needed\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4406a062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b0ff31aa2d4c4b8a4a03952b3d7b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a732f417b0444da9893657a7e1e0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Day 1 — Understand\n",
      "- Task 1 (est. 20 min) — Review the SQL keywords related to Data Definition Language (DDL) and Data Manipulation Language (DML). Focus on understanding the purpose of each keyword, especially `JOIN`. Take notes on how `INNER JOIN`, `LEFT JOIN`, and `RIGHT JOIN` differ. [CITATION: 5]\n",
      "- Task 2 (est. 15 min) — Read through the SQL commands for selecting, inserting, updating, and deleting data. Pay special attention to the `SELECT` statement and how it can be used with `JOIN`. [CITATION: 1]\n",
      "\n",
      "### Day 2 — Apply\n",
      "- Task 1 (est. 30 min) — Complete a coding exercise that involves creating a sample database with at least two tables. Use `INNER JOIN`, `LEFT JOIN`, and `RIGHT JOIN` to query data from these tables. [CITATION: 1]\n",
      "- Task 2 (est. 15 min) — Write SQL queries to insert, update, and delete data in your sample database. Ensure you practice using the `WHERE` clause effectively. [CITATION: 1]\n",
      "\n",
      "### Day 3 — Review\n",
      "- Task 1 (est. 20 min) — Revisit your notes on SQL joins. Create a comparison chart that outlines the differences between `INNER JOIN`, `LEFT JOIN`, and `RIGHT JOIN`. [CITATION: 5]\n",
      "- Task 2 (est. 15 min) — Review the SQL commands for data manipulation (INSERT, UPDATE, DELETE). Summarize their syntax and use cases in your notes. [CITATION: 1]\n",
      "\n",
      "### Day 4 — Apply\n",
      "- Task 1 (est. 30 min) — Solve a set of SQL exercises focusing on joins. Use sample datasets to practice writing queries that combine data from multiple tables using different types of joins. [CITATION: 1]\n",
      "- Task 2 (est. 15 min) — Write a short SQL script that demonstrates the use of `JOIN` in a real-world scenario, such as retrieving student enrollment data from a database. [CITATION: 2]\n",
      "\n",
      "### Day 5 — Reflect\n",
      "- Task 1 (est. 20 min) — Reflect on your understanding of SQL joins. Write a brief summary explaining when to use each type of join and why. [CITATION: 5]\n",
      "- Task 2 (est. 15 min) — Discuss with a peer or mentor the challenges you faced with SQL joins and how you overcame them. Take notes on their insights. [CITATION: 5]\n",
      "\n",
      "### Day 6 — Review\n",
      "- Task 1 (est. 20 min) — Go over your comparison chart of SQL joins again. Add any new insights or clarifications you have gained since the last review. [CITATION: 5]\n",
      "- Task 2 (est. 15 min) — Revisit the SQL commands for data manipulation. Create a flashcard set to quiz yourself on the syntax and use cases of each command. [CITATION: 1]\n",
      "\n",
      "### Day 7 — Checkpoint\n",
      "- Task 1 (est. 30 min) — Complete a mini-quiz that includes questions on SQL joins and data manipulation commands. Include multiple-choice questions and coding tasks to assess your understanding. [CITATION: 1]\n",
      "- Task 2 (est. 15 min) — Write a short reflection on your learning journey over the past week. Identify areas where you feel confident and areas that still need improvement. [CITATION: 5]\n"
     ]
    }
   ],
   "source": [
    "example_feedback = \"I lost points on SQL joins and I keep mixing up inner vs left vs right joins.\"\n",
    "plan_text = generate_7_day_plan(example_feedback, top_k_slides=4, top_k_labs=6, model_name=\"gpt-4o-mini\")\n",
    "print(plan_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddfac8",
   "metadata": {},
   "source": [
    "# Prototype Eval (test cases)\n",
    "\n",
    "Evaluating the prototype on at least 10 examples to check:\n",
    "- Topic Correctness\n",
    "- Citation Correctness\n",
    "- Format Correctness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c5dcc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef37a191b1844ea69134cbe38ff10016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935b5d011ea84047b4a89ecd5faa259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Day 1 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the concept of SQL joins, focusing on inner, left, and right joins. Take notes on their differences and use cases. [CITATION: Slide: Introduction to Structured Databases I (1).pdf Page: 62.0]\n",
      "- **Task 2 (est. 10–20 min)** — Read through the SQL examples provided in the course materials, specifically focusing on the `DELETE` and `DROP TABLE` commands to understand their implications. [CITATION: Lab: w10-class1.ipynb]\n",
      "\n",
      "### Day 2 — Apply\n",
      "- **Task 1 (est. 20–30 min)** — Complete a coding exercise that requires writing SQL queries using inner, left, and right joins on a sample dataset. [CITATION: Lab: w9-class2.ipynb]\n",
      "- **Task 2 (est. 15–25 min)** — Use Python's `pd.read_sql_query` to execute a query that retrieves users with missing age values. [CITATION: Lab: w9-class2.ipynb]\n",
      "\n",
      "### Day 3 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Revisit your notes on SQL joins and summarize the key differences between inner, left, and right joins. [CITATION: Slide: Introduction to Structured Databases I (1).pdf Page: 62.0]\n",
      "- **Task 2 (est. 10–20 min)** — Review the code snippets related to handling missing values in dataframes, focusing on `dropna()`. [CITATION: Lab: w7-class3.ipynb]\n",
      "\n",
      "### Day 4 — Apply\n",
      "- **Task 1 (est. 20–30 min)** — Write a Python script that connects to a database and retrieves records using different types of joins. Document the results. [CITATION: Lab: w9-class2.ipynb]\n",
      "- **Task 2 (est. 15–25 min)** — Create a dataframe with missing values and practice using `dropna()` to remove rows and columns with missing data. [CITATION: Lab: w7-class3.ipynb]\n",
      "\n",
      "### Day 5 — Reflect\n",
      "- **Task 1 (est. 15–25 min)** — Reflect on the challenges faced while applying SQL joins in coding exercises. Write a brief summary of what you learned. [CITATION: Context from course materials]\n",
      "- **Task 2 (est. 10–20 min)** — Consider how you can improve your understanding of SQL joins. Identify specific areas for further study or practice. [CITATION: Context from course materials]\n",
      "\n",
      "### Day 6 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Go over your previous notes and summaries on SQL joins and missing value handling. Ensure you can explain these concepts clearly. [CITATION: Slide: Introduction to Structured Databases I (1).pdf Page: 62.0]\n",
      "- **Task 2 (est. 10–20 min)** — Review the SQL commands related to data manipulation (e.g., `DELETE`, `DROP TABLE`) and their consequences. [CITATION: Lab: w10-class1.ipynb]\n",
      "\n",
      "### Day 7 — Checkpoint\n",
      "- **Mini-quiz (est. 30 min)** — Complete a small coding task that involves writing SQL queries with different joins and handling missing values in a dataframe. Document your approach and results. [CITATION: Context from course materials]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0370a392cf2b4db09a63980625cf6b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea738034c444b1b8a0587186ebce414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "**Day 1 — Understand**\n",
      "- Task 1 (est. 15–25 min) — Review the concepts of overfitting and regularization in machine learning. Take notes on how these concepts affect model performance and generalization. [CITATION: Not provided]\n",
      "- Task 2 (est. 10–20 min) — Read through the slides on classification algorithms and note the differences between them, including the benefits of ensemble methods. [CITATION: Slide: Introduction to Decision Trees.pdf Page: 62.0]\n",
      "\n",
      "**Day 2 — Apply**\n",
      "- Task 1 (est. 30 min) — Implement the `k_means` function from Lab [5]. Experiment with different values of k (e.g., k=3) and observe how the final centroids and assignments differ. [CITATION: Lab: lab.ipynb]\n",
      "- Task 2 (est. 20 min) — Complete the TODO in Lab [4] by implementing a simple machine learning model. Test its performance on a sample dataset. [CITATION: Lab: classification_challenge.ipynb]\n",
      "\n",
      "**Day 3 — Review**\n",
      "- Task 1 (est. 15–25 min) — Revisit your notes on overfitting and regularization. Reflect on how these concepts relate to the tasks you completed on Day 2. [CITATION: Not provided]\n",
      "- Task 2 (est. 10–20 min) — Review the slides on Random Forests and summarize the key points related to ensemble methods and their advantages. [CITATION: Slide: Random Forests.pdf Page: 60.0]\n",
      "\n",
      "**Day 4 — Apply**\n",
      "- Task 1 (est. 30 min) — Work on the `gradient_descent_update` function from Lab [3]. Implement it and test how changing the learning rate (`lr`) affects the model's predictions. [CITATION: Lab: neural_networks.ipynb]\n",
      "- Task 2 (est. 20 min) — Explore the unsupervised learning algorithms applicable to the PCA components from Lab [2]. Write a brief explanation of which algorithm you would choose and why. [CITATION: Lab: nlp_vectors.ipynb]\n",
      "\n",
      "**Day 5 — Reflect**\n",
      "- Task 1 (est. 15–25 min) — Reflect on the coding tasks from Days 2 and 4. Write down any challenges you faced and how you overcame them. [CITATION: Not provided]\n",
      "- Task 2 (est. 10–20 min) — Create a mind map that connects the concepts of overfitting, regularization, and the algorithms you have learned about so far. [CITATION: Not provided]\n",
      "\n",
      "**Day 6 — Review**\n",
      "- Task 1 (est. 15–25 min) — Revisit your notes on classification algorithms and ensemble methods. Consider how these concepts apply to the tasks you have completed. [CITATION: Not provided]\n",
      "- Task 2 (est. 10–20 min) — Review the slides on the transformer architecture and summarize its applications beyond text processing. [CITATION: Slide: Transformer Architecture.pdf Page: 19.0]\n",
      "\n",
      "**Day 7 — Checkpoint**\n",
      "- Task 1 (est. 30 min) — Complete a mini-quiz or a small coding task that covers the concepts of overfitting, regularization, and the implementation of k-means clustering. [CITATION: Not provided]\n",
      "- Task 2 (est. 20 min) — Write a reflection on what you have learned throughout the week and identify areas where you still feel uncertain or need further practice. [CITATION: Not provided]\n",
      "\n",
      "This plan is designed to help you reinforce your understanding of key concepts in machine learning while applying them through practical coding tasks and reflecting on your learning process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2672c28e7bbe4264a71bb1c8a6f526f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec6177dc31e42659b68a09744ef3515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "#### Day 1 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the section titled \"How does backpropagation work?\" from the provided IBM link to grasp the foundational concepts of backpropagation in neural networks. [CITATION: 4]\n",
      "- **Task 2 (est. 10–20 min)** — Read through the \"Introduction to Neural Networks\" slide to familiarize yourself with the basic concepts of neural networks, including recurrent neural networks and their applications. [CITATION: 9]\n",
      "\n",
      "#### Day 2 — Apply\n",
      "- **Task 1 (est. 30–45 min)** — Complete the coding exercises in the `neural_networks.ipynb` lab, focusing on implementing a simple neural network using the Keras package. [CITATION: 4]\n",
      "- **Task 2 (est. 20–30 min)** — Experiment with modifying the parameters of your neural network in the lab to observe how changes affect the model's performance. [CITATION: 4]\n",
      "\n",
      "#### Day 3 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Revisit the \"Types of Visualizations Review\" slide to reinforce your understanding of latent variables and their significance in data analysis. [CITATION: 6]\n",
      "- **Task 2 (est. 10–20 min)** — Review the \"Applied LLMs & Agents\" slide to understand the pros and cons of using LLMs in complex reasoning workflows. [CITATION: 7]\n",
      "\n",
      "#### Day 4 — Apply\n",
      "- **Task 1 (est. 30–45 min)** — Work through the `vectors_rags.ipynb` lab, focusing on how contextual information affects model responses. [CITATION: 1]\n",
      "- **Task 2 (est. 20–30 min)** — Complete a coding exercise in the `nlp_vectors.ipynb` lab to explore word embeddings and their outputs in the word2vec model. [CITATION: 2]\n",
      "\n",
      "#### Day 5 — Reflect\n",
      "- **Task 1 (est. 15–25 min)** — Write a reflection on your understanding of neural networks and backpropagation. What concepts are still unclear? What aspects do you feel confident about? \n",
      "- **Task 2 (est. 10–20 min)** — Reflect on the challenges you faced while working with the coding exercises. Document any specific areas where you struggled and consider how you might address these in future study sessions.\n",
      "\n",
      "#### Day 6 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Revisit the \"Advanced Abstraction\" slide to reinforce your understanding of object-oriented programming (OOP) concepts relevant to data science. [CITATION: 10]\n",
      "- **Task 2 (est. 10–20 min)** — Review the \"Transformer Architecture\" slide to solidify your knowledge about the versatility of transformers in various data types beyond text. [CITATION: 8]\n",
      "\n",
      "#### Day 7 — Checkpoint\n",
      "- **Task 1 (est. 30 min)** — Complete a mini-quiz covering key concepts from neural networks, backpropagation, and the applications of transformers. This could include multiple-choice questions and short coding tasks to assess your understanding.\n",
      "- **Task 2 (est. 20 min)** — Conduct a self-assessment of your progress over the week. Identify areas of strength and those needing further review or practice. \n",
      "\n",
      "This plan aims to provide a structured approach to mastering neural networks and backpropagation while ensuring a balance of understanding, application, and reflection.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d201771ef84be29039d738fbfaccfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbbb1ca324143598aedf3e4ecc01b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Day 1 — Understand\n",
      "- Task 1 (est. 15–25 min) — Review the \"Types of Visualizations Review.pdf\" to understand different types of charts and when to use them. Pay special attention to examples that clarify the use of each type. [CITATION: Slide: Types of Visualizations Review.pdf Page: 64.0]\n",
      "- Task 2 (est. 10–20 min) — Read through the \"neural_networks.ipynb\" lab, focusing on the section about plotting errors of mini-batch training. Take notes on how visualization is used in this context. [CITATION: Lab: neural_networks.ipynb]\n",
      "\n",
      "### Day 2 — Apply\n",
      "- Task 1 (est. 30 min) — Complete a coding exercise that involves creating different types of visualizations using a sample dataset. Try to implement at least three different chart types. [CITATION: Context insufficient for specific coding exercise]\n",
      "- Task 2 (est. 20 min) — Use the \"svm.ipynb\" lab to explore non-linear decision boundaries by modifying the code to visualize the results. [CITATION: Lab: svm.ipynb]\n",
      "\n",
      "### Day 3 — Understand\n",
      "- Task 1 (est. 15–25 min) — Revisit the \"Types of Visualizations Review.pdf\" and summarize the key points in your own words. Focus on clarifying any confusion you had previously. [CITATION: Slide: Types of Visualizations Review.pdf Page: 64.0]\n",
      "- Task 2 (est. 10–20 min) — Review the \"naive_bayes.ipynb\" lab, paying attention to how Laplace Smoothing is applied and its importance in data visualization. [CITATION: Lab: naive_bayes.ipynb]\n",
      "\n",
      "### Day 4 — Apply\n",
      "- Task 1 (est. 30 min) — Create a visualization that demonstrates the effect of Laplace Smoothing on word frequency counts. Use a dataset of your choice. [CITATION: Context insufficient for specific coding exercise]\n",
      "- Task 2 (est. 20 min) — Solve a Leetcode problem related to data visualization or data manipulation from the \"w9-class3.ipynb\" lab. [CITATION: Lab: w9-class3.ipynb]\n",
      "\n",
      "### Day 5 — Reflect\n",
      "- Task 1 (est. 20 min) — Write a reflection on what you learned about data visualization over the past few days. Include what types of charts you find most useful and why. [CITATION: Context insufficient for specific reflection prompt]\n",
      "- Task 2 (est. 15 min) — Discuss with a peer or mentor about your experiences with visualizations and gather feedback on your understanding. [CITATION: Context insufficient for specific discussion prompt]\n",
      "\n",
      "### Day 6 — Understand\n",
      "- Task 1 (est. 15–25 min) — Revisit the \"neural_networks.ipynb\" lab and summarize the key points about plotting errors in mini-batch training. Reflect on how this relates to visualization. [CITATION: Lab: neural_networks.ipynb]\n",
      "- Task 2 (est. 10–20 min) — Review the \"Applied LLMs & Agents.pdf\" slide to understand how visualizations can aid in complex reasoning workflows. [CITATION: Slide: Applied LLMs & Agents.pdf Page: 41.0]\n",
      "\n",
      "### Day 7 — Checkpoint\n",
      "- Task 1 (est. 30 min) — Complete a mini-quiz or small coding task that tests your understanding of when to use different types of visualizations. Ensure to include at least one question on non-linear decision boundaries. [CITATION: Context insufficient for specific quiz content]\n",
      "- Task 2 (est. 20 min) — Create a summary document of your learnings about data visualization, including key takeaways and areas for further exploration. [CITATION: Context insufficient for specific summary prompt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8bc6a04d81459abbc0ae7fd991213b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1375eeff6ea4a3a93f2e271ca64d6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "#### Day 1 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the differences between supervised and unsupervised learning. Focus on definitions and examples of each type. [CITATION: Not provided]\n",
      "- **Task 2 (est. 10–20 min)** — Read through the slides on Measures of Dispersion & Central Limit Theorem to reinforce your understanding of key statistical concepts that underpin supervised learning. [CITATION: 9]\n",
      "\n",
      "#### Day 2 — Apply\n",
      "- **Task 1 (est. 30 min)** — Complete coding exercises in the `svm.ipynb` lab, focusing on implementing a Support Vector Machine (SVM) model. Experiment with both linear and non-linear decision boundaries. [CITATION: 2]\n",
      "- **Task 2 (est. 20 min)** — Work on a coding exercise that involves implementing Laplace Smoothing in the `naive_bayes.ipynb` lab. [CITATION: 3]\n",
      "\n",
      "#### Day 3 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Revisit the definitions of ordinal, nominal, and binary data types from the feature engineering lab. Take notes on how these types relate to supervised and unsupervised learning. [CITATION: 5]\n",
      "- **Task 2 (est. 10–20 min)** — Go over the types of visualizations and their applications in data science. Reflect on how these visualizations can help in understanding model outputs. [CITATION: 6]\n",
      "\n",
      "#### Day 4 — Apply\n",
      "- **Task 1 (est. 30 min)** — Engage in a coding exercise that involves dummy-encoding categorical variables using the `feature_engineering.ipynb` lab. Make sure to include `drop='first'` in your implementation. [CITATION: 4]\n",
      "- **Task 2 (est. 20 min)** — Create a small dataset and apply both supervised and unsupervised learning techniques to see how they differ in practice. Document your findings. [CITATION: Not provided]\n",
      "\n",
      "#### Day 5 — Reflect\n",
      "- **Task 1 (est. 15–25 min)** — Write a reflection on your understanding of the differences between supervised and unsupervised learning. Include examples of when to use each type. [CITATION: Not provided]\n",
      "- **Task 2 (est. 10–20 min)** — Discuss your reflections with a peer or mentor to gain additional insights and clarify any remaining questions. [CITATION: Not provided]\n",
      "\n",
      "#### Day 6 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Revisit the concepts of linear algebra and Principal Component Analysis (PCA) as they relate to unsupervised learning. Take notes on how these concepts are applied. [CITATION: 10]\n",
      "- **Task 2 (est. 10–20 min)** — Review the coding exercises from the previous labs, focusing on the implementation of SVM and Naive Bayes. Identify areas where you struggled and note them for further study. [CITATION: Not provided]\n",
      "\n",
      "#### Day 7 — Checkpoint\n",
      "- **Task 1 (est. 30 min)** — Complete a mini-quiz or small coding task that tests your understanding of supervised vs. unsupervised learning, including practical applications of both. [CITATION: Not provided]\n",
      "- **Task 2 (est. 20 min)** — Summarize your learning from the week in a short document or presentation, highlighting key concepts and personal insights. [CITATION: Not provided]\n",
      "\n",
      "### Notes:\n",
      "- Some tasks lack specific citations due to insufficient context provided in the original materials.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb61256a91f64bc5a70e63d0cbff7b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adebdecf66aa409caa430f782295fafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "#### Day 1 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the concept of p-values and confidence intervals. Take notes on their definitions and applications in statistical analysis. [CITATION: Not provided]\n",
      "- **Task 2 (est. 10–20 min)** — Read through the slide on latent variables and summarize how they can be inferred from observable variables. [CITATION: Slide: Types of Visualizations Review.pdf Page: 64.0]\n",
      "\n",
      "#### Day 2 — Apply\n",
      "- **Task 1 (est. 30–40 min)** — Complete the coding exercises in the `svm.ipynb` lab, focusing on exploring non-linear decision boundaries. [CITATION: Lab: svm.ipynb]\n",
      "- **Task 2 (est. 20–30 min)** — Implement a simple example of Laplace Smoothing in the `naive_bayes.ipynb` lab to handle zero counts in word frequencies. [CITATION: Lab: naive_bayes.ipynb]\n",
      "\n",
      "#### Day 3 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the concept of gradient descent and its role in updating weights in neural networks. Take notes on how the `gradient_descent_update` function works. [CITATION: Lab: neural_networks.ipynb]\n",
      "- **Task 2 (est. 10–20 min)** — Read about the differences between `GridSearchCV` and `RandomizedSearchCV` in hyperparameter tuning. Summarize the pros and cons of each method. [CITATION: Lab: hyperparameters.ipynb]\n",
      "\n",
      "#### Day 4 — Apply\n",
      "- **Task 1 (est. 30–40 min)** — Work on coding exercises related to K-Nearest Neighbors (kNN) from the slides. Implement a basic kNN algorithm and experiment with different values of k. [CITATION: Slide: Introduction to K-Nearest-Neighbors.pdf Page: 90.0]\n",
      "- **Task 2 (est. 20–30 min)** — Analyze the performance of kNN by calculating bias and variance for different k values. Document your findings. [CITATION: Slide: Introduction to K-Nearest-Neighbors.pdf Page: 90.0]\n",
      "\n",
      "#### Day 5 — Reflect\n",
      "- **Task 1 (est. 15–25 min)** — Reflect on the challenges faced while applying statistical concepts like p-values and confidence intervals in practical scenarios. Write a short paragraph on your thoughts. [CITATION: Not provided]\n",
      "- **Task 2 (est. 10–20 min)** — Consider the implications of using Laplace Smoothing in Naive Bayes. Write down how it affects model performance and accuracy. [CITATION: Lab: naive_bayes.ipynb]\n",
      "\n",
      "#### Day 6 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the key concepts of neural networks, focusing on how they are structured and how they can be used for sentiment analysis. [CITATION: Slide: Introduction to Neural Networks.pdf Page: 83.0]\n",
      "- **Task 2 (est. 10–20 min)** — Revisit the differences in MSE between ridge regression and lasso regression as discussed in the hyperparameters lab. Summarize your understanding. [CITATION: Lab: hyperparameters.ipynb]\n",
      "\n",
      "#### Day 7 — Checkpoint\n",
      "- **Task 1 (est. 30 min)** — Complete a mini-quiz covering the topics of p-values, confidence intervals, gradient descent, and kNN. Include questions that require both theoretical understanding and practical application. [CITATION: Not provided]\n",
      "- **Task 2 (est. 20 min)** — Implement a small coding task that combines elements from the labs, such as using kNN to classify data and evaluating its performance using MSE. [CITATION: Not provided]\n",
      "\n",
      "### Note\n",
      "Some tasks lack specific citations due to insufficient context provided in the original materials.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9689800bade4f9db309c2e77c6f2a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eadb304ce094b1b9f7a1e5289d7c567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Day 1 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the basics of Python functions and loops. Focus on the control flow section from the Python refresher lab. Take notes on how loops and functions are structured and used. [CITATION: Lab: python_refresher.ipynb]\n",
      "- **Task 2 (est. 10–20 min)** — Read through the SQL + Python slide to understand how Python interacts with SQL databases, particularly focusing on the use of SQLite. Take notes on key points. [CITATION: Slide: Advanced SQL I.pdf Page: 43.0]\n",
      "\n",
      "### Day 2 — Apply\n",
      "- **Task 1 (est. 20–30 min)** — Complete coding exercises related to functions and loops from the Python refresher lab. Implement at least two different types of loops (for and while) in your code. [CITATION: Lab: python_refresher.ipynb]\n",
      "- **Task 2 (est. 15–25 min)** — Write a simple SQL query using Python to select users where age is NULL, similar to the example provided in the lab. Test it in your SQLite environment. [CITATION: Lab: w9-class2.ipynb]\n",
      "\n",
      "### Day 3 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Revisit the Python refresher lab, focusing specifically on data structures like lists and dictionaries. Take detailed notes on their usage and differences. [CITATION: Lab: python_refresher.ipynb]\n",
      "- **Task 2 (est. 10–20 min)** — Review the slide on the Transformer Architecture, focusing on the attention mechanism and how it relates to Python implementations. Take notes on key concepts. [CITATION: Slide: NLP & Vector Embeddings.pdf Page: 63.0]\n",
      "\n",
      "### Day 4 — Apply\n",
      "- **Task 1 (est. 20–30 min)** — Create a Python dictionary that mimics a decision tree as described in the decision trees lab. Implement a simple decision-making process using this dictionary. [CITATION: Lab: decision_trees.ipynb]\n",
      "- **Task 2 (est. 15–25 min)** — Practice data gathering skills by simulating a data collection task. Write a Python script that queries a mock database and retrieves data. [CITATION: Lab: feature_engineering.ipynb]\n",
      "\n",
      "### Day 5 — Reflect\n",
      "- **Task 1 (est. 15–20 min)** — Write a reflection on what you learned about Python functions and loops. Discuss any challenges you faced and how you overcame them. \n",
      "- **Task 2 (est. 10–15 min)** — Reflect on your experience with SQL and Python integration. What aspects were clear, and what needs further review?\n",
      "\n",
      "### Day 6 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the advanced SQL concepts, focusing on common table expressions and window functions. Take notes on their definitions and use cases. [CITATION: Slide: Introduction to Structured Databases I (1).pdf Page: 62.0]\n",
      "- **Task 2 (est. 10–20 min)** — Revisit the applied web scraping slide, focusing on regular expressions and their application in data extraction. Take notes on how they can be used in Python. [CITATION: Slide: Applied Web Scraping I.pptx.pdf Page: 41.0]\n",
      "\n",
      "### Day 7 — Checkpoint\n",
      "- **Mini-quiz (est. 20–30 min)** — Create a small coding task that requires using functions, loops, and SQL queries. For example, write a Python function that retrieves users with a specific age from a database and processes that data into a list. Test your code and document any errors or improvements needed. \n",
      "\n",
      "This plan provides a structured approach to review, apply, and reflect on the key concepts of Python programming and SQL integration, tailored to address the student's feedback.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a7976b404f4a3b91b9b1164c8655ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a96cfb3ffb44588a850911775284d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "#### Day 1 — Understand\n",
      "- **Task 1 (est. 15–25 min)** — Review the concept of vector databases and RAGs, focusing on their importance in providing contextual information to models. Take notes on key points. [CITATION: 1]\n",
      "- **Task 2 (est. 10–20 min)** — Read about the transformer architecture and its applications beyond text, including image and audio data. Summarize the main ideas. [CITATION: 7]\n",
      "\n",
      "#### Day 2 — Apply\n",
      "- **Task 1 (est. 30–45 min)** — Complete the coding exercise in `vectors_rags.ipynb` to implement the SentenceTransformer and generate embeddings for a set of facts. [CITATION: 3]\n",
      "- **Task 2 (est. 20–30 min)** — Install the necessary libraries (`gensim` and `nltk`) as shown in `nlp_vectors.ipynb`, and explore basic functionalities of these libraries. [CITATION: 4]\n",
      "\n",
      "#### Day 3 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Revisit your notes on vector databases and RAGs. Reflect on how they enhance the performance of LLMs. [CITATION: 1]\n",
      "- **Task 2 (est. 10–20 min)** — Go over your summary of the transformer architecture and consider its implications for various data types. [CITATION: 7]\n",
      "\n",
      "#### Day 4 — Apply\n",
      "- **Task 1 (est. 30–45 min)** — Work on a coding exercise that involves visualizing embeddings using `matplotlib` as shown in `w3-class3.ipynb`. [CITATION: 2]\n",
      "- **Task 2 (est. 20–30 min)** — Explore the usage of agents in LLMs as described in the slides, and write a short paragraph on their pros and cons. [CITATION: 6]\n",
      "\n",
      "#### Day 5 — Reflect\n",
      "- **Task 1 (est. 15–25 min)** — Reflect on the challenges faced during the coding exercises. Write down any questions or areas of confusion regarding tokenization and stemming. [CITATION: Context not provided]\n",
      "- **Task 2 (est. 10–20 min)** — Consider how the knowledge of latent variables can be applied in your projects. Write a brief explanation. [CITATION: 8]\n",
      "\n",
      "#### Day 6 — Review\n",
      "- **Task 1 (est. 15–25 min)** — Review your reflections on tokenization and stemming. Identify specific areas where you need further clarification or practice. [CITATION: Context not provided]\n",
      "- **Task 2 (est. 10–20 min)** — Revisit the pros and cons of using agents in LLMs. Reflect on how this knowledge can impact your future projects. [CITATION: 6]\n",
      "\n",
      "#### Day 7 — Checkpoint\n",
      "- **Task 1 (est. 30 min)** — Complete a mini-quiz or a small coding task that incorporates vector databases, RAGs, and the transformer architecture. This could involve writing a simple script that utilizes embeddings and visualizes them. [CITATION: Context not provided]\n",
      "\n",
      "This plan is designed to help you gradually build your understanding and application of key concepts in natural language processing, particularly focusing on the areas you found overwhelming.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bfabc962c74f4b8e14b79ab67589a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77692ffd40a24adcbe96ebc79aaadddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "**Day 1 — Understand**\n",
      "- Task 1 (est. 15–25 min) — Review the concepts of data cleaning and preprocessing, focusing on the importance of consistency in datasets and the differences between data cleaning and data validation. Take notes on key points. [CITATION: 4]\n",
      "- Task 2 (est. 10–20 min) — Read through the slides on Types of Visualizations to understand latent variables and their significance in data analysis. Note how they can be inferred from measurable variables. [CITATION: 6]\n",
      "\n",
      "**Day 2 — Apply**\n",
      "- Task 1 (est. 30–45 min) — Complete coding exercises related to data cleaning using the vectors_rags.ipynb lab. Focus on identifying and correcting inconsistencies in a sample dataset. [CITATION: 1]\n",
      "- Task 2 (est. 20–30 min) — Work on a coding task that involves implementing data validation techniques in a provided dataset. [CITATION: 4]\n",
      "\n",
      "**Day 3 — Review**\n",
      "- Task 1 (est. 15–25 min) — Revisit the notes taken on data cleaning and preprocessing. Summarize the key takeaways in your own words. [CITATION: 4]\n",
      "- Task 2 (est. 10–20 min) — Review the concepts of impurity metrics and feature importance from the random_forests.ipynb lab. Reflect on how these concepts relate to data cleaning. [CITATION: 3]\n",
      "\n",
      "**Day 4 — Apply**\n",
      "- Task 1 (est. 30–45 min) — Engage in coding exercises from the svm.ipynb lab, focusing on exploring non-linear decision boundaries. [CITATION: 2]\n",
      "- Task 2 (est. 20–30 min) — Implement a small project that requires you to clean a dataset and prepare it for analysis, applying the techniques learned. [CITATION: 4]\n",
      "\n",
      "**Day 5 — Reflect**\n",
      "- Task 1 (est. 15–25 min) — Write a reflection on the challenges faced during data cleaning and preprocessing. What strategies worked well, and what could be improved? [CITATION: 4]\n",
      "- Task 2 (est. 10–20 min) — Discuss with a peer or mentor about the importance of data validation and cleaning in data science projects. [CITATION: 4]\n",
      "\n",
      "**Day 6 — Review**\n",
      "- Task 1 (est. 15–25 min) — Go over the notes from the previous days, focusing on the coding exercises and the importance of data cleaning. Summarize your learning. [CITATION: 4]\n",
      "- Task 2 (est. 10–20 min) — Review the concept of Laplace Smoothing from the naive_bayes.ipynb lab and its application in handling zero counts in datasets. [CITATION: 5]\n",
      "\n",
      "**Day 7 — Checkpoint**\n",
      "- Task 1 (est. 30 min) — Complete a mini-quiz that tests your understanding of data cleaning, preprocessing, and validation concepts. [CITATION: 4]\n",
      "- Task 2 (est. 30 min) — Perform a small coding task that requires you to apply data cleaning techniques to a new dataset, ensuring it is ready for analysis. [CITATION: 4] \n",
      "\n",
      "This plan provides a structured approach to mastering data cleaning and preprocessing, with a balance of review, application, and reflection.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4cdbd6b02246278754d24de181ff9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044606e184de4967966df437188932d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 7-Day Micro-Task Plan\n",
      "\n",
      "**Day 1 — Understand**\n",
      "- Task 1 (est. 15–25 min) — Review the slides on K-Nearest Neighbors, focusing on the concepts of \"Who is my neighbor?\" and \"What is a Manhattan distance?\" [CITATION: Slide: Introduction to K-Nearest-Neighbors.pdf Page: 90.0]\n",
      "- Task 2 (est. 10–20 min) — Read about clustering algorithms, specifically K-means and hierarchical clustering, and take notes on their differences and applications. [CITATION: Context from course materials]\n",
      "\n",
      "**Day 2 — Apply**\n",
      "- Task 1 (est. 30 min) — Complete coding exercises related to K-means clustering from the lab materials, focusing on implementing the algorithm and visualizing the clusters. [CITATION: Lab: vectors_rags.ipynb]\n",
      "- Task 2 (est. 20 min) — Modify the number of clusters (k) in your K-means implementation and observe the changes in clustering outcomes. Document your observations. [CITATION: Lab: lab.ipynb]\n",
      "\n",
      "**Day 3 — Review**\n",
      "- Task 1 (est. 15–25 min) — Revisit the notes taken on K-means and hierarchical clustering, and summarize the key points in your own words. [CITATION: Context from course materials]\n",
      "- Task 2 (est. 10–20 min) — Reflect on the questions provided in the lab regarding clustering: How does random initialization affect the final clusters? [CITATION: Lab: lab.ipynb]\n",
      "\n",
      "**Day 4 — Apply**\n",
      "- Task 1 (est. 30 min) — Work on coding exercises related to hierarchical clustering, implementing the algorithm and comparing it with K-means results. [CITATION: Context from course materials]\n",
      "- Task 2 (est. 20 min) — Experiment with feature scaling in your clustering algorithms and note how it impacts the clustering outcomes. [CITATION: Lab: lab.ipynb]\n",
      "\n",
      "**Day 5 — Reflect**\n",
      "- Task 1 (est. 15–25 min) — Write a reflection on your experiences with K-means and hierarchical clustering, focusing on what you found challenging and what you learned. [CITATION: Context from course materials]\n",
      "- Task 2 (est. 10–20 min) — Discuss with a peer or in a study group about the differences between K-means and hierarchical clustering, and share your reflections. [CITATION: Context from course materials]\n",
      "\n",
      "**Day 6 — Review**\n",
      "- Task 1 (est. 15–25 min) — Revisit your notes on K-means and hierarchical clustering, focusing on the impact of the number of clusters (k) and feature scaling. [CITATION: Context from course materials]\n",
      "- Task 2 (est. 10–20 min) — Reflect on the question: What changes do you observe when you modify the number of clusters (k)? Write down your thoughts. [CITATION: Lab: lab.ipynb]\n",
      "\n",
      "**Day 7 — Checkpoint**\n",
      "- Task 1 (est. 30 min) — Complete a mini-quiz or small coding task that tests your understanding of K-means and hierarchical clustering, including their implementations and differences. [CITATION: Context from course materials]\n",
      "- Task 2 (est. 20 min) — Prepare a brief presentation summarizing your learnings about clustering algorithms, focusing on K-means and hierarchical clustering, and share it with your peers. [CITATION: Context from course materials]\n"
     ]
    }
   ],
   "source": [
    "# Define feedback sample (easy, medium, hard and noisy)\n",
    "# AI was able to assist us with generating 10 diverse feedback examples quickly.\n",
    "# To ensure that the examples fit the categories of easy, medium, hard and noisy we will have to create new ones and/ edit the ones provided for us.\n",
    "# We will use these for now just to test the functionality of the code. \n",
    "\n",
    "Test_feedbacks = [\n",
    "    \"I lost points on SQL joins and I keep mixing up inner vs left vs right joins.\",\n",
    "    \"I struggled with the concepts of overfitting and regularization in our machine learning assignments.\",\n",
    "    \"The sections on neural networks and backpropagation were really hard for me to grasp.\",\n",
    "    \"I found the data visualization part confusing, especially when to use different types of charts.\",\n",
    "    \"I had trouble understanding the differences between supervised and unsupervised learning.\",\n",
    "    \"The statistical concepts like p-values and confidence intervals were challenging to apply in practice.\",\n",
    "    \"I got lost in the details of Python programming, particularly with functions and loops.\",\n",
    "    \"The project on natural language processing was overwhelming, especially tokenization and stemming.\",\n",
    "    \"I found it difficult to follow the steps in data cleaning and preprocessing for our datasets.\",\n",
    "    \"The explanations about clustering algorithms like K-means and hierarchical clustering were unclear.\"\n",
    "]\n",
    "\n",
    "# Loop through the test feedbacks and generate plans\n",
    "for feedback in enumerate(Test_feedbacks):\n",
    "    plan = generate_7_day_plan(\n",
    "        feedback,\n",
    "        top_k_slides=5, \n",
    "        top_k_labs=5, \n",
    "        model_name=\"gpt-4o-mini\")\n",
    "    \n",
    "    print(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff36218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Topic \n",
    "#Topic_keywords = {\n",
    "#    \"sql\": [\"join\", \"left join \", \"right join\", \"inner join\", \"outer join\", \"select\", \"from\", \"where\", \"group by\", \"order by\", \"having\", \"union\", \"intersect\", \"except\", \"subquery\", \"cte\", \"window function\"],\n",
    "#    \"loops\": [\"for loop\", \"while loop\", \"do while loop\", \"nested loop\", \"break\", \"continue\", \"infinite loop\", \"loop control\"]\n",
    "\n",
    "#    def detect_topic(query):\n",
    "#        result = []\n",
    "#         for topic, keywords in Topic_keywords.items():\n",
    "#            for keyword in keywords:\n",
    "#                if keyword in query.lower():\n",
    "#                    result.append(topic)\n",
    "#        return result       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
