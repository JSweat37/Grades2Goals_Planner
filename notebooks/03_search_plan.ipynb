{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af13f703",
   "metadata": {},
   "source": [
    "the goal of thois notebook is to \n",
    "pull from faiss and connect to llm to output 7 day plan \n",
    "\n",
    "Implement semantic search (topk) and a simple 7-day plan composer that attaches citations from chunks.\n",
    "\n",
    "LLM planner\n",
    "\n",
    "use openai that can\n",
    "1. take a query (student's input)\n",
    "2. search indicies using (search_slides and search_labs)\n",
    "3. format results with citations \n",
    "4. call the chat model using api key\n",
    "5. gen 7day  plan \n",
    "6. save the plan to JSON/CSV \n",
    "\n",
    "**Inputs**\n",
    "- `../data/processed/slides_chunks.parquet`\n",
    "- `../data/processed/labs_chunks.parquet`\n",
    "- `../data/processed/faiss_slides.index`\n",
    "- `../data/processed/faiss_labs.index`\n",
    "\n",
    "\n",
    "- `../data/processed/faiss_labs.index`\n",
    "\n",
    "**Outputs**\n",
    "- Printed study plan in the notebook\n",
    "- JSON file of the plan for saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84587bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ea6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides chunks: True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\slides_chunks.parquet\n",
      "Labs chunks:   True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\labs_chunks.parquet\n",
      "Slides index:  True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\faiss_slides.index\n",
      "Labs index:    True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\faiss_labs.index\n"
     ]
    }
   ],
   "source": [
    "# Load DAta\n",
    "# Folders\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "PROCESSED_FOLDER = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "SLIDES_CHUNKS_PATH = PROCESSED_FOLDER / \"slides_chunks.parquet\"\n",
    "LABS_CHUNKS_PATH   = PROCESSED_FOLDER / \"labs_chunks.parquet\"\n",
    "FAISS_SLIDES_PATH  = PROCESSED_FOLDER / \"faiss_slides.index\"\n",
    "FAISS_LABS_PATH    = PROCESSED_FOLDER / \"faiss_labs.index\"\n",
    "\n",
    "\n",
    "print(\"Slides chunks:\", SLIDES_CHUNKS_PATH.exists(), SLIDES_CHUNKS_PATH)\n",
    "print(\"Labs chunks:  \", LABS_CHUNKS_PATH.exists(),   LABS_CHUNKS_PATH)\n",
    "print(\"Slides index: \", FAISS_SLIDES_PATH.exists(),  FAISS_SLIDES_PATH)\n",
    "print(\"Labs index:   \", FAISS_LABS_PATH.exists(),    FAISS_LABS_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41d4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides rows: 40\n",
      "Labs rows:   1410\n"
     ]
    }
   ],
   "source": [
    "# Load chunks \n",
    "slides_df = pd.read_parquet(\"../data/processed/slides_chunks.parquet\")\n",
    "labs_df   = pd.read_parquet(\"../data/processed/labs_chunks.parquet\")\n",
    "\n",
    "# Drop empty rows\n",
    "slides_df = slides_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "labs_df   = labs_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "print(\"Slides rows:\", len(slides_df))\n",
    "print(\"Labs rows:  \", len(labs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b814a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS indexes from disk\n",
    "slides_index = faiss.read_index(FAISS_SLIDES_PATH.as_posix())\n",
    "labs_index   = faiss.read_index(FAISS_LABS_PATH.as_posix())# LLM call (OpenAI) to build a 7-day plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a059f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "\n",
    "def encode_normalized(texts):\n",
    "    \"\"\"Convert a single query string (what the student types) into a normalized float32 vector.\n",
    "    - normalize_embeddings=True ensures vectors have length 1, so FAISS inner product ≈ cosine similarity.\n",
    "    - We return a NumPy array with dtype float32 because FAISS expects float32 vectors.\"\"\"\n",
    "    embeddings = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(\"Embedding model loaded:\", embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87de2254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2acd90facc4f7988b6d136a4072ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a50b4901094e6b93730765d4a387e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides index size: 40\n",
      "Labs index size: 1410\n"
     ]
    }
   ],
   "source": [
    "# Create 2 FAISS indices, one for slides and one for labs\n",
    "\n",
    "# Slides index\n",
    "slides_embeddings = encode_normalized(slides_df['text'].tolist())\n",
    "slides_index = faiss.IndexFlatIP(slides_embeddings.shape[1])\n",
    "slides_index.add(slides_embeddings)\n",
    "faiss.write_index(slides_index, FAISS_SLIDES_PATH.as_posix())\n",
    "\n",
    "# Labs index\n",
    "labs_embeddings = encode_normalized(labs_df['text'].tolist())\n",
    "labs_index = faiss.IndexFlatIP(labs_embeddings.shape[1])\n",
    "labs_index.add(labs_embeddings)\n",
    "faiss.write_index(labs_index, FAISS_LABS_PATH.as_posix())\n",
    "\n",
    "print(\"Slides index size:\", slides_index.ntotal)\n",
    "print(\"Labs index size:\", labs_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44485042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_slides(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the slides index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the slides index.\n",
    "    3) For each match, look up the original row in slides_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = slides_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from slides_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = slides_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"slide\",\n",
    "            \"file\": row['file'], # slide filename\n",
    "            \"page\": row['page'], # slide page number\n",
    "            \"text\": row['text'], # slide text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbca334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_labs(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the labs index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the labs index.\n",
    "    3) For each match, look up the original row in labs_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = labs_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from labs_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = labs_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"lab\",\n",
    "            \"file\": row['file'], # lab filename\n",
    "            \"text\": row['text'], # lab text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c29779",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)  # take environment variables from .env file\n",
    "\n",
    "# Create a single OpenAI client (reads key from environment)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def generate_7_day_plan(feedback_text, top_k_slides: 5, top_k_labs: 5, model_name: \"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    End-to-end:\n",
    "    1) Search slides and labs separately.\n",
    "    2) Concatenate the results (labs first by default since they are practical).\n",
    "    3) Build a single context block with simple citations.\n",
    "    4) Call OpenAI to write a 7-day plan with spaced review.\n",
    "    5) Return the generated text.\n",
    "\n",
    "    Args:\n",
    "        feedback_text (str): Student's feedback, e.g., \"I lost points on SQL joins and confusion matrix.\"\n",
    "        top_k_slides (int): Number of slide chunks to include.\n",
    "        top_k_labs   (int): Number of lab chunks to include.\n",
    "        model_name   (str): OpenAI chat model.\n",
    "\n",
    "    Returns:\n",
    "        str: The study plan text generated by the LLM.\n",
    "    \"\"\"\n",
    "    # Search slides and labs\n",
    "    slides_results = search_slides(feedback_text, top_k=top_k_slides)\n",
    "    labs_results   = search_labs(feedback_text, top_k=top_k_labs)\n",
    "\n",
    "    # Combine results \n",
    "    combined_results = pd.concat([labs_results, slides_results], ignore_index=True)\n",
    "\n",
    "    # Build context block with citations\n",
    "    context_blocks = []\n",
    "    for i, row in combined_results.iterrows():\n",
    "        citation = f\"[{i+1}]\"\n",
    "        if row['source_type'] == 'slide':\n",
    "            context_blocks.append(f\"{citation} (Slide: {row['file']} Page: {row['page']}) {row['text']}\")\n",
    "        else:\n",
    "            context_blocks.append(f\"{citation} (Lab: {row['file']}) {row['text']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks) # double newline for readability\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are an academic coach for a data science course. \"\n",
    "        \"You must create a concrete 7-day micro-task plan using ONLY the provided context. \"\n",
    "        \"Ensure tasks alternate between review (reading/notes), application (coding exercises), and reflection.\"\n",
    "        \"Each day should include 2–4 actionable tasks, with estimated time, and a citation line that points back to the source. \"\n",
    "        \"Use spaced review on Day 1, Day 3, and Day 6. \"\n",
    "        \"If context is insufficient for any part, state that clearly.\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Student feedback: {feedback_text}\\n\\n\"\n",
    "        f\"Context from course materials (slides and labs):\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"Now write the 7-day plan in this structure:\\n\"\n",
    "        \"Day 1 — Understand\\n\"\n",
    "        \"- Task 1 (est. 15–25 min) — description [CITATION]\\n\"\n",
    "        \"- Task 2 (est. 10–20 min) — description [CITATION]\\n\"\n",
    "        \"Day 2 — Apply\\n\"\n",
    "        \"- Task 1 ...\\n\"\n",
    "        \"...\\n\"\n",
    "        \"Day 7 — Checkpoint\\n\"\n",
    "        \"- Mini-quiz or small coding task ...\\n\"\n",
    "        \"\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Use only facts available in the context above.\\n\"\n",
    "        \"- Each task line should end with a [CITATION] using the [SOURCE: ...] entry from context.\\n\"\n",
    "        \"- If something is unclear or missing, say so.\\n\"\n",
    "    )\n",
    "\n",
    "    # Call OpenAI chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2,  # low temperature for focused output\n",
    "        max_tokens=1000   # adjust as needed\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4406a062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724fa1e14d4f455c88471f05ae9fa435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cc5f935e234a1882e7b1a004418c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review SQL Joins\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Study Inner Join vs Left Join vs Right Join\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Practice SQL Joins with Sample Data\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Complete SQL Join Exercises\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review SQL Keywords and Clauses\",\"est_mins\":25,\"citation_url\":\"[5]\"},{\"title\":\"Practice SELECT statements with WHERE clause\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Work on SQL Join Scenarios\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Analyze SQL Join Results\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Review SQL Data Manipulation Language (DML)\",\"est_mins\":25,\"citation_url\":\"[5]\"},{\"title\":\"Practice INSERT, UPDATE, DELETE commands\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review SQL Join Types and Use Cases\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Complete SQL Join Practice Problems\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Checkpoint Quiz on SQL Joins\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":15,\"citation_url\":\"\"}]}]}}\n"
     ]
    }
   ],
   "source": [
    "example_feedback = \"I lost points on SQL joins and I keep mixing up inner vs left vs right joins.\"\n",
    "plan_text = generate_7_day_plan(example_feedback, top_k_slides=4, top_k_labs=6, model_name=\"gpt-4o-mini\")\n",
    "print(plan_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddfac8",
   "metadata": {},
   "source": [
    "# Prototype Eval (test cases)\n",
    "\n",
    "Evaluating the prototype on at least 10 examples to check:\n",
    "- Topic Correctness\n",
    "- Citation Correctness\n",
    "- Format Correctness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5dcc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b9639876d7462db082b782a9f9d898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe6423095454c0e82452883e967499d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review SQL Joins\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Understand Inner vs Left vs Right Joins\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Practice SQL Joins with Sample Data\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Complete SQL Join Exercises\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review SQL Query Syntax\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Review Common SQL Functions\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Work on SQL Join Problems\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Analyze SQL Join Results\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Explore Advanced SQL Concepts\",\"est_mins\":30,\"citation_url\":\"[SOURCE: Slide: Introduction to Structured Databases I (1).pdf Page: 62.0]\"},{\"title\":\"Practice SQL with Python\",\"est_mins\":30,\"citation_url\":\"[SOURCE: Slide: Introduction to Structured Databases I (1).pdf Page: 62.0]\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review SQL Join Types\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Reflect on SQL Join Mistakes\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Checkpoint Quiz on SQL Joins\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":30,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cc810a69574cff9797ae32f447883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a184bf65abbf4b468c3acbf13c807ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review Overfitting and Regularization Concepts\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Explore Gradient Descent Mechanism\",\"est_mins\":20,\"citation_url\":\"[3]\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Implement a Simple Neural Network\",\"est_mins\":30,\"citation_url\":\"[4]\"},{\"title\":\"Experiment with Gradient Descent Updates\",\"est_mins\":25,\"citation_url\":\"[3]\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review Clustering Algorithms\",\"est_mins\":20,\"citation_url\":\"\"},{\"title\":\"Analyze K-Means Functionality\",\"est_mins\":25,\"citation_url\":\"[5]\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Apply PCA to Reduce Dimensions\",\"est_mins\":30,\"citation_url\":\"[2]\"},{\"title\":\"Run K-Means with Different K Values\",\"est_mins\":25,\"citation_url\":\"[5]\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Study Decision Trees and Their Applications\",\"est_mins\":25,\"citation_url\":\"[9]\"},{\"title\":\"Review Ensemble Methods in Machine Learning\",\"est_mins\":20,\"citation_url\":\"\"}]}},{\"day\":6,\"tasks\":[{\"title\":\"Review Transformer Architecture\",\"est_mins\":25,\"citation_url\":\"[8]\"},{\"title\":\"Discuss the Role of Latent Variables\",\"est_mins\":20,\"citation_url\":\"[7]\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Checkpoint Quiz on Overfitting and Regularization\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":20,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed14edde44614053bd28832e01b6fd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc0df3c47eb4a35a11ca8eae9ed8078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review Neural Networks Concepts\",\"est_mins\":25,\"citation_url\":\"[9]\"},{\"title\":\"Explore Backpropagation Overview\",\"est_mins\":20,\"citation_url\":\"https://www.ibm.com/think/topics/backpropagation\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Implement a Simple Neural Network using Keras\",\"est_mins\":30,\"citation_url\":\"[4]\"},{\"title\":\"Experiment with Different Activation Functions\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review Non-linear Decision Boundaries\",\"est_mins\":25,\"citation_url\":\"[3]\"},{\"title\":\"Understand the Role of Latent Variables\",\"est_mins\":20,\"citation_url\":\"[6]\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Practice Feature Engineering Techniques\",\"est_mins\":30,\"citation_url\":\"[5]\"},{\"title\":\"Analyze the Transformer Architecture\",\"est_mins\":25,\"citation_url\":\"[8]\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Explore Applications of LLMs and Agents\",\"est_mins\":30,\"citation_url\":\"[7]\"},{\"title\":\"Review the Importance of Data Management\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review Key Concepts from Neural Networks\",\"est_mins\":25,\"citation_url\":\"[9]\"},{\"title\":\"Reflect on the Implementation of Backpropagation\",\"est_mins\":20,\"citation_url\":\"[4]\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Mini-Quiz on Neural Networks and Backpropagation\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":20,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f669182f143d4eb99bf49c650f3a64f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c3d7caaa4841ecbebebff6e9361add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review Types of Visualizations\",\"est_mins\":25,\"citation_url\":\"6\"},{\"title\":\"Explore Data Visualization Techniques\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Practice with Vectors in Data Visualization\",\"est_mins\":30,\"citation_url\":\"1\"},{\"title\":\"Implement Non-linear Decision Boundaries\",\"est_mins\":30,\"citation_url\":\"2\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review Laplace Smoothing in Naive Bayes\",\"est_mins\":25,\"citation_url\":\"4\"},{\"title\":\"Analyze Errors in Neural Networks\",\"est_mins\":20,\"citation_url\":\"5\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Solve Leetcode Problems Related to Data Visualization\",\"est_mins\":30,\"citation_url\":\"3\"},{\"title\":\"Study Transformer Architecture Applications\",\"est_mins\":30,\"citation_url\":\"8\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Understand Advanced Control Flow in Data Science\",\"est_mins\":30,\"citation_url\":\"9\"},{\"title\":\"Review Control Flow Concepts\",\"est_mins\":30,\"citation_url\":\"10\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review Agents and Their Usage in LLMs\",\"est_mins\":25,\"citation_url\":\"7\"},{\"title\":\"Reflect on Data Visualization Challenges\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Checkpoint Quiz on Data Visualization\",\"est_mins\":30,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bf1c53429b489e818b0bb8cbe525ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16273856da384518a4b195ab055d1c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review Supervised vs Unsupervised Learning\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Explore Examples of Supervised Learning\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Complete Lab on SVM\",\"est_mins\":30,\"citation_url\":\"Lab: svm.ipynb\"},{\"title\":\"Practice Non-linear Decision Boundaries\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review Naive Bayes and Laplace Smoothing\",\"est_mins\":25,\"citation_url\":\"Lab: naive_bayes.ipynb\"},{\"title\":\"Understand Feature Engineering Concepts\",\"est_mins\":20,\"citation_url\":\"Lab: feature_engineering.ipynb\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Implement Dummy-Encoding in a Sample Dataset\",\"est_mins\":30,\"citation_url\":\"Lab: feature_engineering.ipynb\"},{\"title\":\"Explore Ordinal vs Nominal Values\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Review Measures of Dispersion & Central Limit Theorem\",\"est_mins\":30,\"citation_url\":\"Slide: Measures of Dispersion & Central Limit Theorem.pdf Page: 67.0\"},{\"title\":\"Prepare Questions for TLAB 1 Review\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review Random Forests Concepts\",\"est_mins\":25,\"citation_url\":\"Slide: Random Forests.pdf Page: 60.0\"},{\"title\":\"Explore Unsupervised Machine Learning\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Checkpoint Quiz on Supervised vs Unsupervised Learning\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":20,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c7fdf471bd4c809f88ae1186dfcf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679a0b6f9f5b49e0b42845f0954d4c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review p-values and confidence intervals\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Explore non-linear decision boundaries\",\"est_mins\":20,\"citation_url\":\"Lab: svm.ipynb\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Practice applying p-values in a dataset\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Implement gradient descent in a simple model\",\"est_mins\":25,\"citation_url\":\"Lab: neural_networks.ipynb\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review the concept of Laplace Smoothing\",\"est_mins\":20,\"citation_url\":\"Lab: naive_bayes.ipynb\"},{\"title\":\"Understand the differences between GridSearchCV and RandomizedSearchCV\",\"est_mins\":25,\"citation_url\":\"Lab: hyperparameters.ipynb\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Apply Laplace Smoothing to a dataset\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Experiment with hyperparameters in a model\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Review K-Nearest Neighbors concepts\",\"est_mins\":20,\"citation_url\":\"Slide: Introduction to K-Nearest-Neighbors.pdf\"},{\"title\":\"Understand variance and bias in kNN\",\"est_mins\":25,\"citation_url\":\"Slide: Introduction to K-Nearest-Neighbors.pdf\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review neural network training concepts\",\"est_mins\":25,\"citation_url\":\"Slide: Introduction to Neural Networks.pdf\"},{\"title\":\"Reflect on the application of statistical concepts in practice\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete a mini-quiz on statistical concepts and models\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on learning and areas for improvement\",\"est_mins\":15,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83635583697e4e8d9d7d1a4b3e0cbbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b75307bb164f918842c2949620f25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review Control Flow: Conditionals & Loops\",\"est_mins\":25,\"citation_url\":\"4\"},{\"title\":\"Practice Basic Arithmetic & Variables\",\"est_mins\":15,\"citation_url\":\"4\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Implement Functions in Python\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Work on Loops with Sample Data\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review Data Structures: Lists, Tuples, Sets, Dicts\",\"est_mins\":25,\"citation_url\":\"4\"},{\"title\":\"Practice Creating and Using Functions\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Explore SQL Queries with Python\",\"est_mins\":30,\"citation_url\":\"6\"},{\"title\":\"Practice Data Gathering Skills\",\"est_mins\":20,\"citation_url\":\"2\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Learn about Decision Trees in Python\",\"est_mins\":30,\"citation_url\":\"3\"},{\"title\":\"Implement a Simple Decision Tree\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review Python Refresher Topics\",\"est_mins\":25,\"citation_url\":\"4\"},{\"title\":\"Practice with Advanced SQL Concepts\",\"est_mins\":20,\"citation_url\":\"9\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Mini-quiz on Python Functions and Loops\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":15,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe256530e171481eb15236cb325f5bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a07ed4cb7547b1a57103566ea27103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review Tokenization and Stemming Concepts\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Explore Vector Databases and RAGs\",\"est_mins\":20,\"citation_url\":\"[SOURCE: 1]\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Implement Tokenization in Python\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Practice Stemming with NLTK\",\"est_mins\":25,\"citation_url\":\"[SOURCE: 4]\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review the Role of Sentence Transformers\",\"est_mins\":25,\"citation_url\":\"[SOURCE: 3]\"},{\"title\":\"Understand the Importance of Contextual Information in LLMs\",\"est_mins\":20,\"citation_url\":\"[SOURCE: 1]\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Experiment with SentenceTransformer for Text Embeddings\",\"est_mins\":30,\"citation_url\":\"[SOURCE: 3]\"},{\"title\":\"Learn about the Transformer Architecture\",\"est_mins\":25,\"citation_url\":\"[SOURCE: 7]\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Explore Advanced Control Flow in Data Science\",\"est_mins\":30,\"citation_url\":\"[SOURCE: 9]\"},{\"title\":\"Review Types of Visualizations in NLP\",\"est_mins\":25,\"citation_url\":\"[SOURCE: 8]\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review Agents and Their Usage in LLMs\",\"est_mins\":25,\"citation_url\":\"[SOURCE: 6]\"},{\"title\":\"Understand the Pros and Cons of Using RAGs\",\"est_mins\":20,\"citation_url\":\"[SOURCE: 6]\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete a Mini-Quiz on NLP Concepts\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":20,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248a852d358c43afa02823157a748746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0845094ba1e2446ab56ab5414eb69670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Understand Data Cleaning Concepts\",\"est_mins\":25,\"citation_url\":\"[4]\"},{\"title\":\"Review Data Validation vs. Data Cleaning\",\"est_mins\":15,\"citation_url\":\"[4]\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Explore Data Wrangling Tools\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Practice Basic Data Cleaning Techniques\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review Impurity Metrics in Data\",\"est_mins\":25,\"citation_url\":\"[3]\"},{\"title\":\"Understand Feature Importance Evaluation\",\"est_mins\":15,\"citation_url\":\"[3]\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Learn about Non-linear Decision Boundaries\",\"est_mins\":30,\"citation_url\":\"[2]\"},{\"title\":\"Implement a Simple SVM Model\",\"est_mins\":30,\"citation_url\":\"[2]\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Study Laplace Smoothing in Naive Bayes\",\"est_mins\":25,\"citation_url\":\"[5]\"},{\"title\":\"Practice with Word Frequency Counts\",\"est_mins\":25,\"citation_url\":\"[5]\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review Types of Visualizations\",\"est_mins\":25,\"citation_url\":\"[6]\"},{\"title\":\"Discuss Measures of Dispersion\",\"est_mins\":15,\"citation_url\":\"[9]\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Checkpoint Quiz on Data Cleaning and Preprocessing\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":15,\"citation_url\":\"\"}]}]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cd52b505ee41bca6042ce5d8ca820b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91d998afc74b9488ab87486e5d8d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review K-means Clustering\",\"est_mins\":25,\"citation_url\":\"[4]\"},{\"title\":\"Explore Hierarchical Clustering Concepts\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Implement K-means Algorithm\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Experiment with Different Values of k\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Reflect on Random Initialization Effects\",\"est_mins\":15,\"citation_url\":\"[3]\"},{\"title\":\"Analyze Feature Scaling Impact on Clustering\",\"est_mins\":20,\"citation_url\":\"[3]\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Study Non-linear Decision Boundaries\",\"est_mins\":30,\"citation_url\":\"[2]\"},{\"title\":\"Review Clustering Visualizations\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Explore K-Nearest Neighbors Basics\",\"est_mins\":30,\"citation_url\":\"[10]\"},{\"title\":\"Understand kNN Hyperparameters\",\"est_mins\":25,\"citation_url\":\"[9]\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Reflect on Clustering Outcomes\",\"est_mins\":20,\"citation_url\":\"[3]\"},{\"title\":\"Review Key Concepts of Clustering Algorithms\",\"est_mins\":25,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Complete Checkpoint Quiz on Clustering\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":20,\"citation_url\":\"\"}]}]}}\n"
     ]
    }
   ],
   "source": [
    "# Define feedback sample (easy, medium, hard and noisy)\n",
    "# AI was able to assist us with generating 10 diverse feedback examples quickly.\n",
    "# To ensure that the examples fit the categories of easy, medium, hard and noisy we will have to create new ones and/ edit the ones provided for us.\n",
    "# We will use these for now just to test the functionality of the code. \n",
    "\n",
    "Test_feedbacks = [\n",
    "    \"I lost points on SQL joins and I keep mixing up inner vs left vs right joins.\",\n",
    "    \"I struggled with the concepts of overfitting and regularization in our machine learning assignments.\",\n",
    "    \"The sections on neural networks and backpropagation were really hard for me to grasp.\",\n",
    "    \"I found the data visualization part confusing, especially when to use different types of charts.\",\n",
    "    \"I had trouble understanding the differences between supervised and unsupervised learning.\",\n",
    "    \"The statistical concepts like p-values and confidence intervals were challenging to apply in practice.\",\n",
    "    \"I got lost in the details of Python programming, particularly with functions and loops.\",\n",
    "    \"The project on natural language processing was overwhelming, especially tokenization and stemming.\",\n",
    "    \"I found it difficult to follow the steps in data cleaning and preprocessing for our datasets.\",\n",
    "    \"The explanations about clustering algorithms like K-means and hierarchical clustering were unclear.\"\n",
    "]\n",
    "\n",
    "# Loop through the test feedbacks and generate plans\n",
    "for feedback in enumerate(Test_feedbacks):\n",
    "    plan = generate_7_day_plan(\n",
    "        feedback,\n",
    "        top_k_slides=5, \n",
    "        top_k_labs=5, \n",
    "        model_name=\"gpt-4o-mini\")\n",
    "    \n",
    "    print(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Topic \n",
    "#Topic_keywords = {\n",
    "#    \"sql\": [\"join\", \"left join \", \"right join\", \"inner join\", \"outer join\", \"select\", \"from\", \"where\", \"group by\", \"order by\", \"having\", \"union\", \"intersect\", \"except\", \"subquery\", \"cte\", \"window function\"],\n",
    "#    \"loops\": [\"for loop\", \"while loop\", \"do while loop\", \"nested loop\", \"break\", \"continue\", \"infinite loop\", \"loop control\"]\n",
    "\n",
    "#    def detect_topic(query):\n",
    "#        result = []\n",
    "#         for topic, keywords in Topic_keywords.items():\n",
    "#            for keyword in keywords:\n",
    "#                if keyword in query.lower():\n",
    "#                    result.append(topic)\n",
    "#        return result       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
