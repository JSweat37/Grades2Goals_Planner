{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af13f703",
   "metadata": {},
   "source": [
    "the goal of thois notebook is to \n",
    "pull from faiss and connect to llm to output 7 day plan \n",
    "\n",
    "Implement semantic search (topk) and a simple 7-day plan composer that attaches citations from chunks.\n",
    "\n",
    "LLM planner\n",
    "\n",
    "use openai that can\n",
    "1. take a query (student's input)\n",
    "2. search indicies using (search_slides and search_labs)\n",
    "3. format results with citations \n",
    "4. call the chat model using api key\n",
    "5. gen 7day  plan \n",
    "6. save the plan to JSON/CSV \n",
    "\n",
    "**Inputs**\n",
    "- `../data/processed/slides_chunks.parquet`\n",
    "- `../data/processed/labs_chunks.parquet`\n",
    "- `../data/processed/faiss_slides.index`\n",
    "- `../data/processed/faiss_labs.index`\n",
    "\n",
    "\n",
    "- `../data/processed/faiss_labs.index`\n",
    "\n",
    "**Outputs**\n",
    "- Printed study plan in the notebook\n",
    "- JSON file of the plan for saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84587bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ea6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides chunks: True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\slides_chunks.parquet\n",
      "Labs chunks:   True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\labs_chunks.parquet\n",
      "Slides index:  True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\faiss_slides.index\n",
      "Labs index:    True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\faiss_labs.index\n"
     ]
    }
   ],
   "source": [
    "# Load DAta\n",
    "# Folders\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "PROCESSED_FOLDER = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "SLIDES_CHUNKS_PATH = PROCESSED_FOLDER / \"slides_chunks.parquet\"\n",
    "LABS_CHUNKS_PATH   = PROCESSED_FOLDER / \"labs_chunks.parquet\"\n",
    "FAISS_SLIDES_PATH  = PROCESSED_FOLDER / \"faiss_slides.index\"\n",
    "FAISS_LABS_PATH    = PROCESSED_FOLDER / \"faiss_labs.index\"\n",
    "\n",
    "\n",
    "print(\"Slides chunks:\", SLIDES_CHUNKS_PATH.exists(), SLIDES_CHUNKS_PATH)\n",
    "print(\"Labs chunks:  \", LABS_CHUNKS_PATH.exists(),   LABS_CHUNKS_PATH)\n",
    "print(\"Slides index: \", FAISS_SLIDES_PATH.exists(),  FAISS_SLIDES_PATH)\n",
    "print(\"Labs index:   \", FAISS_LABS_PATH.exists(),    FAISS_LABS_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41d4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides rows: 40\n",
      "Labs rows:   1410\n"
     ]
    }
   ],
   "source": [
    "# Load chunks \n",
    "slides_df = pd.read_parquet(\"../data/processed/slides_chunks.parquet\")\n",
    "labs_df   = pd.read_parquet(\"../data/processed/labs_chunks.parquet\")\n",
    "\n",
    "# Drop empty rows\n",
    "slides_df = slides_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "labs_df   = labs_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "print(\"Slides rows:\", len(slides_df))\n",
    "print(\"Labs rows:  \", len(labs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b814a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS indexes from disk\n",
    "slides_index = faiss.read_index(FAISS_SLIDES_PATH.as_posix())\n",
    "labs_index   = faiss.read_index(FAISS_LABS_PATH.as_posix())# LLM call (OpenAI) to build a 7-day plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a059f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "\n",
    "def encode_normalized(texts):\n",
    "    \"\"\"Convert a single query string (what the student types) into a normalized float32 vector.\n",
    "    - normalize_embeddings=True ensures vectors have length 1, so FAISS inner product ≈ cosine similarity.\n",
    "    - We return a NumPy array with dtype float32 because FAISS expects float32 vectors.\"\"\"\n",
    "    embeddings = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(\"Embedding model loaded:\", embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87de2254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2acd90facc4f7988b6d136a4072ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a50b4901094e6b93730765d4a387e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides index size: 40\n",
      "Labs index size: 1410\n"
     ]
    }
   ],
   "source": [
    "# Create 2 FAISS indices, one for slides and one for labs\n",
    "\n",
    "# Slides index\n",
    "slides_embeddings = encode_normalized(slides_df['text'].tolist())\n",
    "slides_index = faiss.IndexFlatIP(slides_embeddings.shape[1])\n",
    "slides_index.add(slides_embeddings)\n",
    "faiss.write_index(slides_index, FAISS_SLIDES_PATH.as_posix())\n",
    "\n",
    "# Labs index\n",
    "labs_embeddings = encode_normalized(labs_df['text'].tolist())\n",
    "labs_index = faiss.IndexFlatIP(labs_embeddings.shape[1])\n",
    "labs_index.add(labs_embeddings)\n",
    "faiss.write_index(labs_index, FAISS_LABS_PATH.as_posix())\n",
    "\n",
    "print(\"Slides index size:\", slides_index.ntotal)\n",
    "print(\"Labs index size:\", labs_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44485042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_slides(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the slides index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the slides index.\n",
    "    3) For each match, look up the original row in slides_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = slides_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from slides_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = slides_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"slide\",\n",
    "            \"file\": row['file'], # slide filename\n",
    "            \"page\": row['page'], # slide page number\n",
    "            \"text\": row['text'], # slide text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbca334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_labs(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the labs index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the labs index.\n",
    "    3) For each match, look up the original row in labs_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = labs_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from labs_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = labs_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"lab\",\n",
    "            \"file\": row['file'], # lab filename\n",
    "            \"text\": row['text'], # lab text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c29779",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)  # take environment variables from .env file\n",
    "\n",
    "# Create a single OpenAI client (reads key from environment)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def generate_7_day_plan(feedback_text, top_k_slides: 5, top_k_labs: 5, model_name: \"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    End-to-end:\n",
    "    1) Search slides and labs separately.\n",
    "    2) Concatenate the results (labs first by default since they are practical).\n",
    "    3) Build a single context block with simple citations.\n",
    "    4) Call OpenAI to write a 7-day plan with spaced review.\n",
    "    5) Return the generated text.\n",
    "\n",
    "    Args:\n",
    "        feedback_text (str): Student's feedback, e.g., \"I lost points on SQL joins and confusion matrix.\"\n",
    "        top_k_slides (int): Number of slide chunks to include.\n",
    "        top_k_labs   (int): Number of lab chunks to include.\n",
    "        model_name   (str): OpenAI chat model.\n",
    "\n",
    "    Returns:\n",
    "        str: The study plan text generated by the LLM.\n",
    "    \"\"\"\n",
    "    # Search slides and labs\n",
    "    slides_results = search_slides(feedback_text, top_k=top_k_slides)\n",
    "    labs_results   = search_labs(feedback_text, top_k=top_k_labs)\n",
    "\n",
    "    # Combine results \n",
    "    combined_results = pd.concat([labs_results, slides_results], ignore_index=True)\n",
    "\n",
    "    # Build context block with citations\n",
    "    context_blocks = []\n",
    "    for i, row in combined_results.iterrows():\n",
    "        citation = f\"[{i+1}]\"\n",
    "        if row['source_type'] == 'slide':\n",
    "            context_blocks.append(f\"{citation} (Slide: {row['file']} Page: {row['page']}) {row['text']}\")\n",
    "        else:\n",
    "            context_blocks.append(f\"{citation} (Lab: {row['file']}) {row['text']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks) # double newline for readability\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are an academic coach for a data science course. \"\n",
    "        \"You must create a concrete 7-day micro-task plan using ONLY the provided context. \"\n",
    "        \"Ensure tasks alternate between review (reading/notes), application (coding exercises), and reflection.\"\n",
    "        \"Each day should include 2–4 actionable tasks, with estimated time, and a citation line that points back to the source. \"\n",
    "        \"Use spaced review on Day 1, Day 3, and Day 6. \"\n",
    "        \"If context is insufficient for any part, state that clearly.\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Student feedback: {feedback_text}\\n\\n\"\n",
    "        f\"Context from course materials (slides and labs):\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"Now write the 7-day plan in this structure:\\n\"\n",
    "        \"Day 1 — Understand\\n\"\n",
    "        \"- Task 1 (est. 15–25 min) — description [CITATION]\\n\"\n",
    "        \"- Task 2 (est. 10–20 min) — description [CITATION]\\n\"\n",
    "        \"Day 2 — Apply\\n\"\n",
    "        \"- Task 1 ...\\n\"\n",
    "        \"...\\n\"\n",
    "        \"Day 7 — Checkpoint\\n\"\n",
    "        \"- Mini-quiz or small coding task ...\\n\"\n",
    "        \"\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Use only facts available in the context above.\\n\"\n",
    "        \"- Each task line should end with a [CITATION] using the [SOURCE: ...] entry from context.\\n\"\n",
    "        \"- If something is unclear or missing, say so.\\n\"\n",
    "    )\n",
    "\n",
    "    # Call OpenAI chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2,  # low temperature for focused output\n",
    "        max_tokens=1000   # adjust as needed\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4406a062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724fa1e14d4f455c88471f05ae9fa435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cc5f935e234a1882e7b1a004418c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":{\"days\":[{\"day\":1,\"tasks\":[{\"title\":\"Review SQL Joins\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Study Inner Join vs Left Join vs Right Join\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":2,\"tasks\":[{\"title\":\"Practice SQL Joins with Sample Data\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Complete SQL Join Exercises\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":3,\"tasks\":[{\"title\":\"Review SQL Keywords and Clauses\",\"est_mins\":25,\"citation_url\":\"[5]\"},{\"title\":\"Practice SELECT statements with WHERE clause\",\"est_mins\":20,\"citation_url\":\"\"}]},{\"day\":4,\"tasks\":[{\"title\":\"Work on SQL Join Scenarios\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Analyze SQL Join Results\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":5,\"tasks\":[{\"title\":\"Review SQL Data Manipulation Language (DML)\",\"est_mins\":25,\"citation_url\":\"[5]\"},{\"title\":\"Practice INSERT, UPDATE, DELETE commands\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":6,\"tasks\":[{\"title\":\"Review SQL Join Types and Use Cases\",\"est_mins\":25,\"citation_url\":\"\"},{\"title\":\"Complete SQL Join Practice Problems\",\"est_mins\":30,\"citation_url\":\"\"}]},{\"day\":7,\"tasks\":[{\"title\":\"Checkpoint Quiz on SQL Joins\",\"est_mins\":30,\"citation_url\":\"\"},{\"title\":\"Reflect on Learning and Areas for Improvement\",\"est_mins\":15,\"citation_url\":\"\"}]}]}}\n"
     ]
    }
   ],
   "source": [
    "example_feedback = \"I lost points on SQL joins and I keep mixing up inner vs left vs right joins.\"\n",
    "plan_text = generate_7_day_plan(example_feedback, top_k_slides=4, top_k_labs=6, model_name=\"gpt-4o-mini\")\n",
    "print(plan_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Topic \n",
    "Topic_keywords = {\n",
    "    \"sql\": [\"join\", \"left join \", \"right join\", \"inner join\", \"outer join\", \"select\", \"from\", \"where\", \"group by\", \"order by\", \"having\", \"union\", \"intersect\", \"except\", \"subquery\", \"cte\", \"window function\"],\n",
    "    \"loops\": [\"for loop\", \"while loop\", \"do while loop\", \"nested loop\", \"break\", \"continue\", \"infinite loop\", \"loop control\"]\n",
    "\n",
    "    def detect_topic(query):\n",
    "        result = []\n",
    "        for topic, keywords in Topic_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in query.lower():\n",
    "                    result.append(topic)\n",
    "        return result       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
