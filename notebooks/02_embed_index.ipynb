{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78da3c09",
   "metadata": {},
   "source": [
    "# Build Embeddings & FAISS Index (Slides + Labs)\n",
    "\n",
    "The goal of this notebook is to convert our processed text chunks (from slides and labs) into numerical embeddings and organize them in FAISS indexes. These indexes will let us perform fast semantic search later when generating personalized study plans.\n",
    "\n",
    "Inputs (from `data/processed/`)\n",
    "\n",
    "- `slides_chunks.parquet:` cleaned text chunks from lecture slides\n",
    "- `labs_chunks.parquet:` cleaned text chunks from lab notebooks\n",
    "\n",
    "**What this notebook does**\n",
    "\n",
    "1. Loads the chunked text data for slides and labs.\n",
    "\n",
    "2. Uses a sentence transformer embedding model to turn each chunk of text into a numerical vector.\n",
    "\n",
    "3. Normalizes embeddings so cosine similarity = inner product.\n",
    "\n",
    "4. Creates two FAISS indexes (one for slides, one for labs) and saves them to disk.\n",
    "\n",
    "5. Defines helper functions to search the indexes for the most relevant chunks given a query.\n",
    "\n",
    "6. Runs a smoke test query (`\"SQL joins inner left right\"`) to check that both indexes return reasonable matches.\n",
    "\n",
    "**Outputs (written to `data/processed/`)**\n",
    "\n",
    "- `faiss_slides.index:` FAISS index for slide chunks\n",
    "- `faiss_labs.index:` FAISS index for lab chunks\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "- Embeddings turn unstructured text into vectors that can be compared mathematically.\n",
    "- FAISS makes these comparisons very fast, even with thousands of chunks.\n",
    "- With these indexes, we can map student feedback like *“I lost points on SQL joins”* directly to the most relevant slides or lab examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "412fd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2482f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides chunks exist?  True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\slides_chunks.parquet\n",
      "Labs chunks exist?    True C:\\Users\\julmo\\OneDrive - University of Rochester\\TKH Labs\\Grades2Goals_Planner\\data\\processed\\labs_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# Folders\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "RAW_FOLDER = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_FOLDER = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Input files \n",
    "SLIDES_CHUNKS_PATH = PROCESSED_FOLDER / \"slides_chunks.parquet\"\n",
    "LABS_CHUNKS_PATH   = PROCESSED_FOLDER / \"labs_chunks.parquet\"\n",
    "\n",
    "# Output files (Indexes)\n",
    "FAISS_INDEX_PATH = PROCESSED_FOLDER / \"faiss_slides.index\"\n",
    "FAISS_LABS_INDEX_PATH = PROCESSED_FOLDER / \"faiss_labs.index\"\n",
    "\n",
    "print(\"Slides chunks exist? \", SLIDES_CHUNKS_PATH.exists(), SLIDES_CHUNKS_PATH)\n",
    "print(\"Labs chunks exist?   \", LABS_CHUNKS_PATH.exists(), LABS_CHUNKS_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b2acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slides rows: 40\n",
      "Labs rows:   1410\n"
     ]
    }
   ],
   "source": [
    "# Load chunked data\n",
    "slides_df = pd.read_parquet(SLIDES_CHUNKS_PATH)\n",
    "labs_df   = pd.read_parquet(LABS_CHUNKS_PATH)\n",
    "\n",
    "# Drop empty rows\n",
    "slides_df = slides_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "labs_df   = labs_df.dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "print(\"Slides rows:\", len(slides_df))\n",
    "print(\"Labs rows:  \", len(labs_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0f9f5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27d8dc4e112403a88de5945804e76da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julmo\\miniconda3\\envs\\ds\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\julmo\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d537942917d42f9aa15effaba7e49a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62721137441b48918c2c50829f47febc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609c5522e41b4a339e9138d1ec76121e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbfad9a5d5643b5aee6a665d8c532fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9a4e0655524dfca1c652fc8396fadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee51811625774dc8ad87185c94b1eaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ca3fabba214d6a9850a13739a0a246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceee710730424fa80393435e5f7b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab726a782fc4525bed204249e42f5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1341ea5e0d4260a544452888f25e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Embedding model\n",
    "# embedder = SentenceTransformer(\"bert-base-nli-mean-tokens\"); class example\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # smaller and faster model\n",
    "\n",
    "def encode_normalized(texts):\n",
    "    \"\"\"Convert a single query string (what the student types) into a normalized float32 vector.\n",
    "    - normalize_embeddings=True ensures vectors have length 1, so FAISS inner product ≈ cosine similarity.\n",
    "    - We return a NumPy array with dtype float32 because FAISS expects float32 vectors.\"\"\"\n",
    "    embeddings = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(\"Embedding model loaded:\", embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3631901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7ee450e6094c09a0ec7e1408395916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d34a742adc14b94a8a12decda2a5c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create 2 FAISS indices, one for slides and one for labs\n",
    "\n",
    "# Slides index\n",
    "slides_embeddings = encode_normalized(slides_df['text'].tolist())\n",
    "slides_index = faiss.IndexFlatIP(slides_embeddings.shape[1]) # IndexFlatIP for inner product (cosine similarity)\n",
    "slides_index.add(slides_embeddings)\n",
    "faiss.write_index(slides_index, FAISS_INDEX_PATH.as_posix())\n",
    "\n",
    "# Labs index\n",
    "labs_embeddings = encode_normalized(labs_df['text'].tolist())\n",
    "labs_index = faiss.IndexFlatIP(labs_embeddings.shape[1])\n",
    "labs_index.add(labs_embeddings)\n",
    "faiss.write_index(labs_index, FAISS_LABS_INDEX_PATH.as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e7b5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_slides(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the slides index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the slides index.\n",
    "    3) For each match, look up the original row in slides_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = slides_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from slides_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = slides_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"slide\",\n",
    "            \"file\": row['file'], # slide filename\n",
    "            \"page\": row['page'], # slide page number\n",
    "            \"text\": row['text'], # slide text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43c035ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_labs(query_text,top_k =5):\n",
    "    \"\"\"\n",
    "    Search ONLY the labs index and return a small, readable DataFrame of results.\n",
    "\n",
    "    Steps:\n",
    "    1) Encode the query using the same embedding model and normalization.\n",
    "    2) Ask FAISS for the top_k most similar vectors from the labs index.\n",
    "    3) For each match, look up the original row in labs_df to get metadata (file, page, text).\n",
    "    4) Build a small results table with source_type, file, page, text, and the similarity score.\n",
    "    5) Sort by score descending (higher ≈ more similar).\n",
    "    \"\"\"\n",
    "    # Embed query \n",
    "    query_vector = encode_normalized([query_text])\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = labs_index.search(query_vector, top_k)\n",
    "\n",
    "    # Turn indicies into a list of rows from labs_df\n",
    "    rows = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:  # FAISS returns -1 for empty results\n",
    "            continue\n",
    "        row = labs_df.iloc[idx] # get the mathching slide chunks\n",
    "        # result structure\n",
    "        rows.append({\n",
    "            \"source_type\": \"lab\",\n",
    "            \"file\": row['file'], # lab filename\n",
    "            \"text\": row['text'], # lab text chunk that matched\n",
    "            \"score\": score # similarity score  (higher the better)\n",
    "    \n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and sort by score descending\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9092cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a728c682ff49bfa7db58893c61f6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>file</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slide</td>\n",
       "      <td>Introduction to Structured Databases II.pdf</td>\n",
       "      <td>42</td>\n",
       "      <td>Wednesday\\nMore SQL Practice!\\n●\\nSQL Leetcode...</td>\n",
       "      <td>0.353842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slide</td>\n",
       "      <td>Introduction to Structured Databases I (1).pdf</td>\n",
       "      <td>62</td>\n",
       "      <td>Tuesday\\nOn Tuesday we will review…\\n●\\nWhat i...</td>\n",
       "      <td>0.259988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced SQL II.pptx.pdf</td>\n",
       "      <td>41</td>\n",
       "      <td>Wednesday\\nSQL Leetcode Review\\n●\\nAnother Lee...</td>\n",
       "      <td>0.251290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slide</td>\n",
       "      <td>SQL Review.pdf</td>\n",
       "      <td>16</td>\n",
       "      <td>Next Week…\\nNext week will entail:\\n●\\nMonday:...</td>\n",
       "      <td>0.231525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slide</td>\n",
       "      <td>Advanced SQL I.pdf</td>\n",
       "      <td>43</td>\n",
       "      <td>Tuesday\\nSQL + Python\\n●\\nHow do we design a d...</td>\n",
       "      <td>0.179609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_type                                            file  page  \\\n",
       "0       slide     Introduction to Structured Databases II.pdf    42   \n",
       "1       slide  Introduction to Structured Databases I (1).pdf    62   \n",
       "2       slide                        Advanced SQL II.pptx.pdf    41   \n",
       "3       slide                                  SQL Review.pdf    16   \n",
       "4       slide                              Advanced SQL I.pdf    43   \n",
       "\n",
       "                                                text     score  \n",
       "0  Wednesday\\nMore SQL Practice!\\n●\\nSQL Leetcode...  0.353842  \n",
       "1  Tuesday\\nOn Tuesday we will review…\\n●\\nWhat i...  0.259988  \n",
       "2  Wednesday\\nSQL Leetcode Review\\n●\\nAnother Lee...  0.251290  \n",
       "3  Next Week…\\nNext week will entail:\\n●\\nMonday:...  0.231525  \n",
       "4  Tuesday\\nSQL + Python\\n●\\nHow do we design a d...  0.179609  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c883de4aff814ea8800c7f91dcf7e24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lab</td>\n",
       "      <td>sql_refresher.ipynb</td>\n",
       "      <td>### SQL JOINs (Review)\\nJOINs are used to comb...</td>\n",
       "      <td>0.636397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lab</td>\n",
       "      <td>w9-class2.ipynb</td>\n",
       "      <td>### Joins\\n\\ninner joins:\\n```sql\\nSELECT *\\nF...</td>\n",
       "      <td>0.577176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lab</td>\n",
       "      <td>sql_refresher.ipynb</td>\n",
       "      <td>#### SQL JOINs\\nJOINs combine rows from two or...</td>\n",
       "      <td>0.523579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lab</td>\n",
       "      <td>w9-class2.ipynb</td>\n",
       "      <td>#### Examples of Joins</td>\n",
       "      <td>0.518481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lab</td>\n",
       "      <td>w10-class1.ipynb</td>\n",
       "      <td>```sql\\nSELECT s1.name AS student1, s2.name AS...</td>\n",
       "      <td>0.437297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_type                 file  \\\n",
       "0         lab  sql_refresher.ipynb   \n",
       "1         lab      w9-class2.ipynb   \n",
       "2         lab  sql_refresher.ipynb   \n",
       "3         lab      w9-class2.ipynb   \n",
       "4         lab     w10-class1.ipynb   \n",
       "\n",
       "                                                text     score  \n",
       "0  ### SQL JOINs (Review)\\nJOINs are used to comb...  0.636397  \n",
       "1  ### Joins\\n\\ninner joins:\\n```sql\\nSELECT *\\nF...  0.577176  \n",
       "2  #### SQL JOINs\\nJOINs combine rows from two or...  0.523579  \n",
       "3                             #### Examples of Joins  0.518481  \n",
       "4  ```sql\\nSELECT s1.name AS student1, s2.name AS...  0.437297  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(search_slides(\"SQL joins inner left right\", top_k=5))\n",
    "display(search_labs(\"SQL joins inner left right\", top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
