source,file,cell_type,chunk,text
lab,w3-class3.ipynb,code,1,import matplotlib.pyplot as plt
lab,w3-class3.ipynb,code,1,"# explicit demo

fig, ax = plt.subplots()

ax.plot([1, 2, 3, 4, 3, 5, 2, 4])
ax.set_title(""Test Plot"")
ax.set_xlabel(""x-axis"")
ax.set_ylabel(""y-axis"")

fig.savefig(""test.png"")"
lab,w3-class3.ipynb,code,1,"# implicit demo

plt.plot([1,2,3,4,3,5,2,4])
plt.title(""Test Plot"")
plt.xlabel(""x-axis"")
plt.ylabel(""y-axis"")

plt.savefig(""test2.png"")
plt.show()"
lab,w3-class3.ipynb,code,1,"# line plot with 2-axis

fig, ax = plt.subplots()

ax.plot([""a"", ""b"", ""c"", ""d""], [1,3,2,5])
ax.title(""Test Plot"")
ax.xlabel(""x-axis"")
ax.ylabel(""y-axis"")

fig.savefig(""test3.png"")

# bar-plot 2-axis
""""""
fig, ax = plt.subplots()

ax.bar([""a"", ""b"", ""c"", ""d""], [1,3,2,5])
ax.set_title(""Test Plot"")
ax.set_xlabel(""x-axis"")
ax.set_ylabel(""y-axis"")

fig.savefig(""test4.png"")
""""""
# histplot 1 axis
""""""
fig, ax = plt.subplots()

ax.hist([1,2,2,3,3,3,4,4,4,4,4,5,5,5,6,6,7], bins=6)
ax.set_title(""Test Plot"")
ax.set_xlabel(""x-axis"")
ax.set_ylabel(""y-axis"")

fig.savefig(""test5.png"")
""""""
# scatter 2 axis
""""""
fig, ax = plt.subplots()

ax.scatter([1,2,3,4],[6,8,9,10])
ax.set_title(""Test Plot"")
ax.set_xlabel(""x"
lab,w3-class3.ipynb,code,2,"-axis"")
ax.set_ylabel(""y-axis"")

fig.savefig(""test6.png"")
""""""
# overlap
""""""
plt.plot([1,2,3,4,3,5,2,4])
plt.title(""Test Plot"")
plt.xlabel(""x-axis"")
plt.ylabel(""y-axis"")

plt.hist([1,2,2,3,3,3,4,4,4,4,4,5,5,5,6,6,7], bins=6)

plt.savefig(""overlap.png"")
""""""
# fix
""""""
fig, axs = plt.subplots(2)

axs[0].plot([1, 2, 3, 4, 3, 5, 2, 4])

axs[1].hist([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 7], bins=6)

fig.suptitle(""Test Plot"")

fig.savefig(""overlapfix.png"")
""""""
# fix2
""""""
fig1, ax1 = plt.subplots()
fig2, ax2 = plt.subplots()

ax1.plot([1, 2, 3, 4, 3, 5, 2, 4])

ax2.hist([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 7], bins=6)

fig1.savefig(""overlapfix2.png"")
fig2.savefig(""overlapfix3.p"
lab,w3-class3.ipynb,code,3,"ng"")
""""""
# opportunity
""""""
plt.plot([1, 2, 3, 4, 3, 5, 2, 4])
plt.plot([5, 6, 2, 7, 10, 2, 3])
plt.title(""Test Plot"")
plt.xlabel(""x-axis"")
plt.ylabel(""y-axis"")

plt.savefig(""multiple.png"")
"""""""
lab,w4-class1.ipynb,markdown,1,"# Title

## smaller title

paragraph of text"
lab,w4-class1.ipynb,code,1,"print(""hello people"""
lab,w4-class1.ipynb,code,1,"from dataclasses import dataclass

# Our data model (like what an API would return)
@dataclass
class Pokemon:
    name: str
    type: str
    level: int
    hp: int

    def summary(self):
        print(f""{self.name} - Type: {self.type}, Level: {self.level}, HP: {self.hp}\n"")

    def attack(self, other):
        print(f""{self.name} attacks {other.name}!"")
        other.hp -= 10
        print(f""{other.name} now has {other.hp} HP.\n"")

# Simulate getting data from an API
def get_pokemon(name):
    fake_api_data = {
        ""pikachu"": {""name"": ""Pikachu"", ""type"": ""Electric"", ""level"": 5, ""hp"": 100},
        ""charmander"": {""name"": ""Charmander"", ""type"": ""Fire"", ""level"": 6, ""hp"": 95},
    }

    da"
lab,w4-class1.ipynb,code,2,"ta = fake_api_data.get(name.lower())
    if data:
        return Pokemon(**data)
        #return Pokemon(data[""name""], data[""type""], data[""level""], data[""hp""])
    else:
        print(""Sorry, that Pokémon isn't available."")
        return None
    
print(""Welcome to the Pokémon Battle Simulator!\n"")

# Try it out
p1 = get_pokemon(""Pikachu"")
p2 = get_pokemon(""Charmander"")

if p1 and p2:
    p1.summary()
    p2.summary()

    p1.attack(p2)
    p2.attack(p1)"
lab,w4-class1.ipynb,code,1,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn"
lab,w4-class1.ipynb,code,1,"print(""hello world"")
print(""hello world"")

variable = ""hello"""
lab,w4-class1.ipynb,markdown,1,"# 4 pillars of OOP

1. encapsulation"
lab,w4-class1.ipynb,code,1,"from dataclasses import dataclass
""""""""
class BankAccount:
    # we call this by calling BankAccount(1000)
    # this is a constructor
    def __init__(self, balance):
        self.__balance = balance

    def deposit(self, amount):
        self.__balance += amount

    def get_balance(self):
        return self.__balance
""""""


@dataclass
class BankAccount:
    __balance: float  # double underscore still ""mangles"" the name

    def deposit(self, amount):
        self.__balance += amount

    def get_balance(self):
        return self.__balance


bankaccount = BankAccount(1000)
bankaccount.deposit(100)
bankaccount.get_balance()"
lab,w4-class1.ipynb,code,1,"from dataclasses import dataclass
""""""
class CoffeeMachine:
    def __init__(self, brand):
        self.brand = brand

    def make_coffee(self):
        self.__grind()
        self.__brew()
        print(""Coffee ready!"")

    def __grind(self): print(""Grinding..."")
    def __brew(self): print(""Brewing..."")
""""""


@dataclass
class CoffeeMachine:
    brand: str

    def make_coffee(self):
        self.__grind()
        self.__brew()
        print(f""{self.brand} made your coffee!"")

    def __grind(self): print(""Grinding..."")
    def __brew(self): print(""Brewing..."")

coffee_machine = CoffeeMachine(""Nespresso"")
coffee_machine.make_coffee()"
lab,w4-class1.ipynb,code,1,"from dataclasses import dataclass

@dataclass
class Animal:
    name: str

    def speak(self):
        print(""animal speaking"")

@dataclass
class Dog(Animal):
    def another_speak(self):
        print(""Woof!"")

dog = Dog(""Buddy"")
dog.speak()"
lab,w4-class2.ipynb,code,1,"class BoxManager:
    def __init__(self):
        self.boxes = []  # list of dicts

    def add_box(self, color, size):
        self.boxes.append({""c"": color, ""s"": size})

    def find(self, key, value):
        result = []
        for box in self.boxes:
            if box.get(key) == value:
                result.append(box)
        return result

# Usage
bm = BoxManager()
bm.add_box(""red"", ""small"")
bm.add_box(""blue"", ""large"")
print(bm.find(""c"", ""red""))  # unclear keys!"
lab,w4-class2.ipynb,code,1,"class Box:
    def __init__(self, color, size):
        self.color = color
        self.size = size

    def __repr__(self):
        return f""Box(color={self.color}, size={self.size})""

class BoxManager:
    def __init__(self):
        self.boxes = []

    def add_box(self, box):
        self.boxes.append(box)

    def find_by_color(self, color):
        result = []
        for box in self.boxes:
            if box.color == color:
                result.append(box)
        return result

# Usage
bm = BoxManager()
bm.add_box(Box(""red"", ""small""))
bm.add_box(Box(""blue"", ""large""))
print(bm.find_by_color(""red""))  # clear and readable"
lab,w4-class3.ipynb,markdown,1,### The following is the short version shared in class:
lab,w4-class3.ipynb,markdown,1,"Let the sample space be:

$$\Omega = \{\omega_1, \omega_2, \dots, \omega_n\}$$

An event \( A \) is a subset of the sample space:

$$A \subseteq \Omega$$"
lab,w4-class3.ipynb,markdown,1,"### **Axioms of Probability**

1. **Non-negativity**:  
   $$P(A) \geq 0$$

2. **Normalization**:  
   $$P(\Omega) = 1$$

3. **Additivity** (if \( A \cap B = \emptyset \)):  
   $$P(A \cup B) = P(A) + P(B)$$

---"
lab,w4-class3.ipynb,markdown,1,"### **Conditional Probability**

Conditional probability is the probability of event \( A \) occurring given that event \( B \) has occurred. 

$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad \text{if } P(B) > 0$$

Equivalently:

$$P(A|B) = P(A) \quad \text{and} \quad P(B|A) = P(B)$$

If \( A \) and \( B \) are independent, knowing that \( B \) has occurred does not change the probability of \( A \), and vice versa.

---"
lab,w4-class3.ipynb,markdown,1,"### **Joint Probability**

From the definition:

$$P(A \cap B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)$$

---"
lab,w4-class3.ipynb,markdown,1,"### **Independent Events**

Two events \( A \) and \( B \) are independent if the occurrence of one does not affect the probability of the other. Mathematically:

$$P(A \cap B) = P(A) \cdot P(B)$$

Equivalently:

$$P(A|B) = P(A) \quad \text{and} \quad P(B|A) = P(B)$$

---"
lab,w4-class3.ipynb,markdown,1,"### **Mutually Exclusive Events**

Two events \( A \) and \( B \) are mutually exclusive if they cannot occur at the same time. Mathematically:


$$P(A \cap B) = 0$$

Then:

$$P(A|B) = 0$$

---"
lab,w4-class3.ipynb,markdown,1,"### **Law of Total Probability** (Denominator of Bayes):

$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$

---"
lab,w4-class3.ipynb,markdown,1,"### **Bayes' Theorem**

Bayes' Theorem describes the probability of an event \( A \), given that another event \( B \) has occurred. It is expressed as:

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}, \quad \text{if } P(B) > 0$$

Where:
- $ P(A|B) $: Probability of $ A $ given $ B $.
- $ P(B|A) $: Probability of $ B $ given $ A $.
- $ P(A) $: Prior probability of $ A $.
- $ P(B) $: Prior probability of $ B $.

Bayes' Theorem allows us to update probabilities based on new information."
lab,w4-class3.ipynb,markdown,1,"# Applications of Bayes Theorem


---

#### **Applications of Bayes' Theorem**

Bayes' Theorem is widely used in various fields to update probabilities based on new evidence. Below are examples for each application:

---

### **1. Medical Diagnosis**
Bayes' Theorem helps update the probability of a disease given a positive test result.

**Example**:
- Event $ A $: The patient has the disease.
- Event $ B $: The test result is positive.

**Given**:
- $ P(A) = 0.01 $ (1% of the population has the disease).
- $ P(\neg A) = 0.99 $ (99% of the population does not have the disease).
- $ P(B|A) = 0.95 $ (the test correctly identifies the disease 95% of the time).
- $ P(B|\neg A) = 0.05 $ (the test "
lab,w4-class3.ipynb,markdown,2,"gives a false positive 5% of the time).

**Step 1: Calculate $ P(B) $**:
Using the Law of Total Probability:
$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$
$$P(B) = (0.95 \cdot 0.01) + (0.05 \cdot 0.99) = 0.059$$

**Step 2: Apply Bayes' Theorem**:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
$$P(A|B) = \frac{0.95 \cdot 0.01}{0.059} \approx 0.161$$

**Interpretation**:
- Given a positive test result, the probability that the patient actually has the disease is approximately 16.1%.

---

### **2. Spam Filtering**
Bayes' Theorem is used in spam filters to determine whether an email is spam based on the presence of certain words.

**Example**:
- Event $ A $: The email is spam.
- Event"
lab,w4-class3.ipynb,markdown,3," $ B $: The email contains the word ""free.""

**Given**:
- $ P(A) = 0.2 $ (20% of emails are spam).
- $ P(\neg A) = 0.8 $ (80% of emails are not spam).
- $ P(B|A) = 0.7 $ (70% of spam emails contain the word ""free"").
- $ P(B|\neg A) = 0.1 $ (10% of non-spam emails contain the word ""free"").

**Step 1: Calculate $ P(B) $**:
Using the Law of Total Probability:
$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$
$$P(B) = (0.7 \cdot 0.2) + (0.1 \cdot 0.8) = 0.14 + 0.08 = 0.22$$

**Step 2: Apply Bayes' Theorem**:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
$$P(A|B) = \frac{0.7 \cdot 0.2}{0.22} \approx 0.636$$

**Interpretation**:
- If an email contains the word ""free,"" there is a 63.6% chanc"
lab,w4-class3.ipynb,markdown,4,"e that it is spam.

---

### **3. Machine Learning (Naive Bayes Classifier)**
Bayes' Theorem is used in Naive Bayes classifiers to predict the class of a data point based on its features.

**Example**:
- Predict whether a fruit is an apple ($ A $) based on its color ($ B $).

**Given**:
- $ P(A) = 0.5 $ (50% of fruits are apples).
- $ P(\neg A) = 0.5 $ (50% of fruits are not apples).
- $ P(B|A) = 0.8 $ (80% of apples are red).
- $ P(B|\neg A) = 0.3 $ (30% of non-apples are red).

**Step 1: Calculate $ P(B) $**:
Using the Law of Total Probability:
$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$
$$P(B) = (0.8 \cdot 0.5) + (0.3 \cdot 0.5) = 0.4 + 0.15 = 0.55$$

**Step 2: Apply Bayes'"
lab,w4-class3.ipynb,markdown,5," Theorem**:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
$$P(A|B) = \frac{0.8 \cdot 0.5}{0.55} \approx 0.727$$

**Interpretation**:
- If a fruit is red, there is a 72.7% chance that it is an apple.

---

### **4. Decision-Making**
Bayes' Theorem helps in decision-making by updating probabilities based on new evidence.

**Example**:
- A factory produces 60% of its products on Machine A and 40% on Machine B.
- Machine A produces 5% defective items, while Machine B produces 10% defective items.
- An item is randomly selected and found to be defective. What is the probability it was produced by Machine A?

**Given**:
- $ P(A) = 0.6 $ (probability the item is from Machine A).
- $ P(\neg A) = 0.4 $"
lab,w4-class3.ipynb,markdown,6," (probability the item is from Machine B).
- $ P(B|A) = 0.05 $ (probability the item is defective given it is from Machine A).
- $ P(B|\neg A) = 0.1 $ (probability the item is defective given it is from Machine B).

**Step 1: Calculate $ P(B) $**:
Using the Law of Total Probability:
$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$
$$P(B) = (0.05 \cdot 0.6) + (0.1 \cdot 0.4) = 0.03 + 0.04 = 0.07$$

**Step 2: Apply Bayes' Theorem**:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
$$P(A|B) = \frac{0.05 \cdot 0.6}{0.07} \approx 0.429$$

**Interpretation**:
- Given that the item is defective, there is a 42.9% chance it was produced by Machine A.

---

### **Key Takeaways**
1. **Bayes' Theor"
lab,w4-class3.ipynb,markdown,7,"em**:
   - Provides a framework for updating probabilities based on new evidence.
2. **Applications**:
   - Used in fields like medicine, email filtering, machine learning, and decision-making.
3. **Flexibility**:
   - Can be applied to a wide range of problems involving uncertainty and evidence.

---"
lab,w4-class3.ipynb,markdown,1,### Expanded Version:
lab,w4-class3.ipynb,markdown,1,"# Probability Theory: Sample Space and Events

In probability theory, we begin by defining the **sample space** and **events**. These are fundamental concepts that form the basis of all probability calculations.

---"
lab,w4-class3.ipynb,markdown,1,"# Sample Space

The **sample space** is the set of all possible outcomes of a random experiment. It is denoted by:

$$\Omega = \{\omega_1, \omega_2, \dots, \omega_n\}$$

Each element $ \omega_i $ in  $\Omega$ represents a single outcome of the experiment. For example:
- If the experiment is flipping a coin, $\Omega$ = $\{\text{Heads}, \text{Tails}\} $.

- If the experiment is rolling a six-sided die, $ \Omega = \{1, 2, 3, 4, 5, 6\} $.

- If the experiment is flipping a coin **three times**, the sample space consists of all possible sequences of outcomes:

$$\Omega = \{\text{HHH}, \text{HHT}, \text{HTH}, \text{HTT}, \text{THH}, \text{THT}, \text{TTH}, \text{TTT}\}$$

This is an example of a *"
lab,w4-class3.ipynb,markdown,2,"*permutation** where the order of outcomes matters. The total number of outcomes is $2^3 = 8$, since there are two possible outcomes (Heads or Tails) for each of the three flips.

---"
lab,w4-class3.ipynb,markdown,1,"# Events

An event is a subset of the sample space $ \Omega $. It represents a collection of outcomes that share a common property. Events are the building blocks of probability theory, as they allow us to focus on specific outcomes of interest within the sample space.

For example:
- In the coin flip experiment, the event ""getting Heads"" is $ A = \{\text{Heads}\} $.
- In the die roll experiment, the event ""rolling an even number"" is $ A = \{2, 4, 6\} $.

Types of Events
1. Simple Event:
    - A simple event contains exactly one outcome from the sample space.
    - Example: In a coin flip, $ A = \{\text{Heads}\} $ is a simple event.
2. Compound Event:
    - A compound event contains two or m"
lab,w4-class3.ipynb,markdown,2,"ore outcomes from the sample space.
    - Example: In rolling a die, $ B = \{2, 4, 6\} $ (rolling an even number) is a compound event.

3. Certain Event:
    - The event that includes all possible outcomes in the sample space.
    - Example: In rolling a die, $ C = \{1, 2, 3, 4, 5, 6\} = \Omega $.

4. Impossible Event:
    - The event that contains no outcomes (the empty set).
    - Example: In rolling a die, $ D = \{\} $ (rolling a 7) is an impossible event.

Mathematically, an event $ A $ is defined as:

$$A \subseteq \Omega$$

This is saying that event $ A $ is a subset or is within the sample space set denoted by $ \Omega $"
lab,w4-class3.ipynb,markdown,1,"#### Visual Representation

Events and sample spaces can often be visualized using Venn diagrams. For example:
- The sample space $ \Omega $ is represented as a rectangle.
- Events $ A $ and $ B $ are represented as circles within the rectangle.

This foundational understanding of sample spaces and events is critical for solving probability problems and analyzing random experiments.

![Sample Space and Events](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAhgAAAGZCAYAAADLgEjwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXIBJREFUeJzt3Xl4k2XePvzzzt4kXWlpS0tb9n13A5RFQQQZUEEdQRQUtxm3Z5xxXn8zig84o8Mz6LiM44wiKIui6KDihqDI"
lab,w4-class3.ipynb,markdown,2,IgoKVVmkFFooFFq6N03bbNf7x0UDpS1b7+ROk/NzHDnaplm+SdrcZ65VEUIIEBEREalIp3UBREREFH4YMIiIiEh1DBhERESkOgYMIiIiUh0DBhEREamOAYOIiIhUx4BBREREqjNoXQAREUUWIQScTmdA78NqtUJRlIDeB50ZAwYREQWV0+mE3W4P6H04HA7YbLaA3gedGbtIKGC+++47XH/99cjIyIDZbEZycjKGDh2KRx55ROvSzmrmzJnIyspS9Tbb8vMRSs71tRk1ahQURWn2pPZre6GWL1+Of/zjH1qXEbHef/99DBkyBAMHDkSvXr1w1VVXwefzBeW+Fy9ejKlTp5739TZs2IChQ4di4MCB6N27N4YPH46ioqIAVNh6bMGggPj4448xadIkjBo1CvPnz0dqaiqOHj2K77//Hm+//TYWLFigdYlBxedDG507d8ayZcuanG82mzWopqnly5dj586dePjhh7UuRTNFRUWqtTTU1NQgOTn5nC577Ngx3Hvvvdi2bRsyMzMBANu3bw/pbhWPx4Prr78ea9euxaBBgwAAe/fuDdmWGgYMCoj58+ejU6dO+Pzzz2EwnPwz+/Wvf4358+drWJk2+HxoIyoqCpdddpnWZdAZ2Gw2TQ6QR48ehcFgQLt27fznDR482P/9H/7wB6xfvx5utxuxsbF47bXX0K1bN+Tn5+Oi
lab,w4-class3.ipynb,markdown,3,iy7Cvffei48//hi1tbVYunQp/vOf/+Dbb7+FxWLBqlWr0KFDByxevBjLli1DTEwM9u/fj9jYWCxZsgQZGRlN6lmyZAleeukluN1uREdH45///Cf69u3b6DLV1dWorq5Gamqq/7wePXr4vx81ahQGDhyI7OxsHDlyBNdddx3mz58PRVHw7LPP4q233oLH44HRaMSLL76ISy+9FACwZcsWPProo6iqqoIQAvPmzcPkyZOxb98+PPzwwyguLobL5cI999yD3/zmN+f+JAuiAOjTp4+49NJLz+myb7/9thg7dqxISUkRFotF9OzZU/zxj38UDoej0eVuv/12YbPZxJ49e8TVV18trFarSElJEU8//bQQQogtW7aI4cOHC6vVKrp16yYWL17c6PqLFi0SAMSaNWvEzJkzRXx8vLBarWLixIli//79Te4rMzOz0Xk+n0/885//FAMGDBAWi0XExcWJKVOmNLlua5+PzMxMce2114r3339f9OvXT5jNZtGpUyfx/PPPN7pcbW2t+N3vficGDBggYmJiRHx8vLjsssvEqlWrmtym1+sVL7zwgr/22NhYcemll4oPPvig0eXefvttcdlllwmr1SpsNpu4+uqrxfbt289ac3FxsbjvvvtEr169hM1mE0lJSWL06NFiw4YNjS6Xl5cnAIj/+7//EwsWLBBZWVnCZrOJyy67TGzZ
lab,w4-class3.ipynb,markdown,4,sqXJ7S5atEh0795dmEwm0bNnT/HGG280+9o0Z+TIkaJPnz5nvEx2drYAIF577bUmv/vkk08EgEbPUU5OjrjllltEUlKSv6aXXnqp0fW++uorAUAsX75c/L//9/9EamqqiI6OFldddZX45ZdfGtUHoMmpwcsvvyz69+8vbDabsNvtokePHuKxxx476+NuCxwOh//xnv5/Hqzb9Xq94oYbbhDx8fHiuuuuE/PnzxeHDx/2//748eP+79966y1x7bXXCiFO/g2vXr1aCCHE/PnzRWxsrNixY4cQQoj77rvP/zotWrRIWCwW/+v+t7/9TYwfP97/uylTpgghhNi0aZOYMGGCqKurE0IIsWHDBtG/f/9m637ooYeE3W4X48ePF3PnzhV79+71/27kyJFi7NixwuVyiZqaGjFkyBCxYsUKIYT8H22wZcsW//9GaWmpSE5OFps3b/Y/L6WlpcLj8YiLLrpI7NmzRwghRE1NjejXr5/44Ycfzvi8nooBgwJi9uzZAoB44IEHxLfffitcLleLl503b5547rnnxMcffyzWr18vXnnlFdGpUycxevToRpe7/fbbhclkEr169RLPP/+8+OKLL8SsWbMEAPHYY4+J7t27i4ULF4rPP/9cTJw4UQAQ33//vf/6DQGjY8eO4o477hCffvqp+M9//iPat28vOnbsKMrLyxvd1+kHsbvu
lab,w4-class3.ipynb,markdown,5,uksYjUbxyCOPiM8++0wsX75c9OzZUyQnJ4tjx46p9nxkZmaKtLQ0kZGRIV5//XXxySefiOnTp/sPzA0qKirEzJkzxZIlS8SXX34pPvvsM/H73/9e6HQ68cYbbzS6zRkzZghFUcTs2bPFBx98ID799FPxl7/8pVFo+ctf/iIURRF33HGHWL16tXj//ffF0KFDhc1mE7t27Trj4/vll1/EfffdJ95++22xfv16sXr1anHnnXcKnU4nvvrqK//lGt6cs7KyxDXXXCNWrVolVq1aJfr16yfi4+NFRUWF/7INr9fkyZPFRx99JJYuXSq6du0qOnbseF4Bw+12Nzl5vV7/5QYNGiSGDx/e5Po33XSTaN++vXC73UIIIXbt2iViY2NFv379xJtvvinWrFkjHnnkEaHT6cSTTz7pv15DwMjKyhLTp08XH3/8sXjrrbdERkaG6Natm/B4PP7bGz58uEhJSRFbtmzxn4SQB7SGv5c1a9aItWvXildeeUU8+OCDZ33cbUEoBIwGe/bsEa+88oqYPHmyiI2NFfv27RNCCLFs2TJx2WWXiT59+ohevXqJtLQ0IYT8G7bb7f7rr127tlGQfe2118TNN98shJB/w2PGjPH/rry8XFgsFuHz+RoFjD/84Q8iLS1NDBgwwH9KTU0V9fX1zdacn58vFi1aJG699VZhtVrFxo0bhRDyb37p
lab,w4-class3.ipynb,markdown,6,0qX+yz333HPirrvuEkII8fnnn4sRI0aIPn36iAEDBghFUUR9fb1YvXp1k/daIeTfZ1RUVKOasrKyxJIlS87peRWCAYMCpKSkRFx++eX+f3aj0SiGDRsmnn76aVFdXd3i9Xw+n3C73eLrr78WAMSPP/7o/93tt98uAIj33nvPf57b7RZJSUkCQKNP2qWlpUKv14vf/e53/vMaDljXX399o/vcvHmzACCeeuqpRvd16kFsy5YtAoBYsGBBo+sWFBSIqKgo8eijj6r2fGRmZgpFUUR2dnaj88eOHStiYmJETU1Ns/fh8XiE2+0Wd955pxg0aJD//A0bNggA4k9/+lOL9R06dEgYDAbxwAMPNDq/urpapKSkiJtuuumMj6+lWq666qpGz3dDwOjXr5//QCuEEFu3bhUAxFtvvSWEkJ+iOnToIAYPHix8Pp//cvn5+cJoNJ5zwGiuhQCAuPPOO/2Xe+GFFwSARp8Ey8rKhNlsFo888oj/vHHjxon09HRRWVnZ6H7uv/9+YbFYRFlZmRDiZMCYMGFCo8u98847AkCjlpprr7222cdy//33i7i4uLM+xrbq1CBQVFQkHA6HKqeioqJWBZdx48aJBQsWiIMHD4p27dr5Wyd//PFH0a5dOyGE/Btu+F4I+XoPGTLE//OpweFcA8bvf/978fjjj5//EymEuOeee/z/t80FjLvv
lab,w4-class3.ipynb,markdown,7,vlvU19cLu93u/8BVWVkpAIjq6uoWA8bOnTtFx44dL6imBpxFQgHRrl07bNy4Edu2bcMzzzyDyZMnIycnB4899hj69euHkpIS/2UPHDiAadOmISUlBXq9HkajESNHjgQA7Nmzp9HtKoqCCRMm+H82GAzo2rUrUlNT/YOeACAhIQHt27fHwYMHm9Q2ffr0Rj8PGzYMmZmZ+Oqrr1p8PKtXr4aiKLj11lvh8Xj8p5SUFAwYMADr169X7fkAgD59+mDAgAGNzps2bRqqqqqwfft2/3nvvvsuhg8fDrvdDoPBAKPRiIULFzZ63j799FMAwG9/+9sW6/v888/h8Xhw2223NXp8FosFI0eOPOvjA4BXXnkFgwcPhsVi8deybt26Jq8hAFx77bXQ6/X+n/v37w8A/tdr7969KCwsxLRp0xoNusvMzMSwYcPOWkuDLl26YNu2bU1Ojz/+uP8y06dPh9lsxuLFi/3nvfXWW6ivr8esWbMAAHV1dVi3bh2uv/56WK3WRs/RhAkTUFdXh2+//bbRfU+aNKnRz6c/xjO55JJLUFFRgVtuuQUffPBBk7+PcJKcnAy73a7K6VwHeALAkSNHsHnzZv/P5eXlyMvLQ5cuXVBZWQmTyYSUlBQIIfDSSy9d8OPbvHkzcnJyAACvvfYarrzyyiYDSX/1q1/hzTffREFBAQDA5/Ph+++/b3Jb
lab,w4-class3.ipynb,markdown,8,DocDn376KYQQAIDa2lrs2bMHXbp08V9myZIl8Hg8qK2txfLlyzFmzBjU1dXB7XajY8eOAIAXX3zRf/lhw4Zhz549+Oabb/z3XVZWhh49esBqteLNN9/0XzY3NxdlZWXn/Ng5yJMC6qKLLsJFF10EAHC73fjjH/+I5557DvPnz8f8+fPhcDhwxRVXwGKx4KmnnkL37t1htVpRUFCAG264AbW1tY1uz2q1wmKxNDrPZDIhISGhyX2bTCbU1dU1OT8lJaXZ80pLS1t8HEVFRRBCtPgG1rlz5xave6qzPR9nqxGAv873338fN910E2688Ub84Q9/QEpKCgwGA/71r3/h9ddf91/v+PHj0Ov1zd7mqY8PAC6++OJmf6/TnfmzyLPPPotHHnkE9957L+bNm4fExETo9Xo8/vjjzQaMUwfWASdndTS83g2PsaXnIT8//4z1NLBYLP7nuyUJCQmYNGkS3nzzTcybNw96vR6LFy/GJZdcgj59+vjr8Xg8ePHFFxu9OZ/q9BBwtsd4JjNmzIDH48Grr76KKVOmwOfz4eKLL8ZTTz2FsWPHnvX6dHYejwdz585FXl6ePzTefvvtmDx5MgDgxhtvRJ8+fZCRkdGq53zkyJF48sknsXv3bsTGxjY6YDcYMWIE/vrXv2Ly5Mnwer1wu9249tprm/ztCiHwyiuv4KGHHkJUVBTcbjeu
lab,w4-class3.ipynb,markdown,9,ueaaRh8eBg8ejDFjxvgHeU6dOhWKomDu3Lm45JJLkJGR0Sj8xsfH47///S8eeeQRVFdXQ1EUzJs3D5MmTcJHH32E//mf/8Hf//53eL1eJCUlNTsrqyUMGBQ0RqMRc+bMwXPPPYedO3cCAL788ksUFhZi/fr1/lYLAKioqAhYHceOHWv2vK5du7Z4ncTERCiKgo0bNzY7xfFCpj0293ycrUbg5IFr6dKl6NSpE1asWNHoE1F9fX2j6yUlJcHr9eLYsWONRp+fKjExEQCwcuVK/5S987F06VKMGjUK//rXvxqdX11dfd63BZx8jGd6HtQ0a9YsvPvuu/jiiy+QkZGBbdu2NXos8fHx0Ov1mDFjRostQZ06dVK9plmzZqGmpgYbNmzAnDlzMHHiROTk5FzQaxRKrFYrHA5HwO/jTDIzM/H555+3+Pvnn38ezz//vP/nP//5zwCArKysRmFy1KhRjVobZs6ciZkzZ/p/ttlsWL58eZPbP/1y06ZNw7Rp085Yc3R0ND744IMzXmbYsGH461//2uT8Rx99FI8++qj/59///vf+7y+77LJGrTkNunXrhtWrV5/x/s6EAYMC4ujRo80ezBo+zXbo0AEA/AfG0w/Q//73vwNW27JlyzBlyhT/z9988w0OHjyI2bNnt3idiRMn4plnnsGRI0dw0003nfd9nuvz0WDXrl348ccf
lab,w4-class3.ipynb,markdown,10,G3WTLF++HNHR0f6pdIqiwGQyNQoXx44da/IGNH78eDz99NP417/+hblz5zZb37hx42AwGLB///5Gz825UhSlyWv4008/YcuWLf5m2fPRo0cPpKam4q233sLvfvc7/2M8ePAgvvnmmybPV2tdffXVSEtLw6JFi5CRkQGLxYJbbrnF/3ur1YrRo0djx44d6N+/P0wmkyr3azabz9qiYbPZMH78eLhcLlx33XXYtWtXmw8YiqKE7NoNpB4GDAqIcePGIT09Hb/61a/Qs2dP+Hw+ZGdnY8GCBbDb7XjooYcAyLQdHx+Pe++9F3PmzIHRaMSyZcvw448/Bqy277//HrNnz8aNN96IgoIC/OlPf0JaWtoZ53cPHz4cd999N2bNmoXvv/8eI0aMgM1mw9GjR7Fp0yb069cP9913X4vXP9fno0GHDh0wadIkPPnkk0hNTcXSpUvxxRdf4G9/+5v/k9nEiRPx/vvv4ze/+Q2mTp2KgoICzJs3D6mpqdi3b5//tq644grMmDEDTz31FIqKijBx4kSYzWbs2LEDVqsVDzzwALKysjB37lz86U9/woEDB3DNNdcgPj4eRUVF2Lp1K2w2G/73f/+3xcc3ceJEzJs3D3PmzMHIkSOxd+9ezJ07F506dYLH4znXl8ZPp9Nh3rx5mD17Nq6//nrcddddqKiowJNPPnnGrp7T1dbWNhkb
lab,w4-class3.ipynb,markdown,11,0eDU9TH0ej1uu+02PPvss4iJicENN9yA2NjYRpd//vnncfnll+OKK67Afffdh6ysLFRXVyM3NxcfffQRvvzyy/N+nP369cP777+Pf/3rXxgyZAh0Oh0uuugi3HXXXYiKisLw4cORmpqKY8eO4emnn0ZsbGyL3VgUek5vpQi0cxkrFVStGiJK1IIVK1aIadOmiW7dugm73S6MRqPIyMgQM2bMELt372502W+++UYMHTpUWK1WkZSUJGbPni22b98uAIhFixb5L9ewDsbpWlrvoGE9iQanroMxY8YMERcXJ6KiosSECRP8U9NOva/mRve//vrr4tJLLxU2m01ERUWJLl26iNtuu63RdNjWPh8Nda9cuVL06dNHmEwmkZWVJZ599tkmt/vMM8+IrKwsYTabRa9evcSrr74q5syZI07/1/Z6veK5554Tffv2FSaTScTGxoqhQ4eKjz76qNHlVq1aJUaPHi1iYmKE2WwWmZmZYurUqWLt2rVnfHz19fXi97//vUhLSxMWi0UMHjxYrFq1qsnzeOo6GKcDIObMmdPovNdee01069ZNmEwm0b17d/H666+f1zoYaGEWCQD/9NMGOTk5/t998cUXzd5mXl6euOOOO0RaWpowGo0iKSlJDBs2rNEMpIZZJO+++26T657+N11WViamTp0q4uLihKIo/tftjTfeEKNHjxbJycnC
lab,w4-class3.ipynb,markdown,12,ZDKJDh06iJtuukn89NNPZ33cRKFCEeLEcFSiMLd48WLMmjUL27ZtO+vAPy1lZWWhb9++rer7JCLSGqepEhERkeo4BoOIiDRRWQk4nerfrtUKnDaEhjTALhIiIgq6ykpg3jwgEGuIJSYCjz/OkKE1tmAQEVHQOZ0yXERFyRYHtW/X6Tx7wMjKyoLFYmm0eN/y5cvRu3dv9Qo6ITs7Gzk5OWed5v7EE0/gL3/5Cw4cONDmpyMzYBARkWasViA6Wt3bPIfFUv1WrlzZZFv0QMjOzsbq1avPGDB8Ph8WL16MESNGYPHixZgzZ07A6wokDvIkIiI6xVNPPYUHHnjA/7PD4UBCQoJ/Bc+///3vuOSSSzB48GBMmDDBv4fIk08+iWnTpuFXv/oVevfujSuvvBJlZWUoLi7GE088gbVr12LgwIG49957m73fNWvWIDk5GQsWLMCiRYvg8/kC/2ADiAGDiIgi1tSpUzFw4ED/yeVyYebMmVixYgVcLhcAuang6NGjkZiYiOXLlyMnJwdbtmzB9u3bccstt+D+++/33953332HN954A7t370b79u3x73//G+3bt8fcuXMxZswYZGdn45VXXmm2loULF+KOO+7A4MGDER8fj3Xr1gXlOQgUdpEQEVHEaq6LJD09HYMGDcKHH36IqVOnYtGiRf59PFatWoXvv/8eQ4YMAQB4vd5G
lab,w4-class3.ipynb,markdown,13,OwOPHz/ev/ni0KFD8fPPP59THSUlJfjiiy/w6quvAgDuvPNOLFy4sE1vbseAQUREdJpZs2Zh8eLFGDhwIHJzczF+/HgAckfTP//5z7jjjjuavd6pA0b1ev05L5XfsM36wIEDAcjgUlpaitLS0iY787YV7CIhIiLNOJ1AdbV6J7XW1bj++uuxdetWPPPMM5gxY4a/lWLSpEl4+eWXUVZWBgBwu93YsWPHWW8vJiYGlZWVLf7+9ddfx8qVK5Gfn4/8/HwUFBRgwoQJ57U9eqhhCwYREQWd1SrXqygpOb9ZH+ciMfHcp75OnTq1UavDiy++iCuuuAJmsxk33ngjXn75Zf+uxwAwY8YMlJaWYtSoUVAUBR6PB3feeScGDRp0xvu56qqr8Pe//x0DBgzA0KFDG43D+O6771BcXIwxY8Y0us6MGTPwxBNP4MEHHzy3BxNiuNAWERFpgit5hjcGDCIiIlIdx2AQERGR6hgwiIiISHUMGERERKQ6BgwiIiJSHaepEhGRNlyVgDcA00j0VsDEaSRaY8AgIqLgc1UCO+cB9SXq37Y5Eej7+FlDRiht17548WI8/PDDyMrKAiBXDJ07dy4mT56sei3BwoBBRETB53XKcKGPki0Oat+u1wng7K0YobRd+5gxY7By5UoAwNatW3HttdcyYBAREV0QvRUwRqt7m97WLQ361FNPoaio
lab,w4-class3.ipynb,markdown,14,CC+++CIAuV17RkYGcnJykJiYiL///e9455134PF4kJKSgn//+9/o2LEjnnzySeTk5KC6uhr79+9HSkoKVq5cCY/HgyeeeAJVVVUYOHAgLrvsshZ3VG1QVlaG+Pj4Vj0OrTFgEBFRxDp9qfCtW7di5syZGDx4MBYsWACTydTidu16vR5LlizB/fffjw8++ACAXPZ727ZtSEhIwK9//Wv8+9//xmOPPYa5c+di9erV/haK5qxduxYDBw6E0+nEkSNHsGLFioA//kBiwCAioogVKtu1A427SHbu3IkxY8Zg+/bt6NChQ6seo1Y4TZWIiOg0Ddu15+bmNrtde3Z2NrKzs/Hzzz8jOzvbf70L3a79dH379kVGRgY2b97cqsehJQYMIiKi0wR7u/bTHT58GPv27UP37t0v7AGEAHaREBGRdtReB+M8by8Utmtv0DAGQwgBj8eDv/71rxgwYMB5PZ5Qwt1UiYgo+EJgHQwKLAYMIiLSBlfyDGsMGERERKQ6DvIkIiIi1TFgEBERkeoYMIiIiEh1DBhERESkOgYMIiIiUh0DBhEREamOAYOIiIhUx4BBREREqmPAICIiItUxYBAREZHqGDCIiIhIdQwYREREpDoGDCIiIlIdAwYRERGpjgGDiIiIVMeAQURERKpjwCAiIiLVMWAQERGR6hgwiIiISHUMGERERKQ6BgwiIiJS
lab,w4-class3.ipynb,markdown,15,HQMGERERqc6gdQHBIoSA0+nUugwiImpDrFYrFEXRuow2KWIChtPphN1u17oMIiJqQxwOB2w2m9ZltEnsIiEiIiLVRUwLxqmKioqYSImIqFk1NTVITk7Wuow2LyIDhs1mY8AgIiIKIHaREBERkeoYMIiIiEh1DBhERESkOgYMIiIiUh0DBhEREamOAYOIiIhUx4BBREREqmPAICIiItUxYBAREZHqGDCIiIhIdQwYREREpDoGDCIiIlIdAwYRERGpjgGDiIiIVMeAQURERKpjwCAiIiLVMWAQERGR6hgwiIiISHUMGERERKQ6BgwiIiJSHQMGERERqY4Bg4iIiFTHgEFERESqY8AgIiIi1TFgUJv3zTeAXg9cc43WlVDE2DITWK6cPK1sB3x1DVD+k9aVEYUMBgxq815/HXjgAWDTJuDQIa2roYiReg1w/VF5umodoBiArydqXRVRyGDAoDatpgZ45x3gvvuAiROBxYu1rogiht4MRKXIU/xAoPcfAWcBUHdc68qIQgIDBrVpK1YAPXrI0623AosWAUJoXRVFHLcDyF8G2LsC5nZaV0MUEgxaF0DUGgsXymAByDEYDgewbh0wZoy2dVEEOLIaeMcuv/fUAFGpwMjVgMLPbUQAWzCoDdu7F9i6Ffj1r+XPBgNw881yTAZRwCWPBsZny9PV3wEpVwPrxwM1B7WujCgk
lab,w4-class3.ipynb,markdown,16,sAWD2qyFCwGPB0hLO3meEIDRCJSXA/Hx2tVGEcBgA6K7yu+jASQMAVbGArmvAgOe0rQ0olDAFgxqkzwe4M03gQULgOzsk6cffwQyM4FlyzQukCKPogDQAd5arSshCglswaA2afVq2Upx551AbGzj302dKls37r9fm9ooQnjrgdpj8ntXOZDzEuBxAGm/0rYuohDBFgxqkxYulAM5Tw8XADBlimzN2L496GVRJDn6GfDfVHn6/FKgdBtw+btA8iitKyMKCYoQkTGpr6amBna7HPHtcDhgs9k0roiIiEIRjxfqYAsGERERqY4Bg4iIiFTHgEFERESqY8AgIiIi1TFgEBERkeoYMIiIiEh1DBhERESkOq7kSSFFCKCuDqipkTujnnqqqQGqquQKnnV1gNd78uTzATodoNefPBkMQFycXIzLbm98stnkV6tVXo8Iwgd4nHI1To9D7pDqccit2D0OwF0BuCoB4QGE95STT+6gquhPnvQWwBgPmGLkniUG+2knm7yMomj9qIkChgGDNCGEDA3HjsnT0aNAfr78WlsLuFwnT15v4/dho1EGCEVpfBKi6cnlkl8bKApgMp08mc1Au3ZA585Ahw5ASoo8xcczeIQt4QPqy4C6IqDuGOAsBBwHAFepXP7b5zp5wqnrECqAziS/Kor86j8JeRINX72AcJ9yn0IGD50J0JtOfI2S
lab,w4-class3.ipynb,markdown,17,W7zbOgFRKYAlRX412Bk8KCwwYFDACQGUlJwME0eOAPv3A8XFMmTU1cn3U7MZiIqSB/6YmJMhoCFMqMHnaxxe6upkLT//LH+v18uWjfh4GToyMk6GjpQU2SpCbYjPI0NE7TH5tebQiTBRLlslhFdmA4NVtijoTLJ1QXciBCgqpUxxInScGl7c1UBdMVC6Vf5eb5HhwtIesHcBrGkng4c5kaGD2hwuFU4BUVsrD9y5ucCOHcDhw0B1tTzAK4rsomjoprCEUEuxy9W4e8brlefbbEBiIjBoENC9O9CtG7eDD1mucqBqH1CVA1TsAOpLZHcHAEAPGE/pqtAZNS3VTwjAV3eiO6YG8NZAJh8dYIwGrOlA/CC5Pby9C2CI0rrisMbjhToYMEgVQgCFhTJQ7NkjWwRKS+UB22YDEhJkmDCGyPv5+fD5ZOgoL5djQBRFju3o1g3o1w/o2hXIymqbjy0s+NyAIx9w5AIVPwHVuYCrAoAADDGAKV62SqjVGhFMPrdsaXGVyeChMwHmdkBcPyCmlwwcUR1CJ6GHCR4v1MGAQRfM6wX27QN27ZKtFEeOyAOwXi8/3SckyG6PcOPxABUVQFmZ7GKxWoGkJGDgQKB3b6BvX9nVQwHkqQUqdwKVu4HybKDuOOB1AjoLYE4AjHGALgz7s7x1soXGVS67XIwxsislfhAQ2weI7gbo9FpX
lab,w4-class3.ipynb,markdown,18,2ebxeKEOBgw6byUlMlBs2iS7QZxO2TqRkCDHTkTS4Egh5OMvKwMqK+VjT0sDhg8HBg8GOnXih0vVCAHU5AFl24HjmwHnEQA+wBgLmBIAvTWynmzhA9xVJ1o3HPLxR3cBki6XgcOSqHWFbRaPF+pgwKBz4nLJloqtW4EffpAhIyoKSE2V4YIkl0sOZC0vl2GrZ09g2DDZuhEbq3V1bZSrUrZSlHwDVP4CeKplC4UlRc7IIMntAOqOylYOczsgYQjQ7hLZssHn6bzweKEOBgxqkRBycOb27bK1oqBAdg+0by8HPEZSS8X5EkJ2Fx07JkNHcjJw6aXAxRfLQaKcjXIWPg9QtRco2waUbAXqi+WATHOK7BaIpJaK8yV8cmBrfTGgGABrR6D9FbJVw5rO5+4c8HihDgYMakIIOVhzzRrg++9l039MjJymabFoXV3b4/EAx4/LVh+zWbZqjBsHDBnCoNGEzy27QI5+DlT9ItelMCcC5qTwHFMRaN46OUXXUyW7ktpdDKSMlYNDGTRaxOOFOhgwyE8IOWjziy9kV4jDIccTxMfzvUgtNTWyJUgIOSC0IWhE/AwUnxso+0EGi8rdABT5ydvA/1NVCCHHatQWyum57S4BUsfKQaH8526Cxwt1MGAQhAByck4Gi5oaID1dTsXke09gOJ3AoUPyue/VC7jmmggNGj43UPr9iRaL
lab,w4-class3.ipynb,markdown,19,PZDBIpPrPASKEHLJc+fhE0HjYiD1agaN0/B4oQ4GjAjWECzWrAG2bZMHvbQ0Botgqq2VQcPnk0Fj3DjgoosiIGicHiwUHRCVwWARLA1Bo/aInH3ib9Hozn9+8HihFgaMCHX4MLBqlWyxcDpPtliQNhqChtcrg8Z118mZJ2H3Xi+EnBFyeBVQuUeu2WDNkPtyUPD5g8ZhQG+TQaPjdXIwaATj8UIdDBgRpq4OWLsWWL1a7gWSkcFgEUpqa4GDB+Xgz5EjZdBIStK6KpXUHQcKVgHFX8sdSW2ZDBahxFUBOA/JvVA6TARSx8j9USIQjxfqYMCIEELIdSxWrpTLeMfHy+6QsPuEHCbKy2WLRseOMmRccUUb7jbxuYHiDcDhDwBngWyxMHEjl5AkhOw2cZXL5cgzpsp1NCLsjSLSjxdqYcCIAGVlwAcfAF99JVswunQJzyW8w43PJ1sznE45LmPqVPnatSnVucChlXKGiN4G2DLa5p4gkcZbDzj2yxaMlCuB9MkRFQoj+XihJgaMMOb1Aps3A//9L5CfL8dZtGundVV0vpxO4MABuRLouHHAhAltYPVUtwMo/AQ4+plcztrWWW6JTm1LfamccWLPAjreACQOi4i9TiLxeBEIDBhhqqgIWLYM+O47wGSSu33qw/99IWwJIV/ToiK5UNe0aXIn15BU8TOQv1yuxGluD1iS
lab,w4-class3.ipynb,markdown,20,I66JPaz4vIAzH/C5gHaXAlnTgahkrasKqEg7XgQKA0YYys4GliyRn3q7dGkDn3bpnLndcoO5qChgyhS5fkbIrAbq8wCFnwEF7wHeWsDeRS7vTeHB4wAcBwBbJ6DzDCB+oNYVBUwkHS8CiQEjjLjdwCefyC6R+nqga1e2WoQjIYCjR+VA0JEjgVtvDYGZQK5yIG+ZnCFiigcsqWy1CEfCCzhyAZ0ZSL8eSJsQliEyEo4XwRAqn32olUpLgaVLgY0b5TiLrCytK6JAURSgQwcgOhpYtw44cgSYORPo0UOjgqr2AgcWy71DbJ0BY7RGhVDAKXogugdQexTIXwLUHAQ63QqYE7SujEIQWzDCwO7dwBtvyFU52SUSWdxuuX9MbCxw883AlVcGcZdb4QOOrQMOrpADOaO7c0OySOJ2ADX75eveeSYQ20vrilQTzseLYGLAaMO8Xrl/yLvvyo3JunULof54Choh5MqsDgdw1VXALbfI1o2AclcD+W8BRevknhZR3AY8Ivk8QHWObLXKuAlIGRMWs0zC8XihBR6O2qi6OtklsmaN3Eq9Z0++v0cqRZELclVWAp9+ChQWAvfeC6SkBOgOa48C+/4NVPwIWLMAU2yA7ohCns4AxPSSi3MdWCgXUut0a8SuAEqNccWbNqiqCnjlFTmgMy1Nrm/BcEGxsXIfk59+Ap57DsjLC8Cd
lab,w4-class3.ipynb,markdown,21,OPKAX/4BVP4MRPdiuCD55mNNByxpcu2Tfa/IFi6KeAwYbUxJCfDii8DXX8vxFrF8f6dTmExAnz5yivI//iGXh1dNxU7gl+fkCo/RvQG9ScUbpzbPFCunJhd/Dex9Aagr0boi0hgDRhtSWCg/mf7wg+wSYbcgNUevly0ZRUUyjG7bpsKNlm4Dcl6UG5bF9A6LfnYKAIMNiO4pl4bf+w/AWah1RaQhBow24tAh4PnngV9+kQcP7iVCZ6LTyWmrDofsTvvmm1bc2PFvTjR718gZA9xLhM5Eb5bjMir3AHufB2oKtK6INMJ3ijYgLw944QW5gmOvXm14V00KKkWR3WgeD/Dqq7Jb7bzmjAkBFH0N5P5Hzhawd+FgHzo3OqMMGY5c2V3iCMSAIAp1DBghLjdXhov8fBkuOA2VzoeiAJ06ye8XLgS+/PI8rlz0JbB/IQAFsHdiuKDzozPI7rSaPBkyqnO1roiCjAEjhB05Ipu3Dx8Gevfmst904TIzZcvXm28C3357Dlco+RbIe1N+ErVlBrw+ClOKXoYM52HZzcYxGRGFASNElZcD//mP7B7p2TOIqzNS2OrYEfD5gMWL5eqvLarcDRxYJFfqtHYMVnkUrhQdENNTdpPk/lvuW0MRgYetEOR0Aq+9Jtcz6NGDLRekns6dZXh99VWgoLmxdzWHgNxXAVeF3FeESA2KHojp
lab,w4-class3.ipynb,markdown,22,AVT8JLvdPE6tK6IgYMAIMW633Gp9yxage3e5rgGRWhRFhtZDh2T3W8mpSxXUlchPmM4CuaEVx1yQmnQmwN5NzkrKWyIHDlNYY8AIIUIA770HrF0rd0ONitK6IgpHOp3sdtu9W7aUORyQG1ftf012j0T34FRUCgyDFbBlAce+AAreO89pTdTWcE5CCFmzBvjgAyA5We4vQhQoRqNsyfjuOyDG7sLdVy6GoeQ72Yyt4zxoCiBjDGBOBgpWAcY4oMM4rSuiAOHHlBDx3XfA8uVyq/XERK2roUhgsQBdOvvgzl2BI9u/grB15iZVFByWJLnq58HlQMl3WldDAcKAEQIOH5bjLrxeuXkZUbD0br8Zwzt9gj0H03DoWKD3eCc6hTUd8HnleAznEa2roQBgwNBYfb3cdr2wUI7wJwqWaMMR9LGvgMFkRK03Abt2AVXcBJOCyd4ZqC0E8pYC3nqtqyGVMWBo7OOP5WZU3bpxrQsKHr1Sj/7RSxFtKESFJwtxcUB1tZwa7fFqXR1FDEUnZ5aUbpVbvVNY4SFNQzt3Ah9+CCQlccYIBVc36ydIs2xFmbsbAB0UBUhIkC1p+/ZpXR1FFEMUYE4CDn8IVOzUuhpSEQOGRioqgGXLgNpaOWuEKFjam3aih/1DOL1J8IiTydZolEF3716g+LiGBVLksSQDXieQv0wu8kZhgQFDAz4f
lab,w4-class3.ipynb,markdown,23,sGKF3Hq9WzeuZ0TBY9FVoF/0MhgVJxzepsnWbpeLvf30E1DHLnEKFkWRXSWVvwAH35HL1FObx4ChgY0bga++kotpcXdUChYFPvSOfgeJpl9OdI00TbYNXSUlJcCuXVwHiYJIZ5Ab6xV9CRzfpHU1pAIGjCA7ehR45x3AbAZiY7WuhiJJumULOkV9hQpPJnxnWGNPrwfi4uRGe4cPB68+IpjiAJ0ZOLgCqD2qdTXUSgwYQSQE8NFHMmRkZWldDUUSk+JAD/sHENCh3hd31stbLLI145e9gMsV+PqI/GyZcurq4Y/YhNbGMWAE0e7dwKZNQHo6x11QcHWyrkWCMRcV7qxzvk5cHFBWBhzIC1hZRE0pOiCqo+wmqdqjdTXUCgwYQeLxyH1G6upkHzdRsNj1R9HV9hlqfe3gw7nvM6LXy1klublAtSOABRKdzpwAeGuBw6u462obxoARJFu2ANnZ7BqhYBPobv8Idn0Rqj0dzvva0dFATQ2QkwOwsZqCytYJKMsGSrZoXQldIAaMIKiulgtqmUyA1ap1NRRJkky7kRm1CZWejmhu1sjZKIrc2ffQIaCEa2NQMBmsgM4kF+Bycw37togBIwjWrgX272frBQWXDm70sH0Ag1KLOt+F98tFRcm1MX7ZK9dwIQoaWxbg2A8cW6t1JXQBGDACrLAQ+PRTuQU717ygYEqP+hYp
lab,w4-class3.ipynb,markdown,24,5mxUuDu1+rbi44Fjx4ACTlulYNIZAFM7oPAzwFmodTV0nhgwAuyTT+SiRampWldCkcSoONHd9hG8MMEtWt8vZzTKQZ85ObI1gyhoojoA9ceBwk+1roTOEwNGABUWAt9+K8MFp6VSMHWwfI944wFUujNUu83YWKC8HCjk+kcUTIoCWFLlYE+2YrQpDBgBtHmzXEcgMVHrSiiS6OBBp6gv4RNG+GBS7Xb1ennKy+NYDAoycyLgKuOMkjaGASNAKiuBr7+Wa16w9YKCqb15J9qZ9qLKk676bUdHyy4/7rZKQaUogCkBKP4KcFdpXQ2dIwaMANm6VS4JnpKidSUUWQQyo9ZDp3jgFjbVb91kkq0X+flcF4OCzJICOI8CpVu1roTOEQNGALhcwJdfyjUvOHOEginemIdUc/YFLap1rqKj5YySioqA3QVRUzoDoLcCx9YBXm6Q0xYwYATAjh3AgQNAWprWlVCk6WjZBJOuCnXnsKHZhbJYgPp6ufgWUVBZ0wDHAaA8W+tK6BwwYKjM5wO++kp+b7FoWwtFFqv+ODKiNsPpbY8LWbXzXCkKYLPJgOF0BuxuiJrSn3hTLfoKEBxpHOoYMFSWkwPs3MnWCwq+dMt3sOmPo8bbPuD3ZbPJcHGYC29RsEWlARU/A1U5WldCZ8GAobIdO4DaWrl/A1GwKPCio2Uz6n3REEH4t1YU
lab,w4-class3.ipynb,markdown,25,OeDzUAHg42hPCiZjjNxptSJb60roLBgwVORyAd9/LxckIgqmeGMeYoyH4fQmBe0+bTagqgqoKA/aXRJJxligdBsHe4Y4BgwV5ebKqalJwXuPJwIgd001KTVwBWBqakuMRrls+HGuiUHBZk4Eao8CjlytK6EzYMBQ0a5dQF2d3H2SKFgU+JBm2QqXsCOQgzub3K8iQ8aRQkCwm4SCyWAFvHVA5W6tK6EzYMBQiccjF9di9wgFW5wxH7HGQ6jxBL/pzGaTq9ZWVAb9rinSGWPkols+j9aVUAsYMFSyf7/c3Iz7jlCwJZr2wKg4TrRgBJfJJMcesZuEgs6cBDiPAI79WldCLWDAUMmuXXL2iC14XeBEsnvEvPXEluzB3/TG301yhEuHU5AZbCdmk+zSuhJqAQOGCrxe2T0SHa11JRRpYgwFiDPmB3X2yOmsVrlseBW7SSjYDNEnukm8WldCzWDAUEF+vvwEx9kjFGyJpl9g0lWj3qfdwitm84lukhLNSqBIZU4Cao8ANflaV0LNYMBQQUGBXNWQ3SMUbPHG/fDBAC26RxooCqDTAeVcD4OCzWADPE7AWaB1JdQMBgwVHDwo32QV7d7jKQLp4EE7U46mrRcNzGagtJSrelKQNbzx1jBghCIGjFYSAti7F7AHfwA/RTiboQhRugq4QiRg1NUBDofWlVDE0duB6l+4GEsI
lab,w4-class3.ipynb,markdown,26,YsBopbIyoLiYe49Q8MUYDsOoc8Dl0z7dNkxXra7SuhKKOMZooLYIcLGPLtQwYLTSkSNyPwbOIKFgizEcgQIRlM3NzkZR5AfIKgYMCjZjDOCpBpzc2jfUaP/O1MYdOQL4fHItAKJgSjDmwCMsWpfhZzAApWVaV0ERR2cEhEfOJqGQwoDRSgcOAHq91lVQpDEotYgzHgyJAZ4NzGa5HoabKzdT0BkAR57WRdBpGDBaweMB9u1j9wgFX4zhCMy6KtT7QuePr2E9DI7DoKAzRgPVOdyXJMQwYLRCcbH8xMYBnhRs0YZCGJQ6eETobN1rMMjt26s5k4SCzRgDuCqAem6KE0oYMFqhslJOzbOETjc4RQizrhICCrRcYOt0DUsS1NdpXQlFHJ0F8NYDLq5XH0oYMFqhulp2k3CAJwWbWVcJiNAJF6eqq9e6Aoo4OiMg3ICb/XOhhAGjFRqm5HEFTwo2m/44fAi9ZKvXy2XziYKq4U3YU61tHdQIA0YrcM4/acWqL4FHmLUuowkGDNIUWzBCikHrAtqy8nK5yVO4KygAFi0COncGbr313K6TmwusWweUlABRUUCPHsC116pf2//+78nvFUXO6OndG7jqKjnoMBzp4IFFXwHvKQGj8gCQ/TwQ3wPo/5tzu52y3UDeR4CzGDBYgcR+QLebzn69X5YCRVtPNtsZrALRGUDnyYA+
lab,w4-class3.ipynb,markdown,27,BqitlWvDhO3/xuFVQMWPJ3/WRwFRHYCUsYAl+czXdR4Bjn0O1B6Tzfq2TKDjVEAJ0JNVvBEo/gpIvhJIujww9xEyFMDFhVhCSZi+BQfH8eNyieRwt2MHcMklwPbtcmBrbOyZL+/xACtWAH36ADfeKKcuHgngGjiTJwNduwJeL1BUBHzwgRwXc+WVgbtPLZl01dAr9XD7rP7zjn0LpI0Ajm4B6soAS8KZb8PnBnYtBJIGAb3vkOPjqg6eew3xvQR6Tpffu6qAvI+Bnf8BBvx/ciaJyxXmg5/tXYG0yfJ7jwMo/hI4uBzo8T9nvl7BSsDcDuhyFwAR+G3GK7KBxGFA+Y7wDxg6M1BXonUVdIpw/YwRcELIT+fm0GulVpXLBezaBVx0EdC9O5CdfW7X0+mAfv2AhAQgJQUYMiRwNVoscrO52FhZY/fuwLFjgbs/rZl11TAo9f4WDG89cHwH0OFyoF1f4Nh353Y7ig5oPwSISgLs6UCH4edeg84AmGLkyZ4OdBwD1Jcr8NXKoFcf7gM9FT1gtMtTVAqQOFw2z3tqznI9BYjpBViSAEt7oN0lgWu9qMmXSTJ5tPxacx4Jsi3Sm4H6Em56FkIYMC5QfT1QUxP+AWPXLiAxUZ7695cB42z/vwYD0KUL8MUXsrk8mEpLgfx8IC0tuPcbTGZdFfSKyz8Go3g7YE2Wp+SLgKLv
lab,w4-class3.ipynb,markdown,28,zv4a6YxAfE/gwIeA+yzHxLPx1gPF3wNRSQKWGNmCFfYB41ReF1DxM2BKAPTWM182ugdwfINcsyHQyncAcX1lGIrtK38OZzqzbE3yRdIfX2hjF8kFqq6Wb6LhvsjWjh2yJQKQ3RAuF5CXJ8djtGT9etmC0LcvsHixHLfRsNrpJ5/IbpZbbmn+uuvWAd+d5RP4b3/buJvmvffkB0OfT3567t4duDyMW4NNumoo8ECc+Pc99i3Q/iL5u4Re8oBfkSPHY7Qk/1PAcVi2YPz4AtDvN4D5xHO6713ZzdLvnpavX7oL2Ph7+b3PpcAUI9D3HkCnl+HG5VLhgYay6hxg91/l9z43YLADmdPOPKWsPFuO3UgcBuQtBjKny5YMACj5Bij/Eeh2X/PXLftBjt04k8zpckwHIP8IKvcAne+QP8f1Bw68DqSOl5/0w5HOJGeRuB2APpz759oOBowL5PHIA1o470NSUiLHTtx0YuCfTifHVezY0XLAqK0FNm0Cbr4Z6NZNvt++/roMGe3aydVPu3Zt+T6HDgUGDWr59y++2PTT+bhxsh6fDygrA9asAf77X2Dq1PN7vG2FTvGgYYEtZxFQfRDoc6f8naIHkgbL0NFSwHA7gUNfyOu06yNvKvs5GTKs7YGaozKonElcN6D7TQ23J1C4Efj5FWDwI/I8n6/VDzO02ToBHU6MWvbWAmXb
lab,w4-class3.ipynb,markdown,29,gPxlQJfZgCmu6eWFAIrWAe1HA+0uli0deYtkKLGmA3XFgK1jy/cX20feZ0vyFjf+x6j8GTDFy+4bQH41xQOVO4GEAPZXaknRA8IrNz6jkMCAcYF8Pvn/HM5rYOzYIR/ns882Pl+nk0EiqplVqktLZStCyon3tdGjZUvPokUyCBw+DNxwQ8v3abXK0/mw2+VYD0B25bhcslXjyitPnh9OFJw8eh/9FhA+BVueOOXgIuR7bVcnYGzmuawtAoRHgT1dXqfTtYC3Ts5C6XoDUJUP9LrtzDXoTXLsBgBEAYjuCGx6FDj6DWC7BPCFeze4zgiYT/njikoF9jwDlG+XMzZO56mRzfcNB/yEwYDPBeS/CXSYBFTtBrJub/n+9JYzfyo/fRxH+Q6gvhjYOfeUM4U8P1wDBhQAQoYMCgkMGBco3AOGzwf8+CNw9dVyPMWp3nkH+PlnObPkdA1dIQcPyi4SQAYLlwt4/315nUB3KzW8Jm53YO9HKzJgKBBeoGgr0Pk6gYSejS+z63U5LiJtRNPrN3zArtwPtB8sv+9yg2xV3/OGgrSRAua4C6hLkb0FQCSOszuxbLuvhT86vQVQDHKgpfVES0XiZXK8wOH35NgMq0oDh+qKgNpCoNNMOYW2gbdOtprUFcsBpuFGUeQfngj35rO2gwHjAnm94R0wcnLkPiuDBjWdbtir18mpq6eL
lab,w4-class3.ipynb,markdown,30,jZXB4pNP5HPUsaNcL6SoSE4dzckBRowAbLYLq+uBB5oGlLo6wOGQr0dpKbBhg+yOSUq6sPsIdTrFCwg5DsLjBFKHAobTWpOSBgLHtjQfMCzxQNJggX3vys0nYzsBtaVATSGgMwmU/gxkXA2YzrBRq88jp6cCssulcIMc69iuL+BCBAQM4ZV9/cCJLpKtskUiuoV+KZ0BaHcpUPy1bP2wd5UtGs7D8mfnQTkDwpx4YfV0mgkYTvxTle8AotJOjsc4lbWjbGVJvebC7ieknWjBAANGqGDAaIVwDhgN4yyaW8ugd285zuLoUSA1tenvr7sO2LwZ2Ljx5G6z/foB06cDb74JvPUWcPvtF7aHS3NdHh98cPJ7ux3IzJQLbYXtQk8AAIGjW+Q4i9PDBQAkDgAOrVFQXSAQ3UzXfs9bgYJ1wKE1ckCnORZIvhjodx/w40sn1rR4QHaFNKd8j4Itf5bf680C1mSg9yw5NqO4GPJ9Ppw5coG9C+T3OpMMBh1vBOxZLV8n+SrZfFS6DTi2Vo7DiO0FpF8PHH5frqPRebZc9ex8NTRL+bxAxU9y2mxzYnoBxzcByWPliNyw0tCCEe5/fG2HIkRkvBo1NTWw2+0AAIfDAduFfoQ+4dAh4E9/kgfY8x0zQNQanaK+xEVx/8JxVx+tS2lWcTFw8UVApzOMSSRSnadGdg8NeAqwZbTq
lab,w4-class3.ipynb,markdown,31,ptQ+XkSqsP6MF0g63ckuP6JgEtAh1JsIArV2FFHLBABFjnCmkMC3gQuk08lT2E/Ho5AjoAvZrdobhGvXIYWwhj5rptuQwVfiAun1bMEgbfiEHooS2n94DBgUfCdaMHhYCxl8JS5QVJQcpBiuUyEpdHlEw6jO0Gs+awjcFzKAl6hVfC45I+dCBslSQDBgXCC7Xc6wiKg9Fygk1Pti4BFmGJTQW4/b65Wte+G+Rw+FIJ9Lrvth4IDMUMGAcYF0OjllkgGDgq3eFwOvMEOvhN4fn8cjN7sztzC9lShgvPVyOXSOwQgZfCVaITGRAYOCr94XDY8whWTAYAsGacZXH54rlLZhDBitkJgoP7ERBZNHWODy2WEIwYDh8QAmE6DnEn4UbMJz4SuhUkAwYLRCbCxHy5MWFDi9idCH6BgMq7Vhr1eiIDOeYX17CjoGjFaIjuY0VdKG05sYsi0YXNmWNCEAGAO8kyKdFwaMVoiJkYM9vdwdmIKszpfQaNv2UBJ1hl3FiQLC55WDOw1swQglDBitEBMjB7NxoCcFW70vJmQXC+cATwo6Xz2gN7MFI8QwYLRCUpLcdtzh0LoSijQ1niQAOugQOiu9eb1yTJKVyxBQsHkccv0LS5LWldApGDBawWYD0tOBqiqtK6FIU+1JQ70vBiZdtdal+LlcsvUihh8iKdjcVYC1IxfZCjEMGK3U
lab,w4-class3.ipynb,markdown,32,owdQW6t1FRRpnL52qPEkwqwLnXRbXy9Dd1TU2S9LpCpvLRDdXesq6DQMGK2Uni6bhbmrKgWXghJXT5h0NVoX4udyAe3acYoqBZnwyTdhW0etK6HTMGC0Ulqa/NRWEzrv8xQhqjwZJ2aSaD/cUwh5io3VuhKKOJ4aQG8DotK0roROw4DRSqmp8k2V4zAo2Ko86Sc2PavTuhT/HiQxnCVIweauAkyxQFSK1pXQaRgwWsloBLp2BapDZ6wdRYiqEwM9zSEw0LO+Xg7wjOYATwo2TxUQ3U1u1U4hhQFDBV26AO7QmS1IEcItbKhydwyJgZ4ul2zJM/E9noLN5wXsnbWugprBgKGCtDS5gyRDBgVbqbs7DIr205jcbqBdgtZVUMTxueUKntZ0rSuhZjBgqKBLF7mzammp1pVQpClzd4MPBk23bvd45JL5Ce00K4EiVX0JYEkE7F20roSawYChguhoYMAABgwKvhJXD9R42sOqL9GsBqcTsNvlFFWioHKVAXEDAaNd60qoGQwYKhkwQH71eLStgyKLR0ShsO4iROnKNauhrk7OpjIaNCuBIpHvxJttfH9t66AWMWCopFcvICGBrRgUfMWufvAJPXRwBf2+G/YfSW4f9LumSOcqBcztgJheWldCLWDAUElsLNC/PwMGBV+Jqwec3kRNukmcTrnQXGJi0O+aIl19KRDXX66B
lab,w4-class3.ipynb,markdown,33,QSGJAUNFAwfKJcO9Xq0roUjiFjYcrR+MKH1Z0O+7tvZE9winp1Iw+TwAfED8QK0roTNgwFBR797sJiFtFNX3hxDB3b69oXukPbtHKNhcZYApAYhl90goY8BQUXw80LcvAwYFX4mrZ9C7SZxOwGoFktg9QsHmKgXi+gKmeK0roTNgwFDZ4MGym4SLblEwuUQ0jtYPQpQueOm2oXvEZAraXRLJxbWEF4gfrHUldBYMGCobNEhu4V5YqHUlFGkO1w2HFxaYlMDvTVJfLzc368gFFCnYagsBa0cgYZDWldBZMGCozGYDRo8GKitlSwZRsJS4eqK4vjeiDYcDfl/V1UBSEmePUJAJH+CuBJKvBAw2rauhs2DACIChQ+Ubb3Gx1pVQJBHQIb/2SigQAV063OMBhAA6ZclBnkRBU1cMmJOAxMu0roTOAQNGACQlAZdfLgOGEFpXQ5HkaP0gVHg6IcZwJGD3UVUFxMUBKakBuwuipoQA6ouBpOGAJUnraugcMGAEyOWXyz1KKiq0roQiiVeYkee8CkalBgrUX5ClYQBz506AQa/6zRO1zF0BGKOB9pdrXQmdIwaMAOnUSQ745GBPCrYjdZfA4UmB3XBM9dt2OGRwTuPgTgq22kIgfhBg66R1JXSOGDACRFGAkSMBvV6uF0AULHW+OBysHQGrrgSAen10QsipqZkZgMWs2s0S
lab,w4-class3.ipynb,markdown,34,nZ3HCSh6IHkUB/60IQwYAdSvH9CzJ1BQoHUlFGkK6oaj1pcAq/64arfpdAIWC9AxQ7WbJDo3tQVATE8gtq/WldB5YMAIIIMBGD9efu9waFsLRZYqTzrynSMRrT8KBa2fLy2EnJqalQXERLe+PqJz5j6xrkuH8YDOoG0tdF4YMALsoovkKT+fM0oouHJrJqDKk4YYQ+ub0CorgZgYoFtXFQojOldCAM58IOFiIOEirauh88SAEWB6PTB5shwYVxL83bQpgjl9idhXMxFmXRX0uPB1MTweuXJn925y7xGioKkvAQwxQPokQMdpS20NA0YQdO0qV/csLOTqnhRc+bUjcdzVC3HG/Au+jYoKubZLZpZaVRGdA+ED6grlqp3RbDprixgwgmTCBCAtDTgc+FWcifw8Igp7HZMB4IL2KKk/0fDRowdgZPc3BZOzALCmybEX1CYxYARJYiIwcaLsy3a5tK6GIsnR+iE4UnfJiVaMcx8IJIRsvUhLAzpw1U4KJm894K4COkwELNzwpq1iwAiikSOBXr2AvDytK6FIIqDD3ppJqPfFnte01YZpqT17cOkBCjJnPhDbC2g/UutKqBUYMIIoKkoO+FQU2ZJBFCzl7i444LwS0fqj0MFz1st7vXJaaqdOQHx8EAokauCqBKAA6ZMBQ5TW1VArMGAE2ZAhsiUjP1+OzicKln01E1Hi
lab,w4-class3.ipynb,markdown,35,6okE4z6cqatECKCsTHbrde8WvPqI4PMANfmy5SJhiNbVUCsxYASZTgfcfLNc4XPfPq6NQcFT54vDz9XT4RZW2PVFLV6upgYwGoH+/WUXCVFQCAE49gGxPYHMmwGFh6e2jq+gBuLigOnTZZdJUcvv80SqK3b1xV7HJFj1x2FQapv83u2WYy969ADac0dsCqa6IkBvBbKmA6Y4rashFTBgaKRvX+BXvwKOH5cbSBEFyz7nBBypu+REV8nJhVkaukY6dAC6sWuEgslTC9QflwtqxXG/kXDBgKGhiROBiy8GcnO5ABcFj1eY8XP1dFR7OiDOkO8/v6JCrjjbvz9g4KKJFCzCJ7tG2l0CdJigdTWkIgYMDZnNwK23AikpwMGDWldDkaTKk45djpuhV9yw6CpQVydnjvTpw83MKMhq8oGoDkCnWwG9WetqSEUMGBpLT5eDPl0u+QmSKFgKaofjgPMqROsPorrSjc6dgYyOWldFEcVVAfjcclCnNU3rakhlDBgh4PLLgTFjgEOH5AA7omAQ0GFn1Y3YfaQveqfvRe9ePi6oRcHjcQLOQ0DKGCBpuNbVUAAwYIQAnQ6YNg0YNkxOXeVS4hQMQgA/7YnFHtfdyOiVAYtrL+dNU3B4XXLcRdLlQNY0TkkNU3xVQ0RUFDB7NjBgAPDLL7I/nCiQ9u8HEhKAm2Z1RPTAuwBTAuA4
lab,w4-class3.ipynb,markdown,36,oHVZFO58XqD6FyBuANDlDq7WGcYYMEJIXBxwzz1Aly4yZPDDJAXK4cOAwQDMmiUXfUNsL6DzTECnB5zc8pcCRAgZLqK7At3u4XoXYY4BI8SkpgJ33w20by+nrzJkkNqOH5djfaZNAy655JRfJF4qm6u9NUDduW+KRnROGlbqtLQHut4NRHGL3nDHgBGCunUD7rxTTmMtKNC6GgonlZVy9djrrgPGjm3mAiljgfTrgPqiE5tOEanEeQjQW4Aud8oWDAp7DBghavBgYMYMoL6ey4mTOpxOud7KuHHADTe0sAW7ogAdpwApVwPOg3KkP1Fr1RUBPhfQaQaQMFjraihIGDBC2KhRwNSpQHk5Qwa1Tk2NnKF0+eVycTeD4QwX1hnkgSDpctmk7akJWp0UhmqPAa5yIONGoP0orauhIDrT2wxpTFGAyZPl13fekTNLOnTQuipqaxwO4MABYORI4K675IylszJEyUF4ig4o3gDYOgNGe8BrpTBTWwh4HEDmLUD6r1poNqNwxRaMEKfTyZAxfbr8FHqYA/zpPFRVyXBx5ZVyhpLNdh5XNtiArvcAyVcCNQcAd1XA6qQw5DwsW7+ypstNzLjWRcThK94GKAowYQJw++1yEa78fM4uobMrKzs55mL27HNsuTidIQroOhtIvRqoOSibuonORAjAkS/HXHSeKTcwY8tFRGLAaCMU
lab,w4-class3.ipynb,markdown,37,RY76v+suQK+X/ekMGdSSoiLg2DHZ+jVrFmCxtOLG9Bag8x1A+mSg9qgcsEfUnIapqDoD0PUuuQw4w0XE4hiMNkRRgCuuAOx24LXXgD17gB49ZOAgalBQANTWArfcAkyapNLfh94km7qNduDQSvnp1Mqd0egUwgtU7QUsSTJcxA/UuiLSGFsw2qBBg4CHHgI6dgR27wbq6rSuiEKBzydbtrxe4I475FoXqoZPnR5Iv14u7yy8QPU+QPhUvANqs7x1QOVuwNYR6PEQwwUBYMBos7p3B373O7leRk6O7G+nyFVXB+zcCSQmAg88IHfnDUjLtKLIZu8eDwDmRKBypzy4UOSqLwOqc+T6Fj1/B8R017oiChGKEJHRk19TUwO7XU6zczgcsJ3XcPrQ5XQC774LfP45YDQCWVns8ow0JSVAYaFc9vu224I4ldlZCOS9CZRuBaLSAHO7IN0xhQQhgJp8QLiB1GuAjKmAwap1VaoI1+NFsDFghAEhgI0bgbfekvtMdO8OmExaV0WBJgSQlye7RCZMkKtzXtBMkdbw1AIF7wGFnwCKAbB1YsKNBD6XbLUwJwFZtwBJV4TV6x7Ox4tg4iDPMKAowIgRckzG4sXATz/JlozYWK0ro0Cpr5ddYykpctOyYcM0en83RMnBn/ZOQP5yoGoXYO8G6M0aFENB4aqQy8jH9ZfTUO2dtK6I
lab,w4-class3.ipynb,markdown,38,QhRbMMJMdbVsyVi3Ti6q1LFjWH2wIMil4wsKgIED5doomZlaV3RCzUHgwBtAebacYWKK17oiUpMQgLNA7rabPAbI+jVgjNa6qoCIlONFoDFghCGfD/jyS2DFCnkw6tTpPFdwpJDk8chVOQE5iPPmm0PwdXU7gEMrgGPr5M+2znJNBGrbPDWA44AcZ5N5k1zdNYxX5oyk40UgMWCEsbw8YOVKYNs2udBSZibXzGiLhACKi+XiWd26ybEWF18sl5EPScIHlG4DCt6X/fSWFMDcnk1pbZHwypYpbx3Q7mI5kDMCukQi8XgRCAwYYc7jATZtAv77X+DQIdllkpCgdVV0rmprgf37gZgY4Oqr5WDOmBitqzpH7irgyCfA0c8BTzVg6yLHbFDbUF8G1BYA1gyg4/Vyd90IaY2K1OOF2hgwIkRpKbBqFbB+vdzPpHNnwMxxeCHL55PjLBwOubDa1KlydlCbVJUDFKwEyrYDhmg5PiOMm9fbPG+93NxOZwKSRwHp10XcFORIP16ohQEjggghZ5i89x6wa5dsyejQgS3XoaaiQrY2degg9xIZOTIMph17XUDx18DhD+QW3tYMwBSndVV0KiHka+MqA2L7yO6QuH4R+QbB44U6GDAiUG0tsGYN8PHHct2MlBSgPbvINVdVJVstzGbg8suB668HkpO1rkpltUXA4f8CxZsAX71s
lab,w4-class3.ipynb,markdown,39,zTC2lT6fMCUEUF8M1B2T61qkTQRSxkZ0dxaPF+pgwIhgR47I2SYbNsgulORkBg0tVFYChw/LYDFokBxr0bdvGL8OQsglxo+uAcp2MGho5dRgYUoE2o8AUq4ErGlaV6Y5Hi/UwYBBjYJGSYkMGsnJYXyACxGVlbLFwmKRe8pcfTXQu3cIzw5Rm/DJDbKOrpHjM3x1QFRHwMQV4gJKCKCuCKgvkvvJtB8pp51ag7XGfOjj8UIdDBjkV1gIfPWVHAjKoBE4DcEiKqpxsIjY51mIU4LGDwwagdIkWIwCkkczWDSDxwt1MGBQE0ePngwax4/LJcdTUuQnbbowXq9cy6KkBLBagSFDZLDo1SuCg8XphACq9shprWXbAY9THgjN7eVW8XRhvHWyG8RdKcdYtB8FpIwGolK1rixk8XihDgYMatHRo8C33wKbN8tP3F6vbNFo1y6CmvFbQQg5cPPYMcDtBpKS5I6nl10G9OzJYNEiIYCqX4CSb+VOrXXFcsqkJUWO0+ATd3bCB9SXyDEWigGwpst1LBIvA6JStK4u5PF4oQ4GDDqr+no5rfW774Dt2+WAUKsVSE0NwaWqQ4DLJVfdLC8H7HYZJoYNk3uHxMVpXV0b46qQe5sc3wxU7QU8DrnHiSVZhg5qzFMD1B4FvE65dkX8YCDxUjntlBvQnTMeL9TBgEHnpbgY2LFDtmrs
lab,w4-class3.ipynb,markdown,40,3w/U1cn1NBISNNgqPIS43TJQlJTIn1NTgeHDZVdI58780N1qwgc48uQYjeOb5UEUkF0opoSIWWGyWd5aueqmuwzQWQB7F6D95UD8IMCSpHV1bRKPF+pgwKAL4vXK7cK3bZOn0lIZNqxWGTZiY8N73xMh5CqbZWVyB1udDoiPl4M1L7sM6N8/sgNXQHmcQMXPsgulcjfgKpcBxBgtw4bBHt6JTngBV6VcEMvrBPQW2VqRcLHcLyS6O8estBKPF+pgwKBWq6uTG6vl5srWjYMH5dgDQAaNhAQZPNo6t1sGivJy+b3NJge/Dhokl/Hu2lU+XgoiVyXgyJXLkZfvAGqPyW4UnUl2pZgSAJ1R6ypbRwjZSuEqA9wV8jxjLGDLlK0U0V3lBmR6jsJWC48X6mDAIFUJIWee5OYCe/bIpclLSuQ4Dp1OHpTtdnkymUL3g6bHI1soGk5uN2AwyFaKXr2APn1koOjYMbxbatoUnxdwFsjAUbFTDhR1lQPCAyhG2bLRcArVLhUhAJ9LhiSPA/DWyI1p9GbZHRQ/AIjpKUOFOSl0/4HaOB4v1MGAQQHlcsnWjbw8uVplbq7sTnE45O8URXYlNAQPqzW4M1SEkC0wNTUnwwQga7Db5c6lnTsDGRlyu/uuXYHo6ODVR63grgaqc+V24zWH5AZerip54BY+QAGgtwNGO2CwyfELwTxg
lab,w4-class3.ipynb,markdown,41,C5/s4nCfCBO+OgACUEyyJlM7GSSs6bKFwtYJ0HNgazDweKEOBgwKKiHkQlPHjslTYeHJ8FFTI/dJOfUvUlFkS8fpJ51O/u7UU8PtN5x8Ptny4HI1Pnm9jWsymWSYSEoCunQB0tNl10dyMpCYyBaKsOHzyqmbdcfkglPOw4BjP1B3/MQB3tX48opeHuz1JtnlojPJlhBFB6Dhj64hkAh5Eg1fffL2Tj/h1LdbBdBHyXBjTQfsneXaFJYUOZXUGMsWCo3weKGOEG0npHClKHKqZlycnL7ZoK5OTu2sqGjcNVFRcXLcQ2WlvFx1tQwJp4aJU2+/4aTTAUajDBAJCbJ7o2HGS0M3jc0mWymSk2XLBN/Pw5hOD0Qly1MDIQBPtdyEzVMlp3k2tCjUl8lxD65yeb6nBvC5AfhOBonTA0ND8FD0cuyHziQHYJriAXMCYIyTXTTGE101pjg55ZbjJygMMWBQSLBYZBdEZmbLl/F6T3ZleDyyhaKhpcLnOxkqTj3ZbPLU5rc7p8BQFLl419k2WvO65HgIT41snRA+yKBx4o9Q0Z3SsqGTi1sZ7YDexhkdFLEYMKjN0Otla0MMN92kYNOf6CoxxWtdCVGbwQWfiYiISHUMGERERKQ6BgwiIiJSHQMGERERqY4Bg4iIiFTHgEFERESqY8AgIiIi1TFgEBERkeoYMIiIiEh1DBhE
lab,w4-class3.ipynb,markdown,42,RESkOgYMIiIiUh0DBhEREamOAYOIiIhUx4BBREREqmPAICIiItUxYBAREZHqGDCIiIhIdQwYREREpDoGDCIiIlIdAwYRERGpjgGDiIiIVMeAQURERKpjwCAiIiLVMWAQERGR6hgwiIiISHUMGERERKQ6BgwiIiJSHQMGERERqY4Bg4iIiFTHgEFERESqY8AgIiIi1TFgEBERkeoYMIiIiEh1DBhERESkOgYMIiIiUh0DBhEREamOAYOIiIhUx4BBREREqmPAICIiItUxYBAREZHqGDCIiIhIdQwYREREpDqD1gVooaamRusSiIgoRPEYoY6IDBjJyclal0BERBTW2EVCREREqlOEEELrIoJBCAGn06l1GURE1IZYrVYoiqJ1GW1SxAQMIiIiCh52kRAREZHqGDCIiIhIdQwYREREpDoGDCIiIlIdAwYRERGpjgGDiIiIVMeAQURERKpjwCAiIiLVMWAQERGR6hgwiIiISHUMGERERKQ6BgwiIiJSHQMGERERqY4Bg4iIiFTHgEEUARRFwapVq7Qug4giCAMGkQqKi4txzz33ICMjA2azGSkpKRg3bhy2bNmidWmqee+993DppZciNjYW0dHR6NOnDx555BGtyyKiEGXQugCicDBlyhS43W688cYb6Ny5M4qKirBu3TqUlZVpXZoq1q5di1//+tf461//ikmTJkFR
lab,w4-class3.ipynb,markdown,43,FOzevRvr1q3TujQiClWCiFqlvLxcABDr168/4+UWLFgg+vbtK6xWq0hPTxf33XefqK6u9v9+0aJFIjY2Vnz00Ueie/fuIioqSkyZMkU4HA6xePFikZmZKeLi4sT9998vPB6P/3qZmZli7ty54pZbbhE2m02kpqaKF154odF9AxD//e9//T8fPnxY3HTTTSIuLk4kJCSISZMmiby8vBZrf+ihh8SoUaPO+PjmzJkjBgwYIF555RWRnp4uoqKixNSpU0V5ebn/Mlu3bhVjxowR7dq1EzExMWLEiBHihx9+aHQ75eXl4q677hLt27cXZrNZ9OnTR3z00Uf+32/evFlcccUVwmKxiPT0dPHAAw8Ih8NxxtqIKPjYRULUSna7HXa7HatWrUJ9fX2Ll9PpdHjhhRewc+dOvPHGG/jyyy/x6KOPNrqM0+nECy+8gLfffhufffYZ1q9fjxtuuAGffPIJPvnkEyxZsgT/+c9/sHLlykbX+7//+z/0798f27dvx2OPPYb/+Z//wRdffNFsHU6nE6NHj4bdbseGDRuwadMm2O12XHPNNXC5XM1eJyUlBbt27cLOnTvP+Fzk5ubinXfewUcffYTPPvsM2dnZ+O1vf+v/fXV1NW6//XZs3LgR3377Lbp164YJEyaguroaAODz+TB+/Hh88803WLp0KXbv3o1nnnkGer0eAPDzzz9j
lab,w4-class3.ipynb,markdown,44,3LhxuOGGG/DTTz9hxYoV2LRpE+6///4z1kVEGtA64RCFg5UrV4r4+HhhsVjEsGHDxGOPPSZ+/PHHM17nnXfeEe3atfP/vGjRIgFA5Obm+s+75557hNVqbdTSMW7cOHHPPff4f87MzBTXXHNNo9u++eabxfjx4/0/45QWjIULF4oePXoIn8/n/319fb2IiooSn3/+ebO1OhwOMWHCBAFAZGZmiptvvlksXLhQ1NXV+S8zZ84codfrRUFBgf+8Tz/9VOh0OnH06NFmb9fj8Yjo6Gh/C8Xnn38udDqd2Lt3b7OXnzFjhrj77rsbnbdx40ah0+lEbW1ts9chIm2wBYNIBVOmTEFhYSE+/PBDjBs3DuvXr8fgwYOxePFi/2W++uorjB07FmlpaYiOjsZtt92G0tJS1NTU+C9jtVrRpUsX/8/JycnIysqC3W5vdF5xcXGj+x86dGiTn/fs2dNsrT/88ANyc3MRHR3tb31JSEhAXV0d9u/f3+x1bDYbPv74Y+Tm5uLPf/4z7HY7HnnkEVxyySVwOp3+y2VkZCA9Pb1RHT6fD3v37gUgB8Pee++96N69O2JjYxEbGwuHw4FDhw4BALKzs5Geno7u3bu3WPvixYv9ddvtdowbNw4+nw95eXnNXoeItMFBnkQqsVgsGDt2LMaOHYsnnngCs2fPxpw5czBz5kwcPHgQEyZMwL33
lab,w4-class3.ipynb,markdown,45,3ot58+YhISEBmzZtwp133gm32+2/DaPR2Og2FUVp9jyfz3fWehRFafZ8n8+HIUOGYNmyZU1+l5SUdMbb7NKlC7p06YLZs2fjT3/6E7p3744VK1Zg1qxZZ6yh4evMmTNx/Phx/OMf/0BmZibMZjOGDh3q75qJioo64/37fD7cc889ePDBB5v8LiMj44zXJaLgYsAgCpDevXv71574/vvv4fF4sGDBAuh0suHwnXfeUe2+vv322yY/9+zZs9nLDh48GCtWrED79u0RExNzwfeZlZUFq9XaqAXm0KFDKCwsRIcOHQAAW7ZsgU6n87dIbNy4ES+//DImTJgAACgoKEBJSYn/+v3798fhw4eRk5PTbCvG4MGDsWvXLnTt2vWC6yai4GAXCVErlZaW4sorr8TSpUvx008/IS8vD++++y7mz5+PyZMnA5Cf/D0eD1588UUcOHAAS5YswSuvvKJaDZs3b8b8+fORk5ODf/7zn3j33Xfx0EMPNXvZ6dOnIzExEZMnT8bGjRuRl5eHr7/+Gg899BAOHz7c7HWefPJJPProo1i/fj3y8vKwY8cO3HHHHXC73Rg7dqz/chaLBbfffjt+/PFHbNy4EQ8++CBuuukmpKSkAAC6du2KJUuWYM+ePfjuu+8wffr0Rq0WI0eOxIgRIzBlyhR88cUXyMvLw6efforPPvsMAPDHP/4RW7Zs
lab,w4-class3.ipynb,markdown,46,wW9/+1tkZ2dj3759+PDDD/HAAw+o9VQSkUoYMIhayW6349JLL8Vzzz2HESNGoG/fvnj88cdx11134aWXXgIADBw4EM8++yz+9re/oW/fvli2bBmefvpp1Wp45JFH8MMPP2DQoEGYN28eFixYgHHjxjV7WavVig0bNiAjIwM33HADevXqhTvuuAO1tbUttmiMHDkSBw4cwG233YaePXti/PjxOHbsGNasWYMePXr4L9e1a1fccMMNmDBhAq6++mr07dsXL7/8sv/3r7/+OsrLyzFo0CDMmDEDDz74INq3b9/ovt577z1cfPHFuOWWW9C7d288+uij8Hq9AGQLx9dff419+/bhiiuuwKBBg/D4448jNTW1tU8hEalMEUIIrYsgoguXlZWFhx9+GA8//LCmdTz55JNYtWoVsrOzNa2DiEIDWzCIiIhIdQwYREREpDp2kRAREZHq2IJBREREqmPAICIiItUxYBAREZHqGDCIiIhIdQwYREREpDoGDCIiIlIdAwYRERGpjgGDiIiIVMeAQURERKpjwCAiIiLVMWAQERGR6hgwiIiISHUMGERERKQ6BgwiIiJSHQMGERERqY4Bg4iIiFTHgEFERESqY8AgIiIi1TFgEBERkeoYMIiIiEh1DBhERESkOgYMIiIiUh0DBhEREamOAYOIiIhU9/8DrXpANlWthOcAAAAASUVO
lab,w4-class3.ipynb,markdown,47,RK5CYII=)
lab,w4-class3.ipynb,code,1,"import matplotlib.pyplot as plt
from matplotlib.patches import Circle, Rectangle

# Create a figure and axis which is the 
# explicit way to create a plot

fig, ax = plt.subplots()

# Create a rectangle instance to represent the sample space
# Rectangle(x, y, width, height)
rect = Rectangle((-2, -1.5), 4, 3, linewidth=1.5, edgecolor='black', facecolor='none', label=""Sample Space"")
# Add the rectangle to the plot using the ax instance object
ax.add_patch(rect)

# Create circles to represent Event A and Event B
# Circle((x, y), radius)
circle_a = Circle((-0.8, 0), 1, color='blue', alpha=0.5, label=""Event A"")
circle_b = Circle((0.8, 0), 1, color='orange', alpha=0.5, label=""Event B"")
# Add the c"
lab,w4-class3.ipynb,code,2,"ircles to the plot
# using the ax instance object
ax.add_patch(circle_a)
ax.add_patch(circle_b)

# Add text labels for the regions
ax.text(-0.8, 1.1, ""A"", fontsize=10, color='blue', ha='center')
ax.text(0.8, 1.1, ""B"", fontsize=10, color='orange', ha='center')
ax.text(-1.0, 0, ""A & ¬B"", fontsize=10, color='blue', ha='center')
ax.text(1.0, 0, ""B & ¬A"", fontsize=10, color='orange', ha='center')
ax.text(0, 0, ""A & B"", fontsize=10, color='purple', ha='center')
ax.text(0, -1.8, ""Sample Space"", fontsize=10, color='black', ha='center')

# Set the plot limits to fit everything
ax.set_xlim(-2.5, 2.5)
ax.set_ylim(-2,2)

# Remove axes for a cleaner look
# (also they're incorrect here)
ax.axis('off')

# "
lab,w4-class3.ipynb,code,3,"Add a legend
plt.legend(loc=""upper right"", fontsize=8, frameon=False, borderaxespad=-1.8, borderpad=0.0)
plt.legend
# Set the title
plt.title(""Sample Space and Events"")

# Display the plot
plt.show()"
lab,w4-class3.ipynb,markdown,1,"---

# Probability

The probability of an event $ A $ is given by:

$$P(A) = \frac{\text{Number of outcomes in } A}{\text{Total outcomes in } \Omega}$$



This formula shows that probabilities are essentially **ratios**. The numerator represents the number of favorable outcomes for the event $ A $, while the denominator represents the total number of possible outcomes in the sample space $ \Omega $. 

Because the number of outcomes for the event $ A $ will always be between zero and the number of outcomes in the sample space $ \Omega $, the above equation should always result in a value between 0 and 1

$$0 \leq P(A) \leq 1 $$

where:
- $ P(A) = 0 $: The event $ A $ is impossible.
- $ P(A) ="
lab,w4-class3.ipynb,markdown,2, 1 $: The event $ A $ is certain.
lab,w4-class3.ipynb,markdown,1,"### **Some Axioms of Probability**

1. Non-negativity - probabilities cannot be negative:  
   $$P(A) \geq 0$$

2. Normalization (where $ \Omega $ is the set of all possible outcomes):  
   $$P(\Omega) = 1$$

3. Additivity (if $ A \cap B = \emptyset $ meaning A and B are independent):  
   $$P(A \cup B) = P(A) + P(B)$$

4. Complement Rule: also written $A'$ or $\neg A$
   $$P(A^c) = 1 - P(A)$$
   $$P(A') = 1 - P(A)$$
   $$P(\neg A) = 1 - P(A)$$

5. Inclusion - Exclusion: the union of $P(A)$ and $P(B)$ are the sum of them minus the intersection since we've double counted it summing $ P(A) $ and $ P(B) $ 
   $$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$


#### Calculating Probability

$$P(A) ="
lab,w4-class3.ipynb,markdown,2," \frac{\text{Number of outcomes in } A}{\text{Total outcomes in } \Omega}$$

---"
lab,w4-class3.ipynb,markdown,1,"#### Example: Tossing Two Coins

Let’s consider an experiment where we toss two coins. The sample space is:

$$\Omega = \{(\text{Heads}, \text{Heads}), (\text{Heads}, \text{Tails}), (\text{Tails}, \text{Heads}), (\text{Tails}, \text{Tails})\}$$

Some possible events are:
- $ A = \{(\text{Heads}, \text{Heads})\} $: Both coins show Heads.
- $ B = \{(\text{Heads}, \text{Tails}), (\text{Tails}, \text{Heads})\} $: Exactly one coin shows Heads.
- $ C = \{(\text{Tails}, \text{Tails})\} $: Both coins show Tails.

note: this assumes ordering does not matter."
lab,w4-class3.ipynb,markdown,1,"Let’s calculate the probabilities for each event:

We already know the $ \Omega $

$$\Omega = \{(\text{Heads}, \text{Heads}), (\text{Heads}, \text{Tails}), (\text{Tails}, \text{Heads}), (\text{Tails}, \text{Tails})\}$$

 Number of outcomes in $ \Omega = 4$

1. **Event $ A $: Both coins show Heads**  
   - Outcomes in $ A = \{(\text{Heads}, \text{Heads})\} $
   - Number of outcomes in $ A = 1$
   - Probability:  
     $$P(A) = \frac{1}{4} = 0.25$$

2. **Event $ B $: Exactly one coin shows Heads**  
   - Outcomes in $ B = \{(\text{Heads}, \text{Tails}), (\text{Tails}, \text{Heads})\} $
   - Number of outcomes in $ B = 2$
   - Probability:  
     $$P(B) = \frac{2}{4} = 0.5$$

3. **Event $ C $: "
lab,w4-class3.ipynb,markdown,2,"Both coins show Tails**  
   - Outcomes in $ C = \{(\text{Tails}, \text{Tails})\} $
   - Number of outcomes in $ C = 1$
   - Probability:  
     $$P(C) = \frac{1}{4} = 0.25$$"
lab,w4-class3.ipynb,code,1,"import matplotlib.pyplot as plt

# Define the events and their probabilities
events = [""A: Both Heads"", ""B: Exactly One Head"", ""C: Both Tails""]
probabilities = [0.25, 0.5, 0.25]

# Create the bar chart
# using the implicit method of plotting
# in matplotlib by using the plt object directly
# Set the figure size
plt.figure(figsize=(8, 5))
# Create the bar chart
# using the events as x-axis and probabilities as y-axis
# and setting the colors to mets colors
plt.bar(events, probabilities, color=['blue', 'orange', 'black'], alpha=1)

# Add labels and title
plt.title(""Probability Distribution of Events"", fontsize=14)
plt.xlabel(""Events"", fontsize=12)
plt.ylabel(""Probability"", fontsize=12)
plt.yli"
lab,w4-class3.ipynb,code,2,"m(0, 1)  # Set y-axis range to [0, 1] since probabilities are between 0 and 1

# Add probability values on top of the bars
# enumerate will return both the index and the value
# of the probabilities list in a list of two-tuples
# and we will unpack the tuples into i and prob
for index, prob in enumerate(probabilities):
    plt.text(index, prob + 0.02, f""{prob:.2f}"", ha='center', fontsize=10)

# Show the plot
plt.show()"
lab,w4-class3.ipynb,markdown,1,"---

## **Properties of Events**"
lab,w4-class3.ipynb,markdown,1,"#### 1. **Union of Events ($A \cup B$)**

The **union** of two events $ A $ and $ B $, denoted $ A \cup B $, is the set of outcomes that belong to **either $ A $, $ B $, or both**. In other words:

$$A \cup B = \{x \in \Omega : x \in A \text{ or } x \in B\}$$

This translates to: A union B is equal to the set of outcomes, $x$, from the sample space, $\Omega$, where $x$ is in $A$ or $x$ in $B$

##### Key Points:
- The union includes all outcomes in $ A $, all outcomes in $ B $, and any outcomes that are in both $ A $ and $ B $.
- If $ A $ and $ B $ are **mutually exclusive** (i.e., $ A \cap B = \emptyset $), then $ A \cup B $ contains no overlap."
lab,w4-class3.ipynb,markdown,1,"##### Formula for Probability of Union:
The probability of the union of two events is given by:

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

- The term $ P(A \cap B) $ is subtracted because it is counted twice when adding $ P(A) $ and $ P(B) $ if  $ A \cap B \neq \emptyset$ .

if $A$ and $B$ are mutually exclusive meaning  $ A \cap B = \emptyset $ then $ P(A \cap B) = 0 $ so this becomes

$$P(A \cup B) = P(A) + P(B)$$

##### Example:
Let’s consider tossing two coins:
- Sample space: $ \Omega = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} $
- Event $ A $: At least one Head $ A = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H})\}"
lab,w4-class3.ipynb,markdown,2," $
- Event $ B $: At least one Tail $ B = \{(\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} $

Union of $ A $ and $ B $: the Event where we get at least one Head or one Tail
$$A \cup B = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} = \Omega$$

Probability:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

- $ P(A) = \frac{3}{4}, P(B) = \frac{3}{4}, P(A \cap B) = \frac{2}{4} $ ($A \cap B$ meaning one Head AND one Tail meaning leaving us these outcomes $ \{(\text{H}, \text{T}), (\text{T}, \text{H})\} $)
- $ P(A \cup B) = \frac{3}{4} + \frac{3}{4} - \frac{2}{4} = 1 $

Essentially if $P(A)$ is the probability getting one Head and $P(B)$"
lab,w4-class3.ipynb,markdown,3," is the probability of getting one Tail, $P(A \cup B)$ is the probability of getting one Head OR one Tail. If we flip a coin twice, we know we're certain to get one of those which is confirmed by the math."
lab,w4-class3.ipynb,markdown,1,"#### 2. **Intersection of Events ($A \cap B$)**

The **intersection** of two events $ A $ and $ B $, denoted $ A \cap B $, is the set of outcomes that belong to **both \( A \) and \( B \)**. In other words:

$$A \cap B = \{x \in \Omega : x \in A \text{ and } x \in B\}$$

This translates to: A intersection B is equal to the set of outcomes, $x$, from the sample space, $\Omega$, where $x$ is in $A$ AND $x$ in $B$

##### Key Points:
- The intersection includes only the outcomes that are common to both $ A $ and $ B $.
- If $ A $ and $ B $ are **mutually exclusive**, then $ A \cap B = \emptyset $ (the empty set)."
lab,w4-class3.ipynb,markdown,1,"#### Formula for Probability of Intersection:
The probability of the intersection of two events is given by:

$$P(A \cap B) = P(A) \cdot P(B \mid A)$$

where $ P(B \mid A) $ is the conditional probability of $ B $ given that $ A $ has occurred.

If $A$ and $B$ are independent, $A$ doesn't affect whether or not $B$ occurs meaning $ P(B \mid A) = P(B) $.

which means for independent events A and B the intersection simplifies:
$$ P(A \cap B) = P(A) \cdot P(B) $$ 


##### Example:
Using the same coin toss example:
- Sample space: $ \Omega = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} $
- Event $ A $: At least one Head $ A = \{(\text{H}, \text{H}), ("
lab,w4-class3.ipynb,markdown,2,"\text{H}, \text{T}), (\text{T}, \text{H})\} $
- Event $ B $: At least one Tail $ B = \{(\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} $

Intersection of $ A $ and $ B $: the Event where we get at least one Head AND one Tail
$$A \cap B = \{(\text{H}, \text{T}), (\text{T}, \text{H})\}  $$

Probability:
$$ P(A \cap B) = P(A) \cdot P(B \mid A) $$
- $ P(A) = \frac{3}{4}, P(B) = \frac{3}{4}, P(B|A)=\frac{2}{3} $
- $ P(A \cap B) = \frac{3}{4} * \frac{2}{3} = \frac{6}{12} = 0.5  $


**Calculate $ P(B \mid A) $:**
- If $ A $ has occurred (at least one Head), the restricted sample space is $ A = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H})\} $.
- Within this r"
lab,w4-class3.ipynb,markdown,3,"estricted sample space, the outcomes in $ B $ (at least one Tail) are $ B \cap A = \{(\text{H}, \text{T}), (\text{T}, \text{H})\} $.
- Number of outcomes in $ B \cap A = 2 $
- Total outcomes in $ A = 3 $
- $ P(B \mid A) = \frac{2}{3} $"
lab,w4-class3.ipynb,markdown,1,"#### **Joint Probability**

From the definition we just saw:

$$P(A \cap B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)$$

---"
lab,w4-class3.ipynb,markdown,1,"### **Conditional Probability**

Conditional probability is the probability of an event $ A $ occurring **given that** another event $ B $ has already occurred. It allows us to focus on a subset of the sample space where $ B $ has occurred and then determine the likelihood of $ A $ within that subset.

The formula for conditional probability is:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad \text{if } P(B) > 0$$

Where:
- $ P(A|B) $: The probability of $ A $ given $ B $.
- $ P(A \cap B) $: The probability of both $ A $ and $ B $ occurring (their intersection).
- $ P(B) $: The probability of $ B $ occurring.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Key Insights**
1. **Subset of the Sample Space**:
   - When we condition on $ B $, we restrict our attention to the outcomes where $ B $ occurs. The conditional probability $ P(A|B) $ measures the proportion of these outcomes that also belong to $ A $.

2. **Relationship to Intersection**:
   - The formula $ P(A|B) = \frac{P(A \cap B)}{P(B)} $ can be rearranged to express the intersection:
     $$P(A \cap B) = P(A|B) \cdot P(B)$$
   - This is useful for calculating joint probabilities when conditional probabilities are known.

3. **Symmetry**:
   - Conditional probability is not symmetric. $ P(A|B) \neq P(B|A) $ in general, unless $ A $ and $ B $ are independent.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Example 1: Drawing Cards**
Let’s consider drawing two cards from a standard deck of 52 cards **without replacement**:
- Event $ A $: The first card is a King.
- Event $ B $: The second card is a King.

**Step 1: Calculate $ P(A) $:**
- Number of Kings = 4
- Total cards = 52
- $ P(A) = \frac{4}{52} = \frac{1}{13} $

**Step 2: Calculate $ P(B|A) $:**
- If $ A $ has occurred (the first card is a King), there are now 3 Kings left in a deck of 51 cards:
  $$P(B|A) = \frac{3}{51} = \frac{1}{17}$$

**Step 3: Calculate $ P(A \cap B) $:**
- Using the formula $ P(A \cap B) = P(A) \cdot P(B|A) $:
  $$P(A \cap B) = \frac{1}{13} \cdot \frac{1}{17} = \frac{1}{221}$$

**Interpretation**:
- The proba"
lab,w4-class3.ipynb,markdown,2,"bility of drawing two Kings in a row is $ \frac{1}{221} $.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Example 2: Rolling Two Dice**
Let’s consider rolling two six-sided dice:
- Sample space: $ \Omega = \{(1,1), (1,2), (1,3), \dots, (6,6)\} $ (36 total outcomes)
- Event $ A $: The sum of the two dice is 7.
- Event $ B $: The first die shows an odd number.


**Step 1: Calculate $ P(B) $:**
- Outcomes where the first die shows an odd number: $ B = \{(1,x), (3,x), (5,x)\} $ for $ x = 1, 2, 3, 4, 5, 6 $.
- Number of outcomes in $ B = 18 $ (3 odd numbers for the first die, each paired with 6 outcomes for the second die).
- Total outcomes in $ \Omega = 36 $.
- $ P(B) = \frac{18}{36} = \frac{1}{2} $.


**Step 2: Calculate $ P(A \cap B) $:**
- Event $ A $: The sum of the two dice is 7. Possibl"
lab,w4-class3.ipynb,markdown,2,"e outcomes:
  $$A = \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}$$
  (6 outcomes total).

- Event $ A \cap B $: The sum is 7 **and** the first die shows an odd number:
  $$A \cap B = \{(1,6), (3,4), (5,2)\}$$
  (3 outcomes total).

- $ P(A \cap B) = \frac{\text{Number of outcomes in } A \cap B}{\text{Total outcomes in } \Omega} = \frac{3}{36} = \frac{1}{12} $.


**Step 3: Calculate $ P(A|B) $:**
- Using the formula $ P(A|B) = \frac{P(A \cap B)}{P(B)} $:
  $$P(A|B) = \frac{\frac{1}{12}}{\frac{1}{2}} = \frac{1}{12} \cdot \frac{2}{1} = \frac{2}{12} = \frac{1}{6}$$


**Interpretation**:
- Given that $ B $ (the first die shows an odd number) has occurred, the probability of $ A $ (the sum of the "
lab,w4-class3.ipynb,markdown,3,"two dice is 7) is $ \frac{1}{6} $.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Conditional Probability and Independence**
1. **Independent Events**:
   - If $ A $ and $ B $ are independent, then:
     $$P(A|B) = P(A) \quad \text{and} \quad P(B|A) = P(B)$$
   - This means the occurrence of $ B $ does not affect the probability of $ A $, and vice versa.

2. **Dependent Events**:
   - If $ A $ and $ B $ are dependent, then:
     $$P(A|B) \neq P(A) \quad \text{or} \quad P(B|A) \neq P(B)$$
   - This means the occurrence of one event changes the probability of the other.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Key Takeaways**
1. **Conditional Probability**:
   - Focuses on a subset of the sample space where the given condition occurs.
   - Formula: $ P(A|B) = \frac{P(A \cap B)}{P(B)} $.

2. **Intersection**:
   - The formula $ P(A \cap B) = P(A|B) \cdot P(B) $ is derived from conditional probability.

3. **Independence**:
   - If $ A $ and $ B $ are independent, $ P(A|B) = P(A) $.

Conditional probability is a powerful tool for analyzing relationships between events and solving real-world probability problems.---

### **Conditional Probability**

Conditional probability is the probability of an event $ A $ occurring **given that** another event $ B $ has already occurred. It allows us to fo"
lab,w4-class3.ipynb,markdown,2,"cus on a subset of the sample space where $ B $ has occurred and then determine the likelihood of $ A $ within that subset.

The formula for conditional probability is:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad \text{if } P(B) > 0$$

Where:
- $ P(A|B) $: The probability of $ A $ given $ B $.
- $ P(A \cap B) $: The probability of both $ A $ and $ B $ occurring (their intersection).
- $ P(B) $: The probability of $ B $ occurring.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Key Insights**
1. **Subset of the Sample Space**:
   - When we condition on $ B $, we restrict our attention to the outcomes where $ B $ occurs. The conditional probability $ P(A|B) $ measures the proportion of these outcomes that also belong to $ A $.

2. **Relationship to Intersection**:
   - The formula $ P(A|B) = \frac{P(A \cap B)}{P(B)} $ can be rearranged to express the intersection:
     $$P(A \cap B) = P(A|B) \cdot P(B)$$
   - This is useful for calculating joint probabilities when conditional probabilities are known.

3. **Symmetry**:
   - Conditional probability is not symmetric. $ P(A|B) \neq P(B|A) $ in general, unless $ A $ and $ B $ are independent.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Example 1: Tossing Two Coins**
Let’s consider tossing two coins:
- Sample space: $ \Omega = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} $
- Event $ A $: Getting Heads on the first coin $ A = \{(\text{H}, \text{H}), (\text{H}, \text{T})\} $
- Event $ B $: Getting at least one Heads $ B = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H})\} $

**Step 1: Calculate $ P(B) $:**
- Outcomes in $ B = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H})\} $
- Number of outcomes in $ B = 3 $
- Total outcomes in $ \Omega = 4 $
- $ P(B) = \frac{3}{4} $

**Step 2: Calculate $ P(A \cap B) $:**
- Outcomes in $ A \cap B = \{"
lab,w4-class3.ipynb,markdown,2,"(\text{H}, \text{H}), (\text{H}, \text{T})\} $
- Number of outcomes in $ A \cap B = 2 $
- $ P(A \cap B) = \frac{2}{4} = 0.5 $

**Step 3: Calculate $ P(A|B) $:**
- Using the formula $ P(A|B) = \frac{P(A \cap B)}{P(B)} $:
  $$P(A|B) = \frac{0.5}{0.75} = \frac{2}{3}$$

**Interpretation**:
- Given that $ B $ (at least one Heads) has occurred, the probability of $ A $ (Heads on the first coin) is $ \frac{2}{3} $.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Example 2: Drawing Cards**
Let’s consider drawing two cards from a standard deck of 52 cards **without replacement**:
- Event $ A $: The first card is a King.
- Event $ B $: The second card is a King.

**Step 1: Calculate $ P(A) $:**
- Number of Kings = 4
- Total cards = 52
- $ P(A) = \frac{4}{52} = \frac{1}{13} $

**Step 2: Calculate $ P(B|A) $:**
- If $ A $ has occurred (the first card is a King), there are now 3 Kings left in a deck of 51 cards:
  $$P(B|A) = \frac{3}{51} = \frac{1}{17}$$

**Step 3: Calculate $ P(A \cap B) $:**
- Using the formula $ P(A \cap B) = P(A) \cdot P(B|A) $:
  $$P(A \cap B) = \frac{1}{13} \cdot \frac{1}{17} = \frac{1}{221}$$

**Interpretation**:
- The proba"
lab,w4-class3.ipynb,markdown,2,"bility of drawing two Kings in a row is $ \frac{1}{221} $.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Conditional Probability and Independence**
1. **Independent Events**:
   - If $ A $ and $ B $ are independent, then:
     $$P(A|B) = P(A) \quad \text{and} \quad P(B|A) = P(B)$$
   - This means the occurrence of $ B $ does not affect the probability of $ A $, and vice versa.

2. **Dependent Events**:
   - If $ A $ and $ B $ are dependent, then:
     $$P(A|B) \neq P(A) \quad \text{or} \quad P(B|A) \neq P(B)$$
   - This means the occurrence of one event changes the probability of the other.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Key Takeaways**
1. **Conditional Probability**:
   - Focuses on a subset of the sample space where the given condition occurs.
   - Formula: $ P(A|B) = \frac{P(A \cap B)}{P(B)} $.

2. **Intersection**:
   - The formula $ P(A \cap B) = P(A|B) \cdot P(B) $ is derived from conditional probability.

3. **Independence**:
   - If $ A $ and $ B $ are independent, $ P(A|B) = P(A) $.

Conditional probability is a powerful tool for analyzing relationships between events and solving real-world probability problems."
lab,w4-class3.ipynb,markdown,1,"---

### **Independent and Dependent Events**

#### **Independent Events**
Two events \( A \) and \( B \) are **independent** if the occurrence of one event does not affect the probability of the other. Mathematically, $ A $ and $ B $ are independent if:

$$P(A \cap B) = P(A) \cdot P(B)$$

Alternatively, independence can also be expressed using conditional probability:

$$P(A|B) = P(A) \quad \text{and} \quad P(B|A) = P(B)$$

This means:
- The probability of $ A $ occurring is the same whether or not $ B $ has occurred.
- The probability of $ B $ occurring is the same whether or not $ A $ has occurred.

##### Example: Tossing Two Coins
Let’s consider tossing two coins:
- Sample space: $ \Omeg"
lab,w4-class3.ipynb,markdown,2,"a = \{(\text{H}, \text{H}), (\text{H}, \text{T}), (\text{T}, \text{H}), (\text{T}, \text{T})\} $
- Event $ A $: Getting Heads on the first coin $ A = \{(\text{H}, \text{H}), (\text{H}, \text{T})\} $
- Event $ B $: Getting Heads on the second coin $ B = \{(\text{H}, \text{H}), (\text{T}, \text{H})\} $

**Are \( A \) and \( B \) independent?**
1. Calculate \( P(A) \):
   $$P(A) = \frac{\text{Number of outcomes in } A}{\text{Total outcomes in } \Omega} = \frac{2}{4} = 0.5$$

2. Calculate \( P(B) \):
   $$P(B) = \frac{\text{Number of outcomes in } B}{\text{Total outcomes in } \Omega} = \frac{2}{4} = 0.5$$

3. Calculate \( P(A \cap B) \):
   $$A \cap B = \{(\text{H}, \text{H})\}$$
   $$P(A \cap B"
lab,w4-class3.ipynb,markdown,3,") = \frac{\text{Number of outcomes in } A \cap B}{\text{Total outcomes in } \Omega} = \frac{1}{4} = 0.25$$

4. Verify Independence:
   $$P(A \cap B) = P(A) \cdot P(B)$$
   $$0.25 = 0.5 \cdot 0.5$$

Since the equality holds, \( A \) and \( B \) are **independent**.

---"
lab,w4-class3.ipynb,markdown,1,"#### **Dependent Events**
Two events $ A $ and $ B $ are **dependent** if the occurrence of one event affects the probability of the other. Mathematically, $ A $ and $ B $ are dependent if:

$$P(A \cap B) \neq P(A) \cdot P(B)$$

Or equivalently:

$$P(A|B) \neq P(A) \quad \text{or} \quad P(B|A) \neq P(B)$$

This means:
- The probability of $ A $ occurring changes depending on whether $ B $ has occurred.
- The probability of $ B $ occurring changes depending on whether $ A $ has occurred.

##### Example: Drawing Cards Without Replacement
Let’s consider drawing two cards from a standard deck of 52 cards **without replacement**:
- Event $ A $: The first card is a King.
- Event $ B $: The second "
lab,w4-class3.ipynb,markdown,2,"card is a King.

**Are $ A $ and $ B $ dependent?**
1. Calculate $ P(A) $:
   $$P(A) = \frac{\text{Number of Kings}}{\text{Total cards}} = \frac{4}{52} = \frac{1}{13}$$

2. Calculate $ P(B|A) $:
   If $ A $ has occurred (the first card is a King), there are now 3 Kings left in a deck of 51 cards:
   $$P(B|A) = \frac{\text{Number of remaining Kings}}{\text{Remaining cards}} = \frac{3}{51}$$

3. Compare $ P(B|A) $ and $ P(B) $:
   - $ P(B) $: If no information about $ A $ is given, the probability of drawing a King on the second card is:
     $$P(B) = \frac{4}{52} = \frac{1}{13}$$
   - $ P(B|A) = \frac{3}{51} \neq P(B) $

Since $ P(B|A) \neq P(B) $, the events $ A $ and $ B $ are **dependent**"
lab,w4-class3.ipynb,markdown,3,".

---"
lab,w4-class3.ipynb,markdown,1,"#### **Key Takeaways**
1. **Independent Events**:
   - The occurrence of one event does not affect the probability of the other.
   - Use $ P(A \cap B) = P(A) \cdot P(B) $.

2. **Dependent Events**:
   - The occurrence of one event affects the probability of the other.
   - Use $ P(A \cap B) = P(A) \cdot P(B|A) $ or $ P(B \cap A) = P(B) \cdot P(A|B) $.

Understanding the distinction between independent and dependent events is crucial for solving probability problems accurately."
lab,w4-class3.ipynb,markdown,1,"---

### **1. Mutually Exclusive Events**

Two events $ A $ and $ B $ are **mutually exclusive** (or disjoint) if they cannot occur at the same time. This means that the intersection of $ A $ and $ B $ is empty:

$$A \cap B = \emptyset$$

In terms of probability, this implies:

$$P(A \cap B) = 0$$


#### **Key Characteristics**
1. **No Overlap**:
   - If $ A $ and $ B $ are mutually exclusive, there are no outcomes that belong to both $ A $ and $ B $.
   - Example: In rolling a die:
     - $ A = \{\text{even numbers}\} = \{2, 4, 6\} $
     - $ B = \{\text{odd numbers}\} = \{1, 3, 5\} $
     - $ A \cap B = \emptyset $, because no number can be both even and odd.

2. **Additivity**:
   - For m"
lab,w4-class3.ipynb,markdown,2,"utually exclusive events:
     $$P(A \cup B) = P(A) + P(B)$$

3. **Conditional Probability**:
   - If $ A $ and $ B $ are mutually exclusive:
     $$P(A|B) = \frac{P(A \cap B)}{P(B)} = 0$$
   - This means that if $ B $ has occurred, the probability of $ A $ is 0.


#### **Example: Tossing a Coin**
- Sample space: $ \Omega = \{\text{Heads}, \text{Tails}\} $
- Event $ A $: Getting Heads $ A = \{\text{Heads}\} $
- Event $ B $: Getting Tails $ B = \{\text{Tails}\} $

1. **Are $ A $ and $ B $ Mutually Exclusive?**
   - Yes, because $ A \cap B = \emptyset $.

2. **Calculate $ P(A \cup B) $:**
   - Since $ A $ and $ B $ are mutually exclusive:
     $$P(A \cup B) = P(A) + P(B)$$
   - If $ P(A) = 0.5"
lab,w4-class3.ipynb,markdown,3," $ and $ P(B) = 0.5 $, then:
     $$P(A \cup B) = 0.5 + 0.5 = 1$$


#### **Key Takeaways**
1. **Mutually Exclusive Events**:
   - Cannot occur at the same time ($ A \cap B = \emptyset $).
   - $ P(A \cap B) = 0 $.

2. **Additivity**:
   - For mutually exclusive events, $ P(A \cup B) = P(A) + P(B) $.

3. **Conditional Probability**:
   - If $ A $ and $ B $ are mutually exclusive, $ P(A|B) = 0 $.

Mutually exclusive events simplify probability calculations and are a fundamental concept in probability theory.

---"
lab,w4-class3.ipynb,markdown,1,"### **Law of Total Probability**

The **Law of Total Probability** allows us to calculate the probability of an event $ B $ by considering all possible ways $ B $ can occur, partitioned by another event $ A $ and its complement $ \neg A $. It is expressed as:

$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$

#### **Key Insights**
1. **Partitioning the Sample Space**:
   - The sample space $ \Omega $ is divided into two disjoint parts: $ A $ and $ \neg A $ (the complement of $ A $).
   - Since $ A \cup \neg A = \Omega $, any event $ B $ must occur in one of these two partitions.

2. **Weighted Contributions**:
   - The probability of $ B $ is calculated as the sum of its probabilit"
lab,w4-class3.ipynb,markdown,2,"ies in each partition, weighted by the probabilities of $ A $ and $ \neg A $.

---

#### **Example: Diagnosing a Disease**
Let’s consider a medical test for a disease:
- Event $ A $: The person has the disease.
- Event $ \neg A $: The person does not have the disease.
- Event $ B $: The test result is positive.

**Given**:
- $ P(A) = 0.01 $ (1% of the population has the disease).
- $ P(\neg A) = 1 - P(A) = 0.99 $ (99% of the population does not have the disease).
- $ P(B|A) = 0.95 $ (the test correctly identifies the disease 95% of the time).
- $ P(B|\neg A) = 0.05 $ (the test gives a false positive 5% of the time).

**Step 1: Apply the Law of Total Probability**:
Using the formula:
$$P(B) ="
lab,w4-class3.ipynb,markdown,3," P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$

Substitute the values:
$$P(B) = (0.95 \cdot 0.01) + (0.05 \cdot 0.99)$$
$$P(B) = 0.0095 + 0.0495 = 0.059$$

**Interpretation**:
- The probability of a positive test result $ P(B) $ is 0.059 (5.9%)."
lab,w4-class3.ipynb,markdown,1,"---

### **Bayes' Theorem**

Bayes' Theorem is a fundamental concept in probability that allows us to calculate the probability of an event $ A $ given that another event $ B $ has occurred. It is particularly useful for updating probabilities based on new information.

The formula for Bayes' Theorem is:

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}, \quad \text{if } P(B) > 0$$

Where:
- $ P(A|B) $: The probability of $ A $ given $ B $ (posterior probability).
- $ P(B|A) $: The probability of $ B $ given $ A $ (likelihood).
- $ P(A) $: The prior probability of $ A $ (before observing $ B $).
- $ P(B) $: The total probability of $ B $ (normalizing constant).

---"
lab,w4-class3.ipynb,markdown,1,"#### **Key Insights**

1. **Posterior Probability**:
   - Bayes' Theorem calculates the **posterior probability**, which is the updated probability of $ A $ after observing $ B $.
   - The posterior probability reflects how our belief about $ A $ changes in light of new evidence $ B $.
   - **Example**:
     - Suppose 1% of a population has a disease ($ P(A) = 0.01 $), and a test has a 95% chance of detecting the disease if present ($ P(B|A) = 0.95 $). If the test result is positive ($ B $), Bayes' Theorem allows us to calculate the updated probability that the person has the disease ($ P(A|B) $). This updated probability is the **posterior**.

---

2. **Prior and Likelihood**:
   - The **pr"
lab,w4-class3.ipynb,markdown,2,"ior probability** $ P(A) $ represents our initial belief about $ A $ before observing any evidence.
     - **Example**: If 1% of the population has a disease, $ P(A) = 0.01 $ is the prior probability of having the disease.
   - The **likelihood** $ P(B|A) $ measures how likely the evidence $ B $ is if $ A $ is true.
     - **Example**: If the test correctly identifies the disease 95% of the time, $ P(B|A) = 0.95 $ is the likelihood of a positive test result given that the person has the disease.

   Together, the prior and likelihood are combined in Bayes' Theorem to calculate the posterior probability:
   $$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

---

3. **Normalization**:
   - The denom"
lab,w4-class3.ipynb,markdown,3,"inator $ P(B) $ ensures that the probabilities sum to 1. It is calculated using the **Law of Total Probability**:
     $$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$
   - This accounts for all possible ways that $ B $ can occur, whether $ A $ is true or not.
   - **Example**:
     - In the medical test scenario:
       - $ P(B|A) = 0.95 $ (positive test given disease).
       - $ P(B|\neg A) = 0.05 $ (false positive rate).
       - $ P(A) = 0.01 $ (prior probability of disease).
       - $ P(\neg A) = 0.99 $ (prior probability of no disease).
       - Using the Law of Total Probability:
         $$P(B) = (0.95 \cdot 0.01) + (0.05 \cdot 0.99) = 0.0095 + 0.0495 = 0.059$$
     - Thi"
lab,w4-class3.ipynb,markdown,4,"s ensures that the total probability of a positive test result is correctly normalized.

---

#### **Expanded Example: Diagnosing a Disease**

Let’s revisit the medical test example with all components explained:
- Event $ A $: The person has the disease.
- Event $ \neg A $: The person does not have the disease.
- Event $ B $: The test result is positive.

**Given**:
- $ P(A) = 0.01 $ (prior probability of having the disease).
- $ P(\neg A) = 0.99 $ (prior probability of not having the disease).
- $ P(B|A) = 0.95 $ (likelihood of a positive test given the disease).
- $ P(B|\neg A) = 0.05 $ (likelihood of a false positive test).

**Step 1: Calculate $ P(B) $ (Normalization)**:
Using the Law o"
lab,w4-class3.ipynb,markdown,5,"f Total Probability:
$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$
$$P(B) = (0.95 \cdot 0.01) + (0.05 \cdot 0.99)$$
$$P(B) = 0.0095 + 0.0495 = 0.059$$

**Step 2: Calculate $ P(A|B) $ (Posterior Probability)**:
Using Bayes' Theorem:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
$$P(A|B) = \frac{0.95 \cdot 0.01}{0.059}$$
$$P(A|B) = \frac{0.0095}{0.059} \approx 0.161$$

**Interpretation**:
- Even with a positive test result, the probability that the person actually has the disease is only 16.1%. This is because the disease is very rare, and false positives contribute significantly to the positive test results.

---

#### **Key Takeaways**
1. **Posterior Probability**:
   - Bayes' The"
lab,w4-class3.ipynb,markdown,6,"orem updates the prior probability based on new evidence to calculate the posterior probability.

2. **Prior and Likelihood**:
   - The prior represents initial beliefs, while the likelihood measures how well the evidence supports the hypothesis.

3. **Normalization**:
   - The Law of Total Probability ensures that the probabilities are properly scaled and sum to 1.

Bayes' Theorem is a powerful tool for reasoning under uncertainty and is widely used in fields like medicine, machine learning, and decision-making."
lab,w4-class3.ipynb,markdown,1,"#### **Example: Diagnosing a Disease**
Let’s revisit the medical test example:
- Event $ A $: The person has the disease.
- Event $ \neg A $: The person does not have the disease.
- Event $ B $: The test result is positive.

**Given**:
- $ P(A) = 0.01 $ (1% of the population has the disease).
- $ P(\neg A) = 0.99 $ (99% of the population does not have the disease).
- $ P(B|A) = 0.95 $ (the test correctly identifies the disease 95% of the time).
- $ P(B|\neg A) = 0.05 $ (the test gives a false positive 5% of the time).

**Step 1: Calculate $ P(B) $ (Total Probability of a Positive Test)**:
Using the Law of Total Probability:
$$P(B) = P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)$$

Substitu"
lab,w4-class3.ipynb,markdown,2,"te the values:
$$P(B) = (0.95 \cdot 0.01) + (0.05 \cdot 0.99)$$
$$P(B) = 0.0095 + 0.0495 = 0.059$$

**Step 2: Apply Bayes' Theorem**:
Using the formula:
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

Substitute the values:
$$P(A|B) = \frac{0.95 \cdot 0.01}{0.059}$$
$$P(A|B) = \frac{0.0095}{0.059} \approx 0.161$$

**Interpretation**:
- Given a positive test result, the probability that the person actually has the disease is approximately 16.1%.

---"
lab,w5-class2.ipynb,code,1,"!echo ""hello world"""
lab,w5-class2.ipynb,code,1,"import bs4

type(bs4)"
lab,w5-class2.ipynb,code,1,globals()
lab,w5-class2.ipynb,code,1,"variable = ""s""

globals()"
lab,w5-class2.ipynb,code,1,locals()
lab,w5-class2.ipynb,code,1,"def func():
    s = ""within the function""
    print(locals())

func()"
lab,w5-class2.ipynb,code,1,!pip install bs4
lab,w5-class2.ipynb,code,1,"import requests

from bs4 import BeautifulSoup"
lab,w5-class2.ipynb,code,1,"ycom = ""https://news.ycombinator.com/"""
lab,w5-class2.ipynb,code,1,"# we know we want to target multiple pages
# but we need to get one page working first
# and then we can attempt to deal with pagination

# get method sends a request asking to get back data
# post method sends a request containing data 

# 2xx - success
# 3xx - redirection
# 4xx - application
# 5xx - server 


response = requests.get(ycom)
response.text
#response.content"
lab,w5-class2.ipynb,code,1,"# types of control structures
# if statement
# for loops
# while loops
# with statement 




if not response.status_code == 200:
    print(""error sending get request to web server"")
else:
    # we get here if 
    # response.status == 200
    soup = BeautifulSoup(response.content, ""html.parser"")

      
# soup.find_all(name, attribute)
#soup.find_all(class_=""title"")"
lab,w5-class2.ipynb,code,1,"# BeautifulSoup() # capital B implies Class so this creates an instance#
#func() # this is a function call because its lowercase"
lab,w5-class2.ipynb,code,1,"articles = soup.find_all(class_=""titleline"")


a_tags = []
for article in articles:
    anchor = article.find('a')
    # print(anchor.text)
    a_tags.append(anchor.text)

a_tags"
lab,w5-class2.ipynb,code,1,"def get_titles_from_page(variable_url: str):
    response = requests.get(variable_url)
    
    if not response.status_code == 200:
        print(""error sending get request to web server"")
    else:
    # we get here if 
    # response.status == 200
        soup = BeautifulSoup(response.content, ""html.parser"")

    articles = soup.find_all(class_=""titleline"")
    a_tags = []

    for article in articles:
        anchor = article.find('a')
        # print(anchor.text)
        a_tags.append(anchor.text)
    
    return a_tags
 
# https://news.ycombinator.com/?p=1
# https://news.ycombinator.com/?p=2
# https://news.ycombinator.com/?p=3
# https://news.ycombinator.com/?p=4
# https://news.ycombinat"
lab,w5-class2.ipynb,code,2,"or.com/?p= 5

URL = ""https://news.ycombinator.com/?p=""



# get_titles_from_page(URL)
URL + ""3"""
lab,w5-class2.ipynb,code,1,"base_url = ""https://news.ycombinator.com/?p=""

all_the_titles = []
for i in range(1,6):
    url = base_url + str(i)
    list_of_titles = get_titles_from_page(url)
    all_the_titles += list_of_titles

print(all_the_titles)"
lab,w5-class2.ipynb,code,1,len(all_the_titles)
lab,w5-class2.ipynb,code,1,int('34')
lab,w5-class2.ipynb,code,1,"""a RAndom Sstring"".upper()"
lab,w5-class2.ipynb,code,1,"s = ""a RAndom Sstring""

s.lower()"
lab,w5-class2.ipynb,code,1,"""aRAndom@Sstring.com"".partition('@')

#('a RAn' , 'd'  ,'om Sstring' )"
lab,w5-class2.ipynb,code,1,"""replaceable text"".replace(""a"", ""#"")"
lab,w5-class2.ipynb,code,1,"""123456"".find('!')"
lab,w5-class2.ipynb,code,1,"""123456"".index('!')"
lab,w5-class2.ipynb,code,1,"""   sdfsf    "".strip()"
lab,w5-class2.ipynb,code,1,"""dunder"".endswith('__')"
lab,w5-class2.ipynb,code,1,"""8"".isdecimal()"
lab,w6-class2.ipynb,code,1,import this
lab,w6-class2.ipynb,code,1,import antigravity
lab,w6-class2.ipynb,code,1,"import pandas as pd

# Dictionary to create a Series
data = {""A"": 1, 'B': 2, 'C': 3, 'D': 4}

# Create the Series
series = pd.Series(data)

# Display the Series
print(series)

series.loc[""A""] # here we're indexing by the label that was set
series.iloc[0] # here we're indexing by the actual numerical index"
lab,w6-class2.ipynb,code,1,"import pandas as pd

# Dictionary to create a Series
data = {'A': 1, 'B': 2, 'C': 3, 'D': 4}

# Create the Series with dictionary data
# Each key becomes an index label in the Series
series = pd.Series(data,)

# Display the Series
print(series)"
lab,w6-class2.ipynb,code,1,"import pandas as pd
from io import StringIO

# Fake CSV data (as a string)
csv_data = """"""
Name,Age,City
Alice,25,New York
Bob,30,Los Angeles
Charlie,35,Chicago
David,40,Houston
""""""

# Use StringIO to simulate a CSV file
fake_csv = StringIO(csv_data)

# Read the fake CSV data into a DataFrame
df_csv = pd.read_csv(fake_csv)

# Display the DataFrame
print(df_csv)"
lab,w6-class2.ipynb,code,1,"import pandas as pd

# Dictionary to create a DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}

# Create the DataFrame
df = pd.DataFrame(data)

# Display the DataFrame

print(df, ""\n\n"", df.head(2), ""\n\n"", df.tail(2))"
lab,w6-class2.ipynb,code,1,df.dtypes
lab,w6-class2.ipynb,code,1,"# this will give us descriptive statistics

df.describe()

df.iloc[:3]"
lab,w6-class2.ipynb,code,1,"# to get info about the dataframe

df.info()"
lab,w6-class2.ipynb,code,1,"df_sorted = df.sort_values('Age', ascending=False)

print(df_sorted)"
lab,w6-class2.ipynb,code,1,"df_with_nan = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', None],
    'Age': [25, 30, None, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
})

print(df_with_nan, ""\n"")

# Drop rows with any NaN values
df_dropped = df_with_nan.dropna()
print(df_dropped)"
lab,w6-class2.ipynb,code,1,"df_filled = df_with_nan.fillna(0)
print(df_filled)"
lab,w6-class2.ipynb,code,1,"df_stored = df_with_nan[""Name""].fillna(""noname"")

print(df_stored)"
lab,w6-class2.ipynb,code,1,"data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}

# Create the DataFrame
df = pd.DataFrame(data)

df_dropped_row = df.drop(1)
print(df_dropped_row, ""\n"")

# Could also do it like this
df_dropped_row = df.drop(1, axis=0)
print(df_dropped_row, ""\n"")

# Drop the 'City' column
df_dropped_column = df.drop('City', axis=1)
print(df_dropped_column)"
lab,w6-class2.ipynb,code,1,"data = {
    'Name': ['Alice', 'Bob', 'Bob', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}

# Create the DataFrame
df = pd.DataFrame(data)

df_replaced = df.replace({'Name': {'Bob': 'Robert'}})
print(df_replaced)"
lab,w6-class2.ipynb,code,1,"data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}

# Create the DataFrame
df = pd.DataFrame(data)
print(df, ""\n"")

# Access the first row and first column using iloc
print(df.iloc[0, 0], ""\n"")  # First row, first column (Alice)

# Access the first 2 rows and all columns
print(df.iloc[0:2, :2])"
lab,w6-class2.ipynb,code,1,"data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}

# Create the DataFrame
df = pd.DataFrame(data)
print(df, ""\n"")

# First set the index to letter labels A, B, C, D
df.index = ['A', 'B', 'C', 'D']
print(df, ""\n"")

# Access data for row 'B' and column 'Age' using loc
print(""df.loc['B', 'Age'] = "", df.loc['B', 'Age'], ""\n"")  # Row 'B', column 'Age'

# Access the first two rows and all columns
# this should fail
print(""df.iloc[0:1, :] returns "")
print(df.iloc[0:1, :])"
lab,w6-class2.ipynb,code,1,"import pandas as pd
import numpy as np

# Creating a DataFrame with employee data
data = {
    'Employee_ID': [101, 102, 103, 104, 105],
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],
    'Age': [25, 30, 35, np.nan, 40],
    'Salary': [55000, 70000, 80000, 60000, 120000],
    'Department': ['HR', 'IT', 'Finance', 'Marketing', 'IT'],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'New York']
}

df = pd.DataFrame(data)
df"
lab,w6-class2.ipynb,code,1,"df_it = df[df['Department'] == 'IT']
print(df_it)

# print(df['Department'] == 'IT')"
lab,w6-class2.ipynb,code,1,"df_nan_age = df[df['Age'].isna()]
print(df_nan_age)"
lab,w6-class2.ipynb,code,1,"df_age_30_plus = df[df['Age'] > 30]
print(df_age_30_plus)"
lab,w6-class2.ipynb,code,1,"df_it_high_salary = df[(df['Salary'] > 60000) & (df['Department'] == 'IT')]
print(df_it_high_salary)"
lab,w6-class2.ipynb,code,1,"df_it_or_high_salary = df[(df['Department'] == 'IT') | (df['Salary'] < 90000)]
print(df_it_or_high_salary)"
lab,w6-class2.ipynb,code,1,"df_new_city = df[df['City'].str.contains('New', na=False)]
print(df_new_city)"
lab,w6-class2.ipynb,code,1,"df_nan_age = df[df['Age'].isna()]
print(df_nan_age)"
lab,w6-class2.ipynb,code,1,"df_dept_avg_salary = df.groupby('Department')['Salary'].mean()

print(df_dept_avg_salary)"
lab,w6-class2.ipynb,code,1,"from IPython.display import display

df_renamed = df.rename(columns={'Age': 'Employee_Age', 'Salary': 'Annual_Salary'})

display(df_renamed)"
lab,w6-class2.ipynb,code,1,"# Plot a bar chart of average salary by department
df.groupby('Department')['Salary'].mean().plot(kind='bar')"
lab,w7-class3.ipynb,code,1,"import pandas as pd
import numpy as np
import seaborn as sns
import warnings

from IPython.display import display

import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
# Set visualization styles
plt.style.use('fivethirtyeight')
sns.set(style=""darkgrid"")
%matplotlib inline"
lab,w7-class3.ipynb,code,1,"#---------- 1. BASICS ----------#

# Creating a Series
s = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
print(""Series example:"")
print(s)"
lab,w7-class3.ipynb,code,1,"# Creating a DataFrame from a dictionary
data = {
    'Name': ['John', 'Anna', 'Peter', 'Linda'],
    'Age': [28, 34, 29, 42],
    'City': ['New York', 'Paris', 'Berlin', 'London'],
    'Salary': [50000, 60000, 55000, 70000]
}
df = pd.DataFrame(data)
print(""\nDataFrame example:"")
display(df)"
lab,w7-class3.ipynb,code,1,"from io import StringIO

csvdata = """"""
product_id,product_name,category,price,in_stock
1001,Laptop,Electronics,1200.50,True
1002,Desk Chair,Furniture,150.75,True
1003,Coffee Maker,Appliances,89.99,True
1004,Wireless Mouse,Electronics,24.99,False
1005,Bookshelf,Furniture,175.00,True
1006,Blender,Appliances,49.95,False
1007,Monitor,Electronics,299.99,True
1008,Office Desk,Furniture,225.50,True
"""""".strip()

# Create a simulated CSV file in memory
simulated_file = StringIO(csvdata)
print(simulated_file.getvalue())


# Example of reading the simulated file
df_products = pd.read_csv(simulated_file)
print(""\nSimulated CSV file loaded as DataFrame:"")
display(df_products)

# Reading data from file
# "
lab,w7-class3.ipynb,code,2,"df = pd.read_csv('data.csv')
# df = pd.read_excel('data.xlsx')
# df = pd.read_json('data.json')
# df = pd.read_sql('SELECT * FROM table', connection)"
lab,w7-class3.ipynb,markdown,1,# 2. DATA INSPECTION
lab,w7-class3.ipynb,code,1,display(df)
lab,w7-class3.ipynb,code,1,"# Basic methods
print(""\nBasic DataFrame properties:"")
print(f""Shape: {df.shape}"")

# [4,5,6,] - list
# (4,5,6) - tuple
# {4,5,6} - set
# {'key':4, 'key2': 5, 'key3': 6} - dict

print(f""we have {df.shape[0] * df.shape[1]} number of values"")"
lab,w7-class3.ipynb,code,1,"# Get information about the DataFrame
print(""\nDataFrame info:"")
df.nfo() i# this does a print behind the scenes so no need to use print/display"
lab,w7-class3.ipynb,code,1,"print(f""Column names: {df.columns.tolist()}"")
print(df)
print()

df_in_espanol = df.copy() # Making a copy of the DataFrame
df_in_espanol.columns = ['Nombre', 'Edad', 'Ciudad', 'Salario']  # Reassigning column names
print(f""Column names: {df_in_espanol.columns.tolist()}"")
print(df_in_espanol)
print(df)"
lab,w7-class3.ipynb,code,1,"print(f""Row names: {df.index.tolist()}"")
print(df)
print()

df_in_espanol.index = ['uno', 'dos', 'tres', 'cuatro']  # Reassigning row names
print(df_in_espanol)"
lab,w7-class3.ipynb,code,1,"print(f""Data types:\n{df.dtypes}"")"
lab,w7-class3.ipynb,code,1,"print(""\nFirst 2 rows:"")
print(df.head(2))
print(""\nLast 2 rows:"")
print(df.tail(2))"
lab,w7-class3.ipynb,code,1,"# Summary statistics
display(df)
print(""\nSummary statistics:"")
display(df.describe())"
lab,w7-class3.ipynb,code,1,"print(""\nSummary statistics for specific column:"")
display(df[""Age""].describe())
print(""\nSummary statistics for specific columnS:"")
display(df[[""Age"", ""Salary""]].describe())
print(""\nSummary statistics for specific columnS with custom names:"")
df_renamed = df[[""Age"", ""Salary""]].rename(columns={""Age"": ""Edad"", ""Salary"": ""Salario""})
display(df_renamed.describe())
print(""\nSummary statistics for cities New York and Paris:"")
display(df[df[""City""].isin([""New York"", ""Paris""])].describe())"
lab,w7-class3.ipynb,code,1,"# Info method
print(""\nDataFrame info:"")
df.info() # this does a print behind the scenes so no need to use print/display"
lab,w7-class3.ipynb,markdown,1,# 3. DATA SELECTION
lab,w7-class3.ipynb,code,1,"# Column selection
print(""\nSelecting a column:"")
print(type(df['Name']))  # Series
print(df['Name'])"
lab,w7-class3.ipynb,code,1,"# Multiple columns
print(""\nSelecting multiple columns:"")
print(type(df[['Name', 'Age']]))  # DataFrame
display(df[['Name']])"
lab,w7-class3.ipynb,code,1,"# Row selection with iloc (integer-based)
# iloc is used for integer-location based indexing
# this means you can select rows and columns 
# by their integer index positions
# note: iloc is zero-based and exclusive of the last index
# iloc[0:2] selects rows 0 and 1
print(""\nSelecting rows with iloc:"")
display(df.iloc[0:2])  # First two rows"
lab,w7-class3.ipynb,code,1,"# Row selection with loc (label-based)
# loc is used for label-based indexing
# this means you can select rows and columns
# by their labels
# Note: loc includes the last index in the range
# so df.loc[0:2] will include rows with index 0, 1, and 2
print(""\nSelecting rows with loc:"")
display(df.loc[0:2, [""Age"", ""City""]])  # Rows with index 0, 1, 2"
lab,w7-class3.ipynb,code,1,"# Conditional selection
print(""\nConditional selection:"")
display(df[df['Age'] > 30])"
lab,w7-class3.ipynb,code,1,"# Conditional selection with multiple conditions
print(""\nConditional selection with multiple conditions:"")
# Using & for AND, | for OR
display(df[(df['Age'] > 30) & (df['City'] == 'Paris')])
# Note: & is used for AND, | is used for OR, ~ is used for NOT
display(df[(df['Age'] > 30) | (df['City'] == 'Paris')])
# Conditional selection with negation
display(df[~(df['Age'] > 30)])"
lab,w7-class3.ipynb,code,1,"# Conditional selection with isin
print(""\nConditional selection with isin:"")
display(df[df['City'].isin(['New York', 'Berlin'])])
display(df[~df['City'].isin(['New York', 'Berlin'])])  # Not in the list"
lab,w7-class3.ipynb,code,1,"# Conditional selection with between
print(""\nConditional selection with between:"")
display(df[df['Age'].between(30, 40)])  # Age between 30 and 40
display(df[~df['Age'].between(30, 40)])  # Not between 30 and 40"
lab,w7-class3.ipynb,code,1,"# Conditional selection with query
# this allows you to use and or instead of & and |
# and also allows you to use != instead of ~
# Note: query is a bit slower than the above methods
print(""\nConditional selection with query:"")
print(df.query('Age > 30 & City == ""Paris""'))
print(df.query('Age > 30 | City == ""Paris""'))
print(df.query('Age > 30 and City == ""Paris""'))  # Logical AND
print(df.query('Age > 30 or City == ""Paris""'))  # Logical OR
print(df.query('Age > 30 and City != ""Paris""'))  # Logical AND with NOT"
lab,w7-class3.ipynb,markdown,1,# 4. DATA CLEANING
lab,w7-class3.ipynb,code,1,"# Create a dataframe with missing values
df_missing = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, np.nan, 8],
    'C': [9, 10, 11, 12]
})"
lab,w7-class3.ipynb,code,1,"print(""\nDataFrame with missing values:"")
display(df_missing)"
lab,w7-class3.ipynb,code,1,"# Check for missing values
print(""\nMissing value check:"")
print(df_missing.isnull().sum())"
lab,w7-class3.ipynb,code,1,"# Fill missing values
print(""\nFilling missing values:"")
print(df_missing.fillna(0))  # Fill with 0
print(""\nForward fill:"")
print(df_missing.ffill())  # Forward fill
print(""\nBackward fill:"")
print(df_missing.bfill())  # Backward fill"
lab,w7-class3.ipynb,code,1,"# Drop missing values
print(""\nDropping rows with any missing values:"")
print(df_missing.dropna())
print(""\nDropping columns with any missing values:"")
print(df_missing.dropna(axis=1))"
lab,w7-class3.ipynb,code,1,"# Removing duplicates
df_dup = pd.DataFrame({
    'A': [1, 1, 2, 3],
    'B': [5, 5, 7, 8]
})
print(""\nDataFrame with duplicates:"")
display(df_dup)
print(""\nAfter removing duplicates:"")
display(df_dup.drop_duplicates())"
lab,w7-class3.ipynb,markdown,1,# 5. DATA MANIPULATION
lab,w7-class3.ipynb,code,1,"data = {
    'Name': ['John', 'Anna', 'Peter', 'Linda'],
    'Age': [28, 34, 29, 42],
    'City': ['New York', 'Paris', 'Berlin', 'London'],
    'Salary': [50000, 60000, 55000, 70000]
}
df = pd.DataFrame(data)
display(df)"
lab,w7-class3.ipynb,code,1,"# Add a new column
df['Income_Category'] = ['Medium', 'High', 'Medium', 'High']
print(""\nAdding a new column:"")
display(df)"
lab,w7-class3.ipynb,code,1,"# Adding a new column using a function
def categorize_income(income):
    if income < 60000:
        return 'Medium'
    elif income < 70000:
        return 'High'
    else:
        return 'Very High'
    
df['Income_Category'] = df['Salary'].apply(categorize_income)
print(""\nAdding a new column using a function:"")
display(df)"
lab,w7-class3.ipynb,code,1,"# Sorting
print(""\nSorting by Age (ascending):"")
display(df.sort_values('Age'))"
lab,w7-class3.ipynb,code,1,"print(""\nSorting by Age (descending):"")
display(df.sort_values('Age', ascending=False))"
lab,w7-class3.ipynb,code,1,"print(""\nSorting by multiple columns:"")
display(df.sort_values(['Income_Category', 'Age']))"
lab,w7-class3.ipynb,code,1,"# Grouping and aggregation
print(""\nGrouping by Income Category (mean):"")
display(df.groupby('Income_Category')[['Age', 'Salary']].mean())"
lab,w7-class3.ipynb,code,1,"print(""\nGrouping with multiple aggregations:"")
display(df.groupby('Income_Category').agg({
    'Age': 'mean',
    'Salary': ['min', 'max', 'mean']
}))"
lab,w7-class3.ipynb,code,1,"# Pivot tables
print(""\nPivot table example:"")
pivot = pd.pivot_table(df, values='Salary', index='Income_Category', 
                      columns='City', aggfunc='mean')
display(pivot)"
lab,w7-class3.ipynb,code,1,"# Pivot table with multiple values
print(""\nPivot table with multiple values:"")
pivot_multi = pd.pivot_table(df, values=['Age', 'Salary'], index='Income_Category', 
                              columns='City', aggfunc='mean')
display(pivot_multi)"
lab,w7-class3.ipynb,markdown,1,# 6. DATA ANALYSIS
lab,w7-class3.ipynb,code,1,"# Apply functions
print(""\nApplying a function to a column:"")
display(df['Age'].apply(lambda x: 'Young' if x < 30 else 'Older'))"
lab,w7-class3.ipynb,code,1,"# Create sample time series data
dates = pd.date_range('20210101', periods=6)
ts_df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=['A', 'B', 'C', 'D'])
print(""\nTime series data:"")
display(ts_df)"
lab,w7-class3.ipynb,code,1,"# Resampling
# Resampling (daily to weekly) - this will take the mean of the values in each week
# this is useful for downsampling
print(""\nResampling (daily to weekly):"")
print(ts_df.resample('W').mean())"
lab,w7-class3.ipynb,code,1,"# Rolling statistics
print(""\nRolling mean (window=2):"")
display(ts_df.rolling(2).mean())"
lab,w7-class3.ipynb,code,1,"display(df_products.head())
print(f""Total number of products: {len(df_products)}"")
print(f""Total number of categories: {df_products['category'].nunique()}"")
print(f""Total number of products in stock: {df_products['in_stock'].sum()}"")
print(f""Total number of products out of stock: {len(df_products) - df_products['in_stock'].sum()}"")"
lab,w7-class3.ipynb,code,1,"# Display the original DataFrame
display(df_products)"
lab,w7-class3.ipynb,code,1,"# Group by 'category' and count how many products are in each category
grouped = df_products.groupby('category')['product_id'].count()
print(""Grouped result (without reset_index):"")
print(grouped)
print(type(grouped))  # This is a Series"
lab,w7-class3.ipynb,code,1,"# Try to filter categories with at least 2 products
# meaning we want to filter the Series 'grouped' 
# This will raise an error because 'product_id' is now a Series
try:
    filtered = grouped[grouped['product_id'] >= 2]
except Exception as e:
    print(""\n❌ Error: Cannot use 'product_id' as a column here because it's now a Series:"")
    print(e)"
lab,w7-class3.ipynb,code,1,"# Now fix it with reset_index()
# reset_index() will convert the Series back to a DataFrame
# and make 'category' a column again
# This will allow us to filter the DataFrame
grouped_fixed = grouped.reset_index()
print(""\nGrouped result (with reset_index):"")
print(grouped_fixed)"
lab,w7-class3.ipynb,code,1,"# Filtering works now
filtered_fixed = grouped_fixed[grouped_fixed['product_id'] >= 2]
print(""\n✅ Filtered result (with reset_index):"")
print(filtered_fixed)"
lab,w7-class3.ipynb,markdown,1,# 7. VISUALIZATION
lab,w7-class3.ipynb,code,1,"# Basic plotting
display(df_products.head(10))

print(""\nCreating some visualizations..."")
plt.figure(figsize=(12, 5))
# Example: Visualize average product price by category using the products dataset
df_products.groupby('category')['price'].mean().plot(kind='bar', title='Average Product Price by Category')
plt.ylabel('Average Price')
plt.xlabel('Category')
plt.tight_layout()
plt.show()"
lab,w7-class3.ipynb,code,1,"# Sample data for visualization
np.random.seed(42)
data = {
    'Category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'] * 5,
    'Value1': np.random.normal(0, 1, 40),
    'Value2': np.random.normal(5, 1, 40),
    'Date': pd.date_range('2021-01-01', periods=40)
}

viz_df = pd.DataFrame(data=data)
display(viz_df.head(10))"
lab,w7-class3.ipynb,code,1,"# Example: Visualize average Value2 by Category as a bar plot (subplot 1)
plt.figure(figsize=(12, 5))

# subplot 1 of 2
# Visualize average Value2 by Category as a bar plot (subplot 1)
plt.subplot(1, 2, 1)
# Group by 'Category' and calculate the mean of 'Value2'
# Plot the average Value2 for each category
# Use a different color for each category
viz_df.groupby('Category')['Value2'].mean().plot(kind='bar', color=['#1f77b4', '#ff7f0e'])
plt.title('Average Value2 by Category')
plt.ylabel('Average Value2')
plt.xlabel('Category')

# subplot 2 of 2
# Visualize Value2 over time as a line plot (subplot 2)
plt.subplot(1, 2, 2)
# Plot Value2 over time
viz_df.plot(x='Date', y='Value2', ax=plt.gca(), l"
lab,w7-class3.ipynb,code,2,"egend=False, title='Value2 Over Time')
plt.ylabel('Value2')
plt.xlabel('Date')

plt.tight_layout()
plt.show()"
lab,w7-class3.ipynb,code,1,"# Seaborn plots
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Category', y='Value1', data=viz_df)
plt.title('Box Plot by Category')

plt.subplot(1, 2, 2)
sns.histplot(data=viz_df, x='Value2', kde=True)
plt.title('Value Distribution')
plt.tight_layout()
plt.show()"
lab,w7-class3.ipynb,code,1,"# This is a scatter plot with a regression line
plt.figure(figsize=(7, 5))
# Scatter plot of Value1 vs Value2 by Category
sns.scatterplot(data=viz_df, x='Value1', y='Value2', hue='Category')
plt.title('Scatter Plot by Category')
# Add a regression line to the scatter plot
# Note: scatter=False means we don't want to plot the scatter points again
# this is useful if you want to plot the regression line only
sns.regplot(data=viz_df, x='Value1', y='Value2', scatter=False, ax=plt.gca())
plt.show()"
lab,w7-class3.ipynb,code,1,"# Example dashboard layout using matplotlib
# Create a figure with subplots for different visualizations
# This is a simple dashboard layout with 4 subplots
# Note: You can adjust the number of rows and columns as needed
fig, axs = plt.subplots(2, 2, figsize=(14, 10))

# Set the title for the entire figure
fig.suptitle('Sample Dashboard', fontsize=16)
# Create different visualizations in each subplot
# 1. Distribution plot of Value1 setting ax to axs[0, 0] which is the first subplot
sns.histplot(viz_df['Value1'], ax=axs[0, 0], kde=True)
# now we set the title for this subplot
axs[0, 0].set_title('Distribution')
# 2. Time series plot of Value2 setting ax to axs[0, 1] which is the second subpl"
lab,w7-class3.ipynb,code,2,"ot
# this is a time series plot of Value2 over time
viz_df.plot(x='Date', y='Value2', ax=axs[0, 1], title='Time Series')
# 3. Bar plot of average Value1 by Category setting ax to axs[1, 0] which is the third subplot
# this is a bar plot of average Value1 by Category
# this is useful for comparing the average Value1 for each category
viz_df.groupby('Category')['Value1'].mean().plot(
    kind='bar', ax=axs[1, 0], title='Category Comparison'
)
corr = viz_df[['Value1', 'Value2']].corr()
# 4. Correlation heatmap setting ax to axs[1, 1] which is the fourth subplot
# this is a heatmap of the correlation between Value1 and Value2
# this is useful for visualizing the correlation between two variables"
lab,w7-class3.ipynb,code,3," 
sns.heatmap(corr, annot=True, cmap='viridis', ax=axs[1, 1])
axs[1, 1].set_title('Correlation')
plt.tight_layout()
plt.show()"
lab,w8-class2.ipynb,markdown,1,"### Exercise 1

#### Create a function that outputs a list of all the even numbers from 1 to 500 that are also divisible by 3, 5 AND 7."
lab,w8-class2.ipynb,code,1,"from hashlib import sha256

def exercise_1():
    for num in range(0, 10, 2):
        print(num)

print(exercise_1()) # []



try:
    assert(sha256(str(exercise_1()).encode('utf-8')).hexdigest() == ""b7e8451a417e44cd5335477fdb1d333005a24471781ff5e74f937996f933e50f"" )
    print(""solution is correct"")
except AssertionError:
    print(""solution is incorrect"")"
lab,w8-class2.ipynb,markdown,1,"### Exercise 2

#### Create a function that takes two arguments (both arguments being lists) and uses them to create a dictionary. The first argument should be used for the keys and the second argument should be used as the values. Each element should match up (meaning the first element of the first argument/list should be the key while the first element of the second argument/list should be the value). If either list is missing an element that the other has, skip it.

Examples
```
ex2([],[]) -> {}
ex2(['a', 'b', 'c'], [1,2,3]) -> {'a':1, 'b':2, 'c':3}
```"
lab,w8-class2.ipynb,code,1,"# assume both lists are of the same length
def exercise_2(lst1, lst2):
    result = {}
    length = len(lst1)
    
    for i in range(0, length):
        result[lst1[i]] =  lst2[i]
    
    return result


try:
    assert(exercise_2([],[]) == {})
    assert(exercise_2(['a', 'b', 'c'],[1,2,3]) == {'a': 1, 'b': 2, 'c': 3})
    assert(exercise_2(['x'], [42]) == {'x': 42})
    assert(exercise_2([1,2,3], [4,5,6]) == {1: 4, 2: 5, 3: 6})
    assert(exercise_2(['key1', 'key2'], ['val1', 'val2']) == {'key1': 'val1', 'key2': 'val2'})
    assert(exercise_2([1,2,3], [3,4,5]) == {1: 3, 2: 4, 3: 5})
    print(""solution is correct"")
except AssertionError:
    print(""solution is incorrect"")"
lab,w9-class2.ipynb,markdown,1,# SQL REVIEW
lab,w9-class2.ipynb,markdown,1,"### Important SQL keywords:

```sql
-- DDL (Data Definition Language)
CREATE       -- create tables, databases
ALTER        -- modify table structure, rename tables/columns
DROP         -- delete tables, databases

-- DML (Data Manipulation Language)
INSERT       -- add rows
UPDATE       -- change existing rows
DELETE       -- remove rows
SELECT       -- read/query data

-- Other Key Clauses
WHERE        -- filter rows
ORDER BY     -- sort rows
GROUP BY     -- group rows
HAVING       -- filter groups
JOIN         -- combine rows from other tables
UNION        -- merge result sets
LIMIT        -- restrict result count
DISTINCT     -- remove duplicates
IN, LIKE, BETWEEN, IS NULL -- special fil"
lab,w9-class2.ipynb,markdown,2,"ters

-- Table constraints
PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, DEFAULT, CHECK
```"
lab,w9-class2.ipynb,markdown,1,"### Data types
```sql
-- Numbers
INT / INTEGER
FLOAT / REAL / DOUBLE
DECIMAL(p, s)  -- exact precision (e.g., money)

-- Strings
VARCHAR(n)     -- variable length
CHAR(n)        -- fixed length
TEXT           -- long text

-- Dates and Times
DATE
TIME
TIMESTAMP
DATETIME

-- Boolean
BOOLEAN / BOOL

-- Others
BLOB           -- binary large object
```"
lab,w9-class2.ipynb,markdown,1,"### Creating tables

```sql
CREATE TABLE users (
  id INT PRIMARY KEY,
  name VARCHAR(100) NOT NULL,
  email VARCHAR(150) UNIQUE,
  age INT DEFAULT 18,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE orders (
  id INT PRIMARY KEY,
  user_id INT NOT NULL,
  product VARCHAR(100) NOT NULL,
  quantity INT DEFAULT 1,
  order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (user_id) REFERENCES users(id)
);
```"
lab,w9-class2.ipynb,markdown,1,"### Altering Tables

```sql
-- Add column
ALTER TABLE users ADD COLUMN phone VARCHAR(20);

-- Modify column (depends on DB)
ALTER TABLE users ALTER COLUMN age SET DEFAULT 21;

-- Drop column
ALTER TABLE users DROP COLUMN phone;

-- Rename column or table
ALTER TABLE users RENAME COLUMN name TO full_name;
ALTER TABLE users RENAME TO members;
```"
lab,w9-class2.ipynb,markdown,1,"### Inserting data into table

inserting one row

```sql
INSERT INTO users (id, name, email, age)
VALUES (1, 'Alice', 'alice@example.com', 25);
```

bulk insert (notice the comma between values)
```sql
INSERT INTO users (id, name, email, age)
VALUES 
(2, 'Bob', 'bob@example.com', 30),
(3, 'Eve', 'eve@example.com', 22);
```"
lab,w9-class2.ipynb,markdown,1,"### Selecting Data

select will return rows from the table given

selecting all columns:
```sql
SELECT * FROM users;
```

selecting specific columns:
```sql
SELECT name, email FROM users;
```

filtering rows while selecting:
note: we do this when we want to filter the rows we've selected
```sql
SELECT * FROM users WHERE age > 25;
SELECT * FROM users WHERE name LIKE 'A%';
SELECT * FROM users WHERE email IS NOT NULL; -- cannot use `=` with Null;
SELECT * FROM users WHERE age BETWEEN 18 AND 30;
```

ordering results:
```sql
SELECT * FROM users ORDER BY age ASC;
SELECT * FROM users ORDER BY created_at DESC;
```

grouping with GROUP BY (this allows us to use an aggregate method):
```sql
SELECT ag"
lab,w9-class2.ipynb,markdown,2,"e, COUNT(*) AS num_users
FROM users
GROUP BY age;
```

filtering by grouping using GROUP BY and HAVING:
```sql
SELECT age, COUNT(*) AS num_users
FROM users
GROUP BY age
HAVING COUNT(*) > 1;
```
note: we do this when we want to filter the results of a grouping"
lab,w9-class2.ipynb,markdown,1,"### Joins

inner joins:
```sql
SELECT *
FROM orders
INNER JOIN users ON orders.user_id = users.id;
```

left joins:
```sql
SELECT *
FROM users
LEFT JOIN orders ON users.id = orders.user_id;
```
right joins:
```sql
SELECT *
FROM users
RIGHT JOIN orders ON users.id = orders.user_id;
```
full outer joins:
```sql
SELECT *
FROM users
FULL OUTER JOIN orders ON users.id = orders.user_id;

cross joins (cartesian product):
```
SELECT *
FROM users
CROSS JOIN roles;
```sql"
lab,w9-class2.ipynb,markdown,1,"## 1. Import Required Libraries
We'll use `sqlite3` for SQL and `pandas` to load the CSV."
lab,w9-class2.ipynb,code,1,"import sqlite3
import pandas as pd"
lab,w9-class2.ipynb,markdown,1,"## 2. Load a Simple CSV Dataset
Let's create a tiny CSV in memory for this demo. In practice, you would use `pd.read_csv('filename.csv')`."
lab,w9-class2.ipynb,markdown,1,"## Side-by-side: Pandas vs SQL with Users and Products
We'll load users and products from CSV, then show how to do the same operations in pandas and SQL."
lab,w9-class2.ipynb,code,1,"import pandas as pd
import sqlite3
from io import StringIO

users_csv = '''id,name,email,age
1,Alice,alice@example.com,25
2,Bob,bob@example.com,30
3,Eve,eve@example.com,22
4,David,david@example.com,28
'''
products_csv = '''id,name,price,stock
1,Widget,19.99,100
2,Gadget,29.99,50
3,Thing,9.99,200
'''
users_df = pd.read_csv(StringIO(users_csv))
products_df = pd.read_csv(StringIO(products_csv))"
lab,w9-class2.ipynb,code,1,users_df
lab,w9-class2.ipynb,code,1,products_df
lab,w9-class2.ipynb,markdown,1,"### Create SQLite DB, tables, and insert data"
lab,w9-class2.ipynb,code,1,"import csv

conn = sqlite3.connect(':memory:')
c = conn.cursor()
c.execute('''CREATE TABLE users (id INT PRIMARY KEY, name TEXT, email TEXT, age INT)''')
c.execute('''CREATE TABLE products (id INT PRIMARY KEY, name TEXT, price REAL, stock INT)''')
with open('users.csv', 'w') as f:
    f.write(users_csv)

with open('users.csv', newline='') as f:
    reader = csv.reader(f)
    next(reader)  # skip header row containing column names
    for row in reader:
        c.execute(
            ""INSERT INTO users (id, name, email, age) VALUES (?, ?, ?, ?)"",
            # csv.reader gives us lists so here we have to use the indices
            (int(row[0]), row[1], row[2], int(row[3]))
        )

with op"
lab,w9-class2.ipynb,code,2,"en('products.csv', 'w') as f:
    f.write(products_csv)

with open('products.csv', newline='') as f:
        # we use DictReader here to turn each row of our csv into a dictionary
    # note using `csv.reader` would work as well but we'd need to skip the first row
    # since it's a list of the column names. DictReader turns our column names into the key
    # for each row's dictionary
    reader = csv.DictReader(f)
    for row in reader:
        c.execute(
            ""INSERT INTO products (id, name, price, stock) VALUES (?, ?, ?, ?)"",
            # csv.DictReader gives us a dictionary for each row and we can refer to each column by name
            (int(row['id']), row['name'], float(row['"
lab,w9-class2.ipynb,code,3,"price']), int(row['stock']))
        )

# We could've also turned our pandas dataframes into sql tables using the following:
# users_df.to_sql('users', conn, if_exists='replace', index=False)
# products_df.to_sql('products', conn, if_exists='replace', index=False)"
lab,w9-class2.ipynb,markdown,1,### Select all users/products (Pandas vs SQL)
lab,w9-class2.ipynb,code,1,"# Pandas
users_df"
lab,w9-class2.ipynb,code,1,products_df
lab,w9-class2.ipynb,code,1,"# this command allows us to select from an sql db 
# and gives us back the results as a dataframe!
pd.read_sql_query('SELECT * FROM users', conn)"
lab,w9-class2.ipynb,markdown,1,### SELECT specific columns: name and age from users
lab,w9-class2.ipynb,code,1,"users_df[[""name"", ""age""]]"
lab,w9-class2.ipynb,code,1,"# Pandas
pd.read_sql_query('SELECT name, age FROM users', conn)"
lab,w9-class2.ipynb,code,1,"# Pure Python
c.execute('SELECT name, age FROM users')
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,### Filter users age > 25 (Pandas vs SQL)
lab,w9-class2.ipynb,code,1,"# Pandas
users_df[users_df['age'] > 25]"
lab,w9-class2.ipynb,code,1,"# SQL
pd.read_sql_query('SELECT * FROM users WHERE age > 25', conn)"
lab,w9-class2.ipynb,code,1,"# Pure Python
c.execute('SELECT * FROM users WHERE Age > 25')
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,### SELECT with ORDER BY and LIMIT: top 2 oldest users
lab,w9-class2.ipynb,code,1,"users_df.sort_values('age', ascending=False).head(2)"
lab,w9-class2.ipynb,code,1,"pd.read_sql_query('SELECT * FROM users ORDER BY age DESC LIMIT 2', conn)"
lab,w9-class2.ipynb,code,1,"c.execute('SELECT * FROM users ORDER BY age DESC LIMIT 2')
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,### WHERE with multiple conditions: users age > 22 and name starts with 'A' or 'D'
lab,w9-class2.ipynb,code,1,"users_df[(users_df['age'] > 22) & (users_df['name'].str.startswith(('A', 'D')))]"
lab,w9-class2.ipynb,code,1,"pd.read_sql_query(""SELECT * FROM users WHERE age > 22 AND (name LIKE 'A%' OR name LIKE 'D%')"", conn)"
lab,w9-class2.ipynb,code,1,"c.execute(""SELECT * FROM users WHERE age > 22 AND (name LIKE 'A%' OR name LIKE 'D%')"")
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,"### WHERE with IN, BETWEEN, IS NULL (add a row with NULL age for demo)"
lab,w9-class2.ipynb,code,1,"c.execute(""INSERT INTO users (id, name, email, age) VALUES (?, ?, ?, ?)"", (5, 'NullGuy', 'nullguy@example.com', None))
conn.commit()"
lab,w9-class2.ipynb,code,1,"# IN
pd.read_sql_query(""SELECT * FROM users WHERE name IN ('Alice', 'Eve')"", conn)"
lab,w9-class2.ipynb,code,1,"# BETWEEN
pd.read_sql_query(""SELECT * FROM users WHERE age BETWEEN 23 AND 29"", conn)"
lab,w9-class2.ipynb,code,1,"# IS NULL
pd.read_sql_query(""SELECT * FROM users WHERE age IS NULL"", conn)"
lab,w9-class2.ipynb,code,1,"# Pure Python for IS NULL
c.execute(""SELECT * FROM users WHERE age IS NULL"")
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,### Group by: count users by age (Pandas vs SQL)
lab,w9-class2.ipynb,code,1,"# Pandas
users_df.groupby('age').size().reset_index(name='num_users')"
lab,w9-class2.ipynb,code,1,"# SQL
pd.read_sql_query('SELECT age, COUNT(*) as num_users FROM users GROUP BY age', conn)"
lab,w9-class2.ipynb,code,1,"# Pure Python
c.execute('SELECT age, COUNT(*) as num_users FROM users GROUP BY age')
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,### GROUP BY with Aggregate Methods and HAVING
lab,w9-class2.ipynb,code,1,"# Pure Python GROUP BY COUNT
c.execute('SELECT age, COUNT(*) as num_users FROM users GROUP BY age')
print(c.fetchall())"
lab,w9-class2.ipynb,code,1,"# COUNT
pd.read_sql_query('SELECT age, COUNT(*) as num_users FROM users GROUP BY age', conn)"
lab,w9-class2.ipynb,code,1,"# SUM and AVG on products
pd.read_sql_query('SELECT SUM(price) as total_price, AVG(price) as avg_price FROM products', conn)"
lab,w9-class2.ipynb,code,1,"# HAVING
pd.read_sql_query('SELECT age, COUNT(*) as num_users FROM users GROUP BY age HAVING num_users > 1', conn)"
lab,w9-class2.ipynb,markdown,1,#### Examples of Joins
lab,w9-class2.ipynb,markdown,1,"## Examples of Joins Between Users and Products (Pandas, SQL via Pandas, Pure Python)"
lab,w9-class2.ipynb,markdown,1,### INNER JOIN: users and products on id (Pandas)
lab,w9-class2.ipynb,code,1,"pd.merge(users_df, products_df, left_on='id', right_on='id', how='inner')"
lab,w9-class2.ipynb,markdown,1,### INNER JOIN: users and products on id (SQL via Pandas)
lab,w9-class2.ipynb,code,1,"pd.read_sql_query('SELECT users.*, products.name as product_name, products.price FROM users INNER JOIN products ON users.id = products.id', conn)"
lab,w9-class2.ipynb,markdown,1,### INNER JOIN: users and products on id (Pure Python)
lab,w9-class2.ipynb,code,1,"c.execute('SELECT users.*, products.name, products.price FROM users INNER JOIN products ON users.id = products.id')
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,### LEFT JOIN: all users and their product if exists (Pandas)
lab,w9-class2.ipynb,code,1,"pd.merge(users_df, products_df, left_on='id', right_on='id', how='left')"
lab,w9-class2.ipynb,markdown,1,### LEFT JOIN: all users and their product if exists (SQL via Pandas)
lab,w9-class2.ipynb,code,1,"pd.read_sql_query('SELECT users.*, products.name as product_name, products.price FROM users LEFT JOIN products ON users.id = products.id', conn)"
lab,w9-class2.ipynb,markdown,1,### LEFT JOIN: all users and their product if exists (Pure Python)
lab,w9-class2.ipynb,code,1,"c.execute('SELECT users.*, products.name, products.price FROM users LEFT JOIN products ON users.id = products.id')
print(c.fetchall())"
lab,w9-class2.ipynb,markdown,1,"### Close the SQLite connection

After our work, we still have an open database connection that we must close."
lab,w9-class2.ipynb,code,1,conn.close()
lab,w9-class3.ipynb,markdown,1,"# Week 9 Class 3


Alteryx

<img src=""alteryx.png"" alt=""Alteryx Logo"" width=""600"">"
lab,w9-class3.ipynb,markdown,1,"# SQL Review

## Creating a Database
To create a database:
```sql
CREATE DATABASE database_name;
```

## Altering a Database
To modify a database (e.g., changing its name):
```sql
ALTER DATABASE old_database_name MODIFY NAME = new_database_name;
```

## Dropping a Database
To delete a database:
```sql
DROP DATABASE database_name;
```

---

## Creating a Table
To create a table:
```sql
CREATE TABLE table_name (
    column1 datatype constraints,
    column2 datatype constraints,
    ...
);
```

## Altering a Table
To add, modify, or drop columns in a table:
```sql
-- Add a new column
ALTER TABLE table_name ADD column_name datatype;

-- Modify an existing column
ALTER TABLE table_name MODIFY co"
lab,w9-class3.ipynb,markdown,2,"lumn_name new_datatype;

-- Drop a column
ALTER TABLE table_name DROP COLUMN column_name;
```

## Dropping a Table
To delete a table:
```sql
DROP TABLE table_name;
```

---

## Inserting Rows
To insert data into a table:
```sql
INSERT INTO table_name (column1, column2, ...)
VALUES (value1, value2, ...);
```

## Updating Rows
To update existing data:
```sql
UPDATE table_name
SET column1 = value1, column2 = value2, ...
WHERE condition;
```

## Deleting Rows
To delete specific rows:
```sql
DELETE FROM table_name
WHERE condition;
```

---

## Selecting Data
### Basic Select
To retrieve all rows and columns:
```sql
SELECT * FROM table_name;
```

### Using WHERE Clause
To filter rows based on a co"
lab,w9-class3.ipynb,markdown,3,"ndition:
```sql
SELECT column1, column2
FROM table_name
WHERE condition;
```

### Grouping Data
To group rows with common values:
```sql
SELECT column1, COUNT(*) -- or other aggregate method can be used here
FROM table_name
GROUP BY column1;
```

### Ordering Data
To sort rows in ascending or descending order:
```sql
SELECT column1, column2
FROM table_name
ORDER BY column1 ASC; -- or DESC
```

### Using HAVING Clause
To filter groups after grouping:
```sql
SELECT column1, COUNT(*) as count
FROM table_name
GROUP BY column1
HAVING COUNT(*) > 1; -- note that even though we alias above we must use the COUNT(*) here
```"
lab,w9-class3.ipynb,markdown,1,"## SQL Order of Operations (Logical Query Processing Phases)

When executing an SQL query, the operations are performed in a specific logical order, regardless of how the query is written. Below is the breakdown of the SQL order of operations using a `Users` table:

```sql
SELECT name, age
FROM Users
WHERE age > 25
ORDER BY age DESC;
```

1. **FROM**  
    - Identify the source table and perform any joins, subqueries, or table expressions.
    - Example: `FROM Users`

2. **WHERE**  
    - Filter rows based on conditions before grouping or aggregation.
    - Example: `WHERE age > 25`

3. **GROUP BY**  
    - Group rows that share a common value into summary rows.
    - Example: `GROUP BY age`"
lab,w9-class3.ipynb,markdown,2,"

4. **HAVING**  
    - Filter groups created by `GROUP BY` based on aggregate conditions.
    - Example: `HAVING COUNT(*) > 1`

5. **SELECT**  
    - Select the columns or expressions to include in the result set.
    - Example: `SELECT name, age`

6. **ORDER BY**  
    - Sort the result set based on one or more columns or expressions.
    - Example: `ORDER BY age DESC`

7. **LIMIT/OFFSET**  
    - Limit the number of rows returned or skip a specific number of rows.
    - Example: `LIMIT 10`

---

### Example Query Breakdown

```sql
SELECT name, age
FROM Users
WHERE age > 25
ORDER BY age DESC;
```

1. **FROM**: Identify the `Users` table.
2. **WHERE**: Filter rows where `age > 25`.
3. **SEL"
lab,w9-class3.ipynb,markdown,3,"ECT**: Retrieve the `name` and `age` columns.
4. **ORDER BY**: Sort the result by `age` in descending order.

### Subquery Example: Find Users with Above-Average Age

```sql
SELECT name, age
FROM Users
WHERE age > (
    SELECT AVG(age)
    FROM Users
)
ORDER BY age DESC;
```
When parsing the SQL query with a subquery, the logical order of operations is as follows:

1. **FROM Clause (Subquery)**:  
    The subquery in the `WHERE` clause is executed first to calculate the average age from the `Users` table.  
    Example:  
    ```sql
    SELECT AVG(age)
    FROM Users
    ```

2. **FROM Clause (Outer Query)**:  
    The `FROM` clause in the outer query identifies the `Users` table as the sour"
lab,w9-class3.ipynb,markdown,4,"ce.

3. **WHERE Clause**:  
    The `WHERE` clause in the outer query is evaluated to filter rows where `age` is greater than the result of the subquery.  
    Example:  
    ```sql
    WHERE age > (subquery result)
    ```

4. **SELECT Clause**:  
    The `SELECT` clause determines which columns (`name` and `age`) to include in the result set.  
    Example:  
    ```sql
    SELECT name, age
    ```

5. **ORDER BY Clause**:  
    Finally, the `ORDER BY` clause is applied to sort the result set by `age` in descending order.  
    Example:  
    ```sql
    ORDER BY age DESC
    ```"
lab,w9-class3.ipynb,code,1,"# imports we'll need for the rest of the notebook
import sqlite3
import pandas as pd
from IPython.display import display"
lab,w9-class3.ipynb,code,1,"# Step 1: Connect to SQLite database (or create it if it doesn't exist)
# connection = sqlite3.connect(""example.db"")
connection = sqlite3.connect("":memory:"")  # for in-memory database, use "":memory:""

# Step 2: Create a cursor object to execute SQL commands
# cursors are used to interact with the database
cursor = connection.cursor()

# Step 3: Create a table
cursor.execute('''
CREATE TABLE IF NOT EXISTS Users (
    id INTEGER PRIMARY KEY AUTOINCREMENT, -- note we don't need to say NOT NULL and UNIQUE
    name TEXT NOT NULL,
    age INTEGER NOT NULL,
    email TEXT UNIQUE NOT NULL
)
''')

# Step 4: Insert data into the table
cursor.execute('''
INSERT INTO Users (name, age, email)
VALUES ('Al"
lab,w9-class3.ipynb,code,2,"ice', 25, 'alice@example.com')
''')
cursor.execute('''
INSERT INTO Users (name, age, email)
VALUES ('Bob', 30, 'bob@example.com')
''')

# Commit the changes
# note that this is similar to git commit
# it saves the changes made to the database
# if we don't commit the changes, they won't be saved
# and the database will be rolled back to the previous state
connection.commit()

# Step 5: Query data from the table
cursor.execute('SELECT * FROM Users')

# this will fetch all rows from the users table
# we could also use fetchone() to get one row at a time
# or fetchmany(size) to get a specific number of rows
# fetchall() returns a list of tuples, where each tuple is a row
rows = cursor.fetchall("
lab,w9-class3.ipynb,code,3,")
print(""Users in the database:"")
for row in rows:
    print(row)

# Step 6: Update a record
cursor.execute('''
UPDATE users
SET age = 26
WHERE name = 'Alice'
''')
connection.commit()

# Step 7: Delete a record
cursor.execute('''
DELETE FROM users
WHERE name = 'Bob'
''')
connection.commit()

# Step 8: Query data again to see changes
cursor.execute('SELECT * FROM users')
rows = cursor.fetchall()
print(""\nUpdated users in the database:"")
for row in rows:
    print(row)

# Step 9: Close the connection
connection.close()"
lab,w9-class3.ipynb,code,1,"# Example data with additional rows
left = pd.DataFrame({
    'id': [1, 2, 3, 5],
    'name': ['Alice', 'Bob', 'Charlie', 'Eve']
})
right = pd.DataFrame({
    'id': [2, 3, 4, 6],
    'age': [30, 35, 40, 50]
})
print(""Left DataFrame:"")
display(left)
print(""Right DataFrame:"")
display(right)

# Inner join
inner_result = pd.merge(left, right, on='id', how='inner')
print(""Inner Join Result:"")
display(inner_result)

# Left join
left_result = pd.merge(left, right, on='id', how='left')
print(""Left Join Result:"")
display(left_result)

# Right join
right_result = pd.merge(left, right, on='id', how='right')
print(""Right Join Result:"")
display(right_result)

# Outer join
outer_result = pd.merge(left, ri"
lab,w9-class3.ipynb,code,2,"ght, on='id', how='outer')
print(""Outer Join Result:"")
display(outer_result)"
lab,w9-class3.ipynb,code,1,"# Create in-memory SQLite DB
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create tables
cursor.execute(""""""
CREATE TABLE IF NOT EXISTS LeftTable (
    id INTEGER,
    name TEXT
)
"""""")
cursor.execute(""""""
CREATE TABLE IF NOT EXISTS RightTable (
    id INTEGER,
    age INTEGER
)
"""""")

# Insert data into LeftTable
left_data = [
    (1, 'Alice'),
    (2, 'Bob'),
    (3, 'Charlie'),
    (5, 'Eve')
]
cursor.executemany(""INSERT INTO LeftTable VALUES (?, ?)"", left_data)

# Insert data into RightTable
right_data = [
    (2, 30),
    (3, 35),
    (4, 40),
    (6, 50)
]
cursor.executemany(""INSERT INTO RightTable VALUES (?, ?)"", right_data)

# Perform joins and display results
join_queries"
lab,w9-class3.ipynb,code,2," = {
    ""Inner Join"": ""SELECT * FROM LeftTable INNER JOIN RightTable ON LeftTable.id = RightTable.id"",
    ""Left Join"": ""SELECT * FROM LeftTable LEFT JOIN RightTable ON LeftTable.id = RightTable.id"",
    ""Right Join"": ""SELECT * FROM LeftTable RIGHT JOIN RightTable ON LeftTable.id = RightTable.id"",
    ""Outer Join"": """"""
        SELECT * FROM LeftTable
        FULL OUTER JOIN RightTable ON LeftTable.id = RightTable.id
    """"""
}

for join_type, query in join_queries.items():
    try:
        print(f""{join_type} Result:"")
        df_result = pd.read_sql_query(query, conn)
        display(df_result)
    except Exception as e:
        print(f""Error with {join_type}: {e}"")

# Close the connection
"
lab,w9-class3.ipynb,code,3,conn.close()
lab,w9-class3.ipynb,markdown,1,## Leetcode Problems
lab,w9-class3.ipynb,markdown,1,"## 🧮 Problem: Users Table

**Table: `users`**

```
+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| id          | int     |
| name        | varchar |
| age         | int     |
| email       | varchar |
+-------------+---------+
```

`id` is the primary key column for this table.

Each row of this table contains information about a user's ID, name, age, and email address.

---

### ❓ Question

Write a SQL query to **find all users who are older than 25 years**.

Return the result table **in any order**.

---

### 🧪 Example 1:

**Input:**

Table: `users`

```
+----+-------+-----+-------------------+
| id | name  | age | email             |
+----+-------+-----+---"
lab,w9-class3.ipynb,markdown,2,"----------------+
| 1  | Alice | 26  | alice@example.com |
| 2  | Bob   | 30  | bob@example.com   |
| 3  | Carol | 22  | carol@example.com |
+----+-------+-----+-------------------+
```

**Output:**

```
+----+-------+-----+-------------------+
| id | name  | age | email             |
+----+-------+-----+-------------------+
| 1  | Alice | 26  | alice@example.com |
| 2  | Bob   | 30  | bob@example.com   |
+----+-------+-----+-------------------+
```

---"
lab,w9-class3.ipynb,code,1,"# Create in-memory SQLite DB
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create table
cursor.execute(""""""
CREATE TABLE IF NOT EXISTS World (
    name TEXT,
    continent TEXT,
    area INTEGER,
    population INTEGER,
    gdp INTEGER
)
"""""")

# Clear table just in case
cursor.execute(""DELETE FROM World"")

# Insert sample data
data = [
    ('Afghanistan', 'Asia', 652230, 25500100, 20343000000),
    ('Albania', 'Europe', 28748, 2831741, 12960000000),
    ('Algeria', 'Africa', 2381741, 37100000, 188681000000),
    ('Andorra', 'Europe', 468, 78115, 3712000000),
    ('Angola', 'Africa', 1246700, 20609294, 100990000000)
]
cursor.executemany(""INSERT INTO World VALUES (?, ?, ?, ?, ?)"""
lab,w9-class3.ipynb,code,2,", data)
query = """"""
SELECT * FROM World
""""""
df_result = pd.read_sql_query(query, conn)
display(df_result)"
lab,w9-class3.ipynb,code,1,"try:
    # Query: Find big countries
    # Find countries with a population greater than 25 million and area greater than 500,000 km²
    # and sort them by population in descending order.
    query = """"""
    -- ENTER YOUR SQL QUERY HERE
    """"""
    # Run query and load results into a DataFrame
    df_result = pd.read_sql_query(query, conn)

    # Show result
    print(""Query Result:"")
    display(df_result)

    # ✅ Test the result
    
    expected = pd.DataFrame({
        'name': ['Afghanistan', 'Algeria'],
        'population': [25500100, 37100000],
        'area': [652230, 2381741]
    })

    # Sort for comparison
    df_result_sorted = df_result.sort_values(by='name').reset_index(drop"
lab,w9-class3.ipynb,code,2,"=True)
    expected_sorted = expected.sort_values(by='name').reset_index(drop=True)

    assert df_result_sorted.equals(expected_sorted), ""Test failed: Output does not match expected result.""
    print(""\n✅ Test passed: Output matches expected result."")

except Exception as e:
    print(""\n❌ An error occurred:"")
    print(e)"
lab,w9-class3.ipynb,markdown,1,"## 🧾 Problem: Product Sales Analysis I

**Table: `Sales`**

```
+-------------+-------+
| Column Name | Type  |
+-------------+-------+
| sale_id     | int   |
| product_id  | int   |
| year        | int   |
| quantity    | int   |
| price       | int   |
+-------------+-------+
```

- `(sale_id, year)` is the primary key.
- `product_id` is a foreign key referencing the `Product` table.
- Each row represents a sale of a product in a specific year.
- `price` is per unit.

**Table: `Product`**

```
+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
+--------------+---------+
```

- `product_id` is the primary k"
lab,w9-class3.ipynb,markdown,2,"ey.
- Each row represents the name of a product.

---

### ❓ Question

Write a SQL query to report the `product_name`, `year`, and `price` for each `sale_id` in the `Sales` table.

Return the result table **in any order**.

---

### 🧪 Example 1:

**Input:**

Table: `Sales`

```
+---------+------------+------+----------+-------+
| sale_id | product_id | year | quantity | price |
+---------+------------+------+----------+-------+
| 1       | 100        | 2008 | 10       | 5000  |
| 2       | 100        | 2009 | 12       | 5000  |
| 7       | 200        | 2011 | 15       | 9000  |
+---------+------------+------+----------+-------+
```

Table: `Product`

```
+------------+--------------+
| produ"
lab,w9-class3.ipynb,markdown,3,"ct_id | product_name |
+------------+--------------+
| 100        | Nokia        |
| 200        | Apple        |
| 300        | Samsung      |
+------------+--------------+
```

**Output:**

```
+--------------+-------+-------+
| product_name | year  | price |
+--------------+-------+-------+
| Nokia        | 2008  | 5000  |
| Nokia        | 2009  | 5000  |
| Apple        | 2011  | 9000  |
+--------------+-------+-------+
```

**Explanation:**

- From `sale_id = 1`, we know Nokia was sold for 5000 in 2008.
- From `sale_id = 2`, Nokia was again sold for 5000 in 2009.
- From `sale_id = 7`, Apple was sold for 9000 in 2011."
lab,w9-class3.ipynb,code,1,"# Create in-memory SQLite DB
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create tables
cursor.execute(""""""
CREATE TABLE IF NOT EXISTS Sales (
    sale_id INTEGER,
    product_id INTEGER,
    year INTEGER,
    quantity INTEGER,
    price INTEGER,
    PRIMARY KEY (sale_id, year)
)
"""""")

cursor.execute(""""""
CREATE TABLE IF NOT EXISTS Product (
    product_id INTEGER PRIMARY KEY,
    product_name TEXT
)
"""""")

# Clear tables
cursor.execute(""DELETE FROM Sales"")
cursor.execute(""DELETE FROM Product"")

# Insert data into Sales
sales_data = [
    (1, 100, 2008, 10, 5000),
    (2, 100, 2009, 12, 5000),
    (7, 200, 2011, 15, 9000)
]
cursor.executemany(""INSERT INTO Sales VALUES (?, ?, ?, "
lab,w9-class3.ipynb,code,2,"?, ?)"", sales_data)

# Insert data into Product
product_data = [
    (100, 'Nokia'),
    (200, 'Apple'),
    (300, 'Samsung')
]
cursor.executemany(""INSERT INTO Product VALUES (?, ?)"", product_data)

query = """"""
SELECT * FROM Sales
""""""
df_result = pd.read_sql_query(query, conn)
display(df_result) # display what the original data looks like"
lab,w9-class3.ipynb,code,1,"try:
    # Query: Join Sales and Product to get product_name, year, price
    query = """"""
    -- ENTER YOUR SQL QUERY HERE
    """"""
    # Run query and load results into a DataFrame
    df_result = pd.read_sql_query(query, conn)

    df_result = pd.read_sql_query(query, conn)
    print(""Query Result:"")
    display(df_result)

    # ✅ Test the result
    
    # ✅ Test the result
    expected = pd.DataFrame({
        'product_name': ['Nokia', 'Nokia', 'Apple'],
        'year': [2008, 2009, 2011],
        'price': [5000, 5000, 9000]
    })

    df_result_sorted = df_result.sort_values(by=['product_name', 'year']).reset_index(drop=True)
    expected_sorted = expected.sort_values(by=['product_name"
lab,w9-class3.ipynb,code,2,"', 'year']).reset_index(drop=True)

    assert df_result_sorted.equals(expected_sorted), ""Test failed: Output does not match expected result.""
    print(""\n✅ Test passed: Output matches expected result."")
except Exception as e:
    print(""\n❌ An error occurred:"")
    print(e)# make sure to close the connection"
lab,w9-class3.ipynb,markdown,1,"## 🧾 Problem: Classes More Than 5 Students

**Table: `Courses`**

```
+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| student     | varchar |
| class       | varchar |
+-------------+---------+
```

- `(student, class)` is the primary key.
- Each row indicates a student and the class they are enrolled in.

---

### ❓ Question

Write a SQL query to **find all classes that have at least five students**.

Return the result table in **any order**.

---

### 🧪 Example 1:

**Input:**

Table: `Courses`

```
+---------+----------+
| student | class    |
+---------+----------+
| A       | Math     |
| B       | English  |
| C       | Math     |
| D       | Biology  |
|"
lab,w9-class3.ipynb,markdown,2," E       | Math     |
| F       | Computer |
| G       | Math     |
| H       | Math     |
| I       | Math     |
+---------+----------+
```

**Output:**

```
+--------+
| class  |
+--------+
| Math   |
+--------+
```

**Explanation:**

- Math has 6 students → included.
- English, Biology, Computer each have < 5 students → excluded."
lab,w9-class3.ipynb,code,1,"# Create in-memory SQLite DB
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create Courses table
cursor.execute(""""""
CREATE TABLE IF NOT EXISTS Courses (
    student TEXT,
    class TEXT,
    PRIMARY KEY (student, class)
)
"""""")

# Clear table just in case
cursor.execute(""DELETE FROM Courses"")

# Insert test data
courses_data = [
    ('A', 'Math'),
    ('B', 'English'),
    ('C', 'Math'),
    ('D', 'Biology'),
    ('E', 'Math'),
    ('F', 'Computer'),
    ('G', 'Math'),
    ('H', 'Math'),
    ('I', 'Math')
]
cursor.executemany(""INSERT INTO Courses VALUES (?, ?)"", courses_data)
query = """"""
SELECT * FROM Courses
""""""
df_result = pd.read_sql_query(query, conn)
df_result # display wha"
lab,w9-class3.ipynb,code,2,t the original data looks like
lab,w9-class3.ipynb,code,1,"try:
    # Query: Classes with at least 5 students
    query = """"""
    -- ENTER YOUR SQL QUERY HERE
    """"""

    # Execute and display
    df_result = pd.read_sql_query(query, conn)
    print(""Query Result:"")
    display(df_result)

    # ✅ Test the result
    expected = pd.DataFrame({'class': ['Math']})

    df_result_sorted = df_result.sort_values(by='class').reset_index(drop=True)
    expected_sorted = expected.sort_values(by='class').reset_index(drop=True)

    assert df_result_sorted.equals(expected_sorted), ""❌ Test failed: Output does not match expected result.""
    print(""\n✅ Test passed: Output matches expected result."")
except Exception as e:
    print(""\n❌ An error occurred:"")
    "
lab,w9-class3.ipynb,code,2,print(e)
lab,w9-class3.ipynb,markdown,1,"## 🧾 Problem: Customer Who Visited but Did Not Make Any Transactions

**Table: `Visits`**

```
+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| visit_id    | int     |
| customer_id | int     |
+-------------+---------+
```

- `visit_id` is the primary key.
- Each row represents a customer visit to the mall.

**Table: `Transactions`**

```
+----------------+----------+--------+
| Column Name    | Type     |
+----------------+----------+--------+
| transaction_id | int      |
| visit_id       | int      |
| amount         | int      |
+----------------+----------+--------+
```

- `transaction_id` is the primary key.
- Each row represents a transaction tied to a "
lab,w9-class3.ipynb,markdown,2,"`visit_id`.

---

### ❓ Question

Write a SQL query to find the **customer IDs who visited the mall but did not make any transactions**, and the **number of such visits** for each customer.

Return the result table in any order.

---

### 🧪 Example 1:

**Input:**

Table: `Visits`

```
+----------+-------------+
| visit_id | customer_id |
+----------+-------------+
| 1        | 23          |
| 2        | 9           |
| 4        | 30          |
| 5        | 54          |
| 6        | 96          |
| 7        | 54          |
| 8        | 54          |
+----------+-------------+
```

Table: `Transactions`

```
+----------------+----------+--------+
| transaction_id | visit_id | amount |
+------"
lab,w9-class3.ipynb,markdown,3,"----------+----------+--------+
| 2              | 5        | 310    |
| 3              | 5        | 300    |
| 9              | 5        | 200    |
| 12             | 1        | 910    |
| 13             | 2        | 970    |
+----------------+----------+--------+
```

**Output:**

```
+-------------+----------------+
| customer_id | count_no_trans |
+-------------+----------------+
| 54          | 2              |
| 30          | 1              |
| 96          | 1              |
+-------------+----------------+
```

**Explanation:**

- Customers 30 and 96 had 1 visit each with no transactions.
- Customer 54 visited 3 times, but only 1 visit had transactions. So 2 visits count."
lab,w9-class3.ipynb,code,1,"import sqlite3
import pandas as pd

# Set up in-memory SQLite DB
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create tables
cursor.execute(""CREATE TABLE IF NOT EXISTS Visits (visit_id INT, customer_id INT)"")
cursor.execute(""CREATE TABLE IF NOT EXISTS Transactions (transaction_id INT, visit_id INT, amount INT)"")

# Clear any data
cursor.execute(""DELETE FROM Visits"")
cursor.execute(""DELETE FROM Transactions"")

# Insert data into Visits
visits_data = [
    (1, 23), (2, 9), (4, 30), (5, 54), (6, 96), (7, 54), (8, 54)
]
cursor.executemany(""INSERT INTO Visits VALUES (?, ?)"", visits_data)

# Insert data into Transactions
transactions_data = [
    (2, 5, 310), (3, 5, 300), (9, 5, 200"
lab,w9-class3.ipynb,code,2,"), (12, 1, 910), (13, 2, 970)
]
cursor.executemany(""INSERT INTO Transactions VALUES (?, ?, ?)"", transactions_data)

cursor.execute(""SELECT * FROM Visits"")
visits_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])
cursor.execute(""SELECT * FROM Transactions"")
transactions_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

display(visits_df, transactions_df)"
lab,w9-class3.ipynb,code,1,"try:
    # Query: Customers who visited but made no transactions
    query = """"""
    -- ENTER YOUR SQL QUERY HERE
    """"""

    df_result = pd.read_sql_query(query, conn)
    print(""Query Result:"")
    display(df_result)

    # ✅ Test
    expected = pd.DataFrame({
        'customer_id': [30, 54, 96],
        'count_no_trans': [1, 2, 1]
    })

    df_result_sorted = df_result.sort_values(by='customer_id').reset_index(drop=True)
    expected_sorted = expected.sort_values(by='customer_id').reset_index(drop=True)

    assert df_result_sorted.equals(expected_sorted), ""❌ Test failed!""
    print(""\n✅ Test passed: Output matches expected result."")

except Exception as e:
    print(""\n❌ An error occu"
lab,w9-class3.ipynb,code,2,"rred:"")
    print(e)"
lab,w9-class3.ipynb,code,1,"# Clean up once we're done
conn.close()"
lab,w10-class1.ipynb,code,1,"import sqlite3
import pandas as pd"
lab,w10-class1.ipynb,markdown,1,"```sql
CREATE TABLE students (
    student_id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT,
    grade TEXT,
    email TEXT,
);
```
```sql
CREATE TABLE courses (
    course_id INTEGER PRIMARY KEY AUTOINCREMENT,
    course_name TEXT,
    credits INTEGER
);
```
```sql
CREATE TABLE enrollments (
    enrollment_id INTEGER PRIMARY KEY AUTOINCREMENT,
    student_id INTEGER,
    course_id INTEGER,
    semester TEXT,
    grade TEXT,
    FOREIGN KEY (student_id) REFERENCES students(student_id),
    FOREIGN KEY (course_id) REFERENCES courses(course_id)
);
```"
lab,w10-class1.ipynb,code,1,"# Connect to SQLite database (in-memory for demo)
# sql.db
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create tables
cursor.execute(""""""
CREATE TABLE students (
    student_id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT,
    grade TEXT,
    email TEXT
)
"""""")
cursor.execute(""""""
CREATE TABLE courses (
    course_id INTEGER PRIMARY KEY AUTOINCREMENT,
    course_name TEXT,
    credits INTEGER
)
"""""")
cursor.execute(""""""
CREATE TABLE enrollments (
    enrollment_id INTEGER PRIMARY KEY AUTOINCREMENT,
    student_id INTEGER,
    course_id INTEGER,
    semester TEXT,
    grade TEXT,
    FOREIGN KEY (student_id) REFERENCES students(student_id),
    FOREIGN KEY (course_id) REFERENCE"
lab,w10-class1.ipynb,code,2,"S courses(course_id)
)
"""""")

# Show all tables in the database
cursor.execute(""SELECT name FROM sqlite_master WHERE type='table' and name NOT LIKE 'sqlite_%'"")

tables = cursor.fetchall()
print(""Tables in database:"", [t[0] for t in tables])"
lab,w10-class1.ipynb,markdown,1,"```sql
INSERT INTO students (name, grade, email) VALUES ('Alice', 'A', 'alice@email.com');
INSERT INTO students (name, grade, email) VALUES ('Bob', 'B', 'bob@email.com');
INSERT INTO courses (course_name, credits) VALUES ('Mathematics', 3);
INSERT INTO courses (course_name, credits) VALUES ('History', 2);
INSERT INTO enrollments (student_id, course_id, semester, grade) VALUES (1, 1, 'Fall2023', 'A');
INSERT INTO enrollments (student_id, course_id, semester, grade) VALUES (2, 2, 'Spring2024', 'B');
```"
lab,w10-class1.ipynb,code,1,"# Insert data
cursor.execute(""INSERT INTO students (name, grade, email) VALUES ('Alice', 'A', 'alice@email.com')"")
cursor.execute(""INSERT INTO students (name, grade, email) VALUES ('Bob', 'B', 'bob@email.com')"")
cursor.execute(""INSERT INTO courses (course_name, credits) VALUES ('Mathematics', 3)"")
cursor.execute(""INSERT INTO courses (course_name, credits) VALUES ('History', 2)"")
cursor.execute(""INSERT INTO enrollments (student_id, course_id, semester, grade) VALUES (1, 1, 'Fall2023', 'A')"")
cursor.execute(""INSERT INTO enrollments (student_id, course_id, semester, grade) VALUES (2, 2, 'Spring2024', 'B')"")
conn.commit()


# Show all tables after insert
for table_name in [t[0] for t in tables]:"
lab,w10-class1.ipynb,code,2,"
    df = pd.read_sql_query(f""SELECT * FROM {table_name}"", conn)
    print(f""Table: {table_name}"")
    display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql

SELECT * FROM students;

SELECT name, grade FROM students WHERE grade = 'A';

SELECT * FROM students WHERE grade <> 'C' ORDER BY name DESC;

SELECT grade, COUNT(*) as num_students FROM students GROUP BY grade;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""SELECT * FROM students"", conn)
display(df)


df = pd.read_sql_query(""SELECT name, grade FROM students WHERE grade = 'A'"", conn)
display(df)


df = pd.read_sql_query(""SELECT * FROM students WHERE grade <> 'C' ORDER BY name DESC"", conn)
display(df)


df = pd.read_sql_query(""SELECT grade, COUNT(*) as num_students FROM students GROUP BY grade"", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql

UPDATE students SET email = 'alice_new@email.com' WHERE name = 'Alice';

UPDATE courses SET credits = 4 WHERE course_name = 'Mathematics';

UPDATE enrollments SET grade = 'A' WHERE student_id = 2 AND course_id = 2;

UPDATE students SET grade = 'B', email = 'bob_new@email.com' WHERE name = 'Bob';

```"
lab,w10-class1.ipynb,code,1,"cursor.execute(""UPDATE students SET email = 'alice_new@email.com' WHERE name = 'Alice'"")

cursor.execute(""UPDATE courses SET credits = 4 WHERE course_name = 'Mathematics'"")


cursor.execute(""UPDATE enrollments SET grade = 'A' WHERE student_id = 2 AND course_id = 2"")

cursor.execute(""UPDATE students SET grade = 'B', email = 'bob_new@email.com' WHERE name = 'Bob'"")
conn.commit()


for table_name in [t[0] for t in tables]:
    df = pd.read_sql_query(f""SELECT * FROM {table_name}"", conn)
    print(f""Table: {table_name}"")
    display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql

SELECT s.name, c.course_name, e.semester, e.grade
FROM students s
JOIN enrollments e ON s.student_id = e.student_id
JOIN courses c ON e.course_id = c.course_id
WHERE e.grade IS NOT NULL
ORDER BY s.name, c.course_name;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT s.name, c.course_name, e.semester, e.grade
FROM students s
JOIN enrollments e ON s.student_id = e.student_id
JOIN courses c ON e.course_id = c.course_id
WHERE e.grade IS NOT NULL
ORDER BY s.name, c.course_name
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT s.name, COUNT(e.course_id) as courses_taken
FROM students s
LEFT JOIN enrollments e ON s.student_id = e.student_id
GROUP BY s.name
HAVING courses_taken > 0;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT s.name, COUNT(e.course_id) as courses_taken
FROM students s
LEFT JOIN enrollments e ON s.student_id = e.student_id
GROUP BY s.name
HAVING courses_taken > 0
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT name, (SELECT COUNT(*) FROM enrollments e WHERE e.student_id = s.student_id) as total_enrollments
FROM students s;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT name, (SELECT COUNT(*) FROM enrollments e WHERE e.student_id = s.student_id) as total_enrollments
FROM students s
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT name FROM students s
WHERE EXISTS (
    SELECT 1 FROM enrollments e WHERE e.student_id = s.student_id AND e.grade = 'A'
);
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT name FROM students s
WHERE EXISTS (
    SELECT 1 FROM enrollments e WHERE e.student_id = s.student_id AND e.grade = 'A'
)
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT s1.name AS student1, s2.name AS student2
FROM students s1
JOIN students s2 ON s1.grade = s2.grade AND s1.student_id <> s2.student_id;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT s1.name AS student1, s2.name AS student2
FROM students s1
JOIN students s2 ON s1.grade = s2.grade AND s1.student_id <> s2.student_id
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT name FROM students
UNION
SELECT course_name FROM courses;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT name FROM students
UNION
SELECT course_name FROM courses
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT name,
    CASE
        WHEN grade = 'A' THEN 'Excellent'
        WHEN grade = 'B' THEN 'Good'
        ELSE 'Needs Improvement'
    END as performance
FROM students;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT name,
    CASE
        WHEN grade = 'A' THEN 'Excellent'
        WHEN grade = 'B' THEN 'Good'
        ELSE 'Needs Improvement'
    END as performance
FROM students
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
SELECT
    s.name,
    e.semester,
    e.grade,
    RANK() OVER (PARTITION BY s.student_id ORDER BY e.grade DESC) as grade_rank
FROM students s
JOIN enrollments e ON s.student_id = e.student_id;
```"
lab,w10-class1.ipynb,code,1,"df = pd.read_sql_query(""""""
SELECT
    s.name,
    e.semester,
    e.grade,
    RANK() OVER (PARTITION BY s.student_id ORDER BY e.grade DESC) as grade_rank
FROM students s
JOIN enrollments e ON s.student_id = e.student_id
"""""", conn)
display(df)"
lab,w10-class1.ipynb,markdown,1,"```sql
DELETE FROM students WHERE name = 'Bob';

DROP TABLE enrollments;
```"
lab,w10-class1.ipynb,code,1,"cursor.execute(""DELETE FROM students WHERE name = 'Bob'"")

cursor.execute(""DROP TABLE enrollments"")
conn.commit()

tables = cursor.execute(""SELECT name FROM sqlite_master WHERE type='table' and name NOT LIKE 'sqlite_%'"").fetchall()
print(""Tables in database:"", [t[0] for t in tables])
for table_name in [t[0] for t in tables]:
    df = pd.read_sql_query(f""SELECT * FROM {table_name}"", conn)
    print(f""Table: {table_name}"")
    display(df)"
lab,w10-class1.ipynb,code,1,"# when we're done, we can close the connection
# to our sqlite database

# it runs in memory, so it will be take up ram
# if we don't close it
# and it will not persist after we close the connection
# which means we lose all our data
conn.close()"
lab,python_refresher.ipynb,markdown,1,"# Phase 2 Python Refresher

### Agenda
- 1. Basic Arithmetic & Variables
- 2. Control Flow: Conditionals & Loops
- 3. Data Structures: Lists, Tuples, Sets, Dicts
- 4. Pandas Review"
lab,python_refresher.ipynb,markdown,1,"## 1. Basic Arithmetic & Variables
In this section, we'll calculate simple expressions and assign them to variables."
lab,python_refresher.ipynb,markdown,1,"Set each number to a variable, set the variables to sum and then print sum"
lab,python_refresher.ipynb,code,1,"# Calculate 1700 + 29 and print the result
num1 = 1700
num2 = 29
ans = num1 + num2
print(ans)"
lab,python_refresher.ipynb,code,1,"nums = []

nums.append(1700)
nums.append(29)"
lab,python_refresher.ipynb,markdown,1,"**Bonus Exercise 1:** Change the values of `num1` and `num2` to two numbers of your choice, compute their sum, product, and difference, and print each result."
lab,python_refresher.ipynb,markdown,1,"## 2. Control Flow: Conditionals & Loops
Count how many 'o' and 'e' letters appear in the sentence ""hello world""."
lab,python_refresher.ipynb,code,1,"sentence = ""hello world""
count = 0

for letter in sentence:
    # if letter in ['o', 'e']:
    if letter == 'o' or letter == 'e':
        count += 1
    
print(f""the number of o's and e's are: {count}"")"
lab,python_refresher.ipynb,markdown,1,"**Bonus Exercise 2:** Modify the loop to count vowels (`a, e, i, o, u`) in a sentence of your choice."
lab,python_refresher.ipynb,markdown,1,"## 3. Data Structures
Find the smallest value in a list of closing stock prices."
lab,python_refresher.ipynb,code,1,"values = [24, 23, 22, 21, 20]
smallest = values[0] # 20
for number in values:
    if number < smallest:
        smallest = number"
lab,python_refresher.ipynb,markdown,1,**Bonus Exercise 3:** Create a list of at least 10 numbers and write a loop to find both the smallest and largest values without using built-in `min` or `max`.
lab,python_refresher.ipynb,markdown,1,"## 4. Pandas Review
Explore a sample DataFrame: head(), describe(), isna(), and basic indexing."
lab,python_refresher.ipynb,code,1,"import pandas as pd

data = {
    'A': [1, 2, None, 4, 5],
    'B': ['x', 'y', 'z', None, 'w'],
    'C': [10.5, 20.3, 30.1, 40.2, None]
}

# Create a sample DataFrame"
lab,python_refresher.ipynb,markdown,1,"**Bonus Exercise 4:** Load your own CSV (or use this sample) and practice:
- Selecting rows where A > 2
- Using `.loc` and `.iloc`
- Chaining methods (e.g., `df.dropna().sort_values('C')`)"
lab,python_refresher.ipynb,markdown,1,"## Python & Pandas Practice Prompt

1. **Create a variable that stores a list of eight numbers of your choice and display the list.**

2. **Without using the built-in `min` or `max` functions, determine and print both the smallest and largest numbers in your list.**

3. **Choose any sentence and count how many vowels (`a, e, i, o, u`) appear in it, regardless of case. Print the total number of vowels.**

4. **Using pandas, create a DataFrame with two columns: `""Name""` and `""Score""`, and at least five rows (include at least one missing value in either column).  
   Display the DataFrame, then display only the rows where `""Score""` is greater than 50.  
   Finally, remove any rows with missing "
lab,python_refresher.ipynb,markdown,2,"values and show the DataFrame sorted by `""Score""` in descending order.**"
lab,sql_refresher.ipynb,markdown,1,"### SQL Refresher with SQLite and Python
This notebook provides a hands-on walkthrough of SQL concepts using an in-memory SQLite database. You'll see examples, explanations, and get a chance to practice."
lab,sql_refresher.ipynb,markdown,1,#### Setup: Creating the In-Memory SQLite Database
lab,sql_refresher.ipynb,code,1,"import sqlite3
import pandas as pd

def run_query(query):
    cursor.execute(query)
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    return pd.DataFrame(rows, columns=columns)

# Create in-memory SQLite DB and cursor
conn = sqlite3.connect("":memory:"")
cursor = conn.cursor()

# Create tables
query = """"""
CREATE TABLE customers (
    customer_id INTEGER PRIMARY KEY,
    first_name TEXT,
    last_name TEXT
);

CREATE TABLE orders (
    order_id INTEGER PRIMARY KEY,
    customer_id INTEGER,
    order_date TEXT,
    total_amount REAL,
    FOREIGN KEY(customer_id) REFERENCES customers(customer_id)
);

CREATE TABLE books (
    book_id INTEGER PRIMARY KEY,
    t"
lab,sql_refresher.ipynb,code,2,"itle TEXT,
    genre TEXT,
    qty INTEGER
);

CREATE TABLE users (
    id INTEGER PRIMARY KEY,
    username TEXT
);

CREATE TABLE likes (
    id INTEGER PRIMARY KEY,
    user_id INTEGER,
    post_id INTEGER
);
""""""
cursor.executescript(query)

# Insert sample data (expanded for richer results)
cursor.executemany(""INSERT INTO customers (first_name, last_name) VALUES (?, ?)"", [
    (""Alice"", ""Smith""),
    (""Bob"", ""Johnson""),
    (""Charlie"", ""Lee""),
    (""Diana"", ""Wang""),
    (""Ethan"", ""Brown""),
    (""Fiona"", ""Garcia""),
    (""George"", ""Martinez""),
    (""Hannah"", ""Kim""),
    (""Ivan"", ""Patel""),
    (""Julia"", ""Nguyen"")
])

cursor.executemany(""INSERT INTO orders (customer_id, order_date, total_amou"
lab,sql_refresher.ipynb,code,3,"nt) VALUES (?, ?, ?)"", [
    (1, ""2023-01-15"", 150.00),
    (2, ""2023-01-17"", 200.00),
    (1, ""2023-01-20"", 50.00),
    (3, ""2023-01-22"", 300.00),
    (4, ""2023-01-25"", 120.00),
    (5, ""2023-01-27"", 80.00),
    (6, ""2023-01-29"", 220.00),
    (7, ""2023-02-01"", 175.00),
    (8, ""2023-02-03"", 90.00),
    (9, ""2023-02-05"", 60.00),
    (10, ""2023-02-07"", 400.00),
    (2, ""2023-02-10"", 130.00),
    (3, ""2023-02-12"", 210.00),
    (4, ""2023-02-14"", 95.00),
    (5, ""2023-02-16"", 180.00)
])

cursor.executemany(""INSERT INTO books (title, genre, qty) VALUES (?, ?, ?)"", [
    (""Book A"", ""Fiction"", 5),
    (""Book B"", ""Fiction"", 10),
    (""Book C"", ""Non-Fiction"", 8),
    (""Book D"", ""Sci-Fi"", 12),
    (""B"
lab,sql_refresher.ipynb,code,4,"ook E"", ""Sci-Fi"", 7),
    (""Book F"", ""Fantasy"", 15),
    (""Book G"", ""Fantasy"", 3),
    (""Book H"", ""Mystery"", 9),
    (""Book I"", ""Mystery"", 11),
    (""Book J"", ""Non-Fiction"", 14),
    (""Book K"", ""Fiction"", 6),
    (""Book L"", ""Fantasy"", 8),
    (""Book M"", ""Sci-Fi"", 10),
    (""Book N"", ""Mystery"", 5),
    (""Book O"", ""Non-Fiction"", 13)
])

cursor.executemany(""INSERT INTO users (username) VALUES (?)"", [
    (""user1"",), (""user2"",), (""user3"",), (""user4"",), (""user5"",),
    (""user6"",), (""user7"",), (""user8"",), (""user9"",), (""user10"",)
])

cursor.executemany(""INSERT INTO likes (user_id, post_id) VALUES (?, ?)"", [
    (1, 1215), (2, 1215), (3, 1216), (1, 1217), (4, 1215), (5, 1216),
    (6, 1217), (7, 121"
lab,sql_refresher.ipynb,code,5,"5), (8, 1218), (9, 1218), (10, 1217), (2, 1216),
    (3, 1217), (4, 1218), (5, 1215), (6, 1216), (7, 1217), (8, 1215),
    (9, 1216), (10, 1218), (1, 1218), (2, 1217), (3, 1215), (4, 1216),
    (5, 1217), (6, 1218), (7, 1216), (8, 1217), (9, 1215), (10, 1216)
])

conn.commit()
# No fetch or DataFrame needed after table creation"
lab,sql_refresher.ipynb,code,1,"# Show table schemas for reference
print(""Customers:"")
display(run_query(""PRAGMA table_info(customers);""))
print(""Orders:"")
display(run_query(""PRAGMA table_info(orders);""))
print(""Books:"")
display(run_query(""PRAGMA table_info(books);""))
print(""Users:"")
display(run_query(""PRAGMA table_info(users);""))
print(""Likes:"")
display(run_query(""PRAGMA table_info(likes);""))"
lab,sql_refresher.ipynb,markdown,1,"#### Basic SQL SELECT Statement
The `SELECT` statement fetches data from a database. Try changing the columns or table name below to see different results."
lab,sql_refresher.ipynb,code,1,"query = ""SELECT * FROM customers;""
df = run_query(query)
display(df)

# Try: Change the table name or select specific columns!
# query = ""SELECT first_name, last_name FROM customers;""
# df = run_query(query)
# display(df)"
lab,sql_refresher.ipynb,markdown,1,"#### GROUP BY and HAVING
`GROUP BY` aggregates data across rows sharing the same value of specified columns. `HAVING` filters these grouped results. Try changing the condition in `HAVING`."
lab,sql_refresher.ipynb,code,1,"query = """"""
SELECT genre, SUM(qty) AS total_qty
FROM books
GROUP BY genre
HAVING total_qty > 10;
""""""
df = run_query(query)
display(df)

# Try: Change the HAVING condition or group by a different column!"
lab,sql_refresher.ipynb,markdown,1,"#### SQL JOINs
JOINs combine rows from two or more tables based on a related column. Try changing the join type or columns."
lab,sql_refresher.ipynb,code,1,"query = """"""
SELECT c.first_name, c.last_name, o.order_id, o.total_amount
FROM customers AS c
JOIN orders AS o ON c.customer_id = o.customer_id;
""""""
df = run_query(query)
display(df)

# Try: Change to LEFT JOIN or select different columns!"
lab,sql_refresher.ipynb,markdown,1,"#### Subquery in WHERE
Subqueries can be used inside a `WHERE` clause to filter results based on another query. Try changing the subquery or the main query."
lab,sql_refresher.ipynb,code,1,"query = """"""
SELECT id, username
FROM users
WHERE id IN (
    SELECT user_id FROM likes WHERE post_id = 1215
);
""""""
df = run_query(query)
display(df)

# Try: Change the post_id or select different columns!"
lab,sql_refresher.ipynb,markdown,1,"#### Subquery in FROM
Subqueries in the `FROM` clause create temporary tables for further querying. Try changing the aggregation or grouping."
lab,sql_refresher.ipynb,code,1,"query = """"""
SELECT AVG(total_likes)
FROM (
    SELECT post_id, COUNT(id) AS total_likes
    FROM likes
    GROUP BY post_id
) AS like_counts;
""""""
df = run_query(query)
display(df)

# Try: Show all post_ids and their like counts!
# query = ""SELECT post_id, COUNT(id) AS total_likes FROM likes GROUP BY post_id;""
# df = run_query(query)
# display(df)"
lab,sql_refresher.ipynb,markdown,1,"#### Converting SQL Results to Pandas DataFrame
After executing SQL queries, you can convert the result set into a pandas DataFrame for easier manipulation and visualization."
lab,sql_refresher.ipynb,code,1,"query = ""SELECT * FROM customers""
df = run_query(query)
df.head()"
lab,sql_refresher.ipynb,markdown,1,"#### Practice: Write Your Own Query
Try writing your own SQL query below! For example, select all orders over $100, or count the number of books per genre."
lab,sql_refresher.ipynb,code,1,"# Example: Select all orders over $100
query = ""SELECT * FROM orders WHERE total_amount > 100;""
df = run_query(query)
display(df)

# Try your own query below:
# query = ""YOUR SQL HERE""
# df = run_query(query)
# display(df)"
lab,sql_refresher.ipynb,markdown,1,"#### Visualizing Aggregated Data
Let's visualize the total quantity of books by genre using a bar chart."
lab,sql_refresher.ipynb,code,1,"import matplotlib.pyplot as plt

df = run_query(""SELECT genre, SUM(qty) AS total_qty FROM books GROUP BY genre;"")
df.plot(kind=""bar"", x=""genre"", y=""total_qty"", legend=False)
plt.ylabel(""Total Quantity"")
plt.title(""Total Books by Genre"")
plt.show()"
lab,sql_refresher.ipynb,markdown,1,"#### More Practice: Try a JOIN or Subquery
Write a query that joins two tables or uses a subquery. For example, find all customers who have placed more than one order."
lab,sql_refresher.ipynb,code,1,"# Example: Customers with more than one order
query = """"""
SELECT c.first_name, c.last_name, COUNT(o.order_id) as num_orders
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_id
HAVING num_orders > 1;
""""""
df = run_query(query)
display(df)

# Try your own JOIN or subquery below:
# query = ""YOUR SQL HERE""
# df = run_query(query)
# display(df)"
lab,sql_refresher.ipynb,markdown,1,"### SQL SELECT Statement (Review)
The `SELECT` statement is fundamental to SQL. It is used to query data from a table. Here are a few variants. Try each and see the results!"
lab,sql_refresher.ipynb,code,1,"# Select specific columns
query = ""SELECT first_name, last_name FROM customers;""
df = run_query(query)
display(df)

# Try: Select only first names or only last names!"
lab,sql_refresher.ipynb,code,1,"# Select with a condition using WHERE
query = ""SELECT * FROM orders WHERE total_amount > 100;""
df = run_query(query)
display(df)

# Try: Change the amount or use a different column!"
lab,sql_refresher.ipynb,code,1,"# Select with ordering
query = ""SELECT * FROM orders ORDER BY total_amount DESC;""
df = run_query(query)
display(df)

# Try: Order by order_date or customer_id!"
lab,sql_refresher.ipynb,markdown,1,"### GROUP BY and HAVING (Review)
`GROUP BY` is used to group rows with the same values in specified columns. `HAVING` is used to filter groups based on aggregate functions. Try changing the grouping or the HAVING condition."
lab,sql_refresher.ipynb,code,1,"# Group books by genre and sum quantity
query = ""SELECT genre, SUM(qty) AS total_qty FROM books GROUP BY genre;""
df = run_query(query)
display(df)

# Try: Group by a different column or use COUNT instead of SUM!"
lab,sql_refresher.ipynb,code,1,"# Filter grouped results using HAVING
query = """"""
SELECT genre, SUM(qty) AS total_qty
FROM books
GROUP BY genre
HAVING total_qty >= 10;
""""""
df = run_query(query)
display(df)

# Try: Change the HAVING threshold!"
lab,sql_refresher.ipynb,markdown,1,"### SQL JOINs (Review)
JOINs are used to combine data from two or more tables based on a related column.
- `INNER JOIN`: returns only matching rows
- `LEFT JOIN`: returns all rows from the left table, even if there are no matches in the right table
Try both join types below!"
lab,sql_refresher.ipynb,code,1,"# INNER JOIN
query = """"""
SELECT c.first_name, o.order_id, o.total_amount
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id;
""""""
df = run_query(query)
display(df)"
lab,sql_refresher.ipynb,code,1,"# LEFT JOIN: shows all customers, even those without orders
query = """"""
SELECT c.first_name, o.order_id
FROM customers c
LEFT JOIN orders o ON c.customer_id = o.customer_id;
""""""
df = run_query(query)
display(df)

# Try: Show all customers and their total order amounts!"
lab,sql_refresher.ipynb,markdown,1,"### Subqueries (Review)
Subqueries can be nested inside `SELECT`, `FROM`, `WHERE`, or `HAVING` clauses. They are used to break down complex queries into manageable parts. Try each example and then write your own!"
lab,sql_refresher.ipynb,code,1,"# Subquery in WHERE clause
query = """"""
SELECT username FROM users
WHERE id IN (
    SELECT user_id FROM likes WHERE post_id = 1215
);
""""""
df = run_query(query)
display(df)

# Try: Change the post_id or select a different column!"
lab,sql_refresher.ipynb,code,1,"# Subquery in FROM clause
query = """"""
SELECT AVG(total_likes)
FROM (
    SELECT post_id, COUNT(*) AS total_likes
    FROM likes
    GROUP BY post_id
) AS like_counts;
""""""
df = run_query(query)
display(df)

# Try: Show the like counts for each post!"
lab,sql_refresher.ipynb,code,1,"# Subquery in HAVING clause
query = """"""
SELECT customer_id, AVG(total_amount) as avg_amt
FROM orders
GROUP BY customer_id
HAVING avg_amt > (
    SELECT AVG(total_amount) FROM orders
);
""""""
df = run_query(query)
display(df)

# Try: Change the HAVING condition or use a different aggregate!"
lab,sql_refresher.ipynb,markdown,1,"#### 🚀 Interactive SQL Playground (Beta!)
Type your own SQL query below and see the results instantly! This cell uses an interactive form for live SQL experimentation. Try different SELECT, JOIN, GROUP BY, or even subqueries. If you break it, just re-run the setup cell above!"
lab,sql_refresher.ipynb,code,1,"import ipywidgets as widgets
from IPython.display import display, clear_output

# Inject custom CSS for dark mode styling
# This will work in JupyterLab, VS Code, and classic Jupyter
# and will only affect this widget

display(widgets.HTML(""""""
<style>
.jp-Notebook .custom-sql-dark textarea, .custom-sql-dark textarea {
    background: #222 !important;
    color: #eee !important;
    border: 1px solid #444 !important;
    font-family: 'Fira Mono', 'Consolas', monospace;
}
</style>
""""""))

# Default query
default_query = ""SELECT * FROM customers;""

# Create a text area for SQL input
sql_input = widgets.Textarea(
    value=default_query,
    placeholder='Type your SQL query here',
    description"
lab,sql_refresher.ipynb,code,2,"='SQL:',
    layout=widgets.Layout(width='100%', height='80px'),
    style={'description_width': 'initial'}
)
sql_input.add_class(""custom-sql-dark"")

# Create a button to run the query
run_button = widgets.Button(
    description='Run Query',
    button_style='success',
    tooltip='Execute the SQL query and show results',
    icon='play'
)

# Output area for results
output = widgets.Output()

# Function to run the query and display results
def on_run_clicked(b):
    with output:
        clear_output()
        try:
            df = run_query(sql_input.value)
            if df.empty:
                print('No results returned.')
            else:
                display(df)
        except Exc"
lab,sql_refresher.ipynb,code,3,"eption as e:
            print(f""Error: {e}"")

run_button.on_click(on_run_clicked)

# Display the interactive form
form = widgets.VBox([
    widgets.HTML('<b>Try your own SQL query below:</b>'),
    sql_input,
    run_button,
    output
])
display(form)"
lab,sql_refresher.ipynb,markdown,1,"*Tip: Try queries like* `SELECT * FROM orders WHERE total_amount > 100;` *or* `SELECT genre, COUNT(*) FROM books GROUP BY genre;` *or even joins and subqueries!*"
lab,hyperparameters.ipynb,markdown,1,"# Hyperparameters Code-Along

Implement the code-blocks below in order to implement RandomSearchCV. We will be using the `diabetes` toy dataset which we can directly load from sklearn."
lab,hyperparameters.ipynb,code,1,"# toy diabetes dataset
from sklearn.datasets import load_diabetes

# regressor models
from sklearn.linear_model import Lasso

# accuracy metrics
from sklearn.metrics import mean_squared_error, r2_score

# train/test/CV split
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# time to time your model
import time"
lab,hyperparameters.ipynb,code,1,"# load dataset
X, y = load_diabetes(return_X_y=True, as_frame=True)

# TODO: split your dataset and create a testing set with 20% of your data
X_train, X_test, y_train, y_test = train_test_split(..., ..., test_size=..., random_state=42)

print(f""Shape of X_train {X_train.shape}"")
print(f""Shape of X_test {X_test.shape}"")
print(f""Shape of y_train {y_train.shape}"")
print(f""Shape of y_test {y_test.shape}"")"
lab,hyperparameters.ipynb,code,1,"# TODO: train a Lasso model on your X_train and y_train
lasso = ...

lasso.fit(X_train, y_train)"
lab,hyperparameters.ipynb,code,1,"# Predict on your test set
y_test_pred = lasso.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)

print(f""Test MSE (Basic LASSO, alpha=1.0): {mse_test:.2f}"")"
lab,hyperparameters.ipynb,code,1,"# tune with gridsearchcv and time
alpha_grid = {'alpha': np.linspace(0.01, 10, 100)}

lasso_model = Lasso()

# start the timer
start_time = time.time()

grid_search = GridSearchCV(estimator=lasso_model, param_grid=alpha_grid, cv=5)
grid_search.fit(X_train, y_train)

# end the timer after we're done fitting
end_time = time.time()
# calculate elapsed time
elapsed_time_grid = end_time - start_time

# Extract the best model from grid search
best_alpha_grid = grid_search.best_params_['alpha']
y_test_pred_grid = grid_search.best_estimator_.predict(X_test)
mse_test_grid = mean_squared_error(y_test, y_test_pred_grid)

print(f""GridSearchCV - Best alpha: {best_alpha_grid}"")
print(f""GridSearchCV - Test"
lab,hyperparameters.ipynb,code,2," MSE: {mse_test_grid:.2f}"")
print(f""GridSearchCV - Time elapsed: {elapsed_time_grid:.2f} seconds"")"
lab,hyperparameters.ipynb,code,1,"# tune with randomsearchcv
alpha_grid = {'alpha': np.linspace(0.01, 10, 100)}

lasso_model = Lasso()

# start the timer
start_time = time.time()

grid_search = RandomizedSearchCV(estimator=lasso_model, param_distributions=alpha_grid, cv=5)
grid_search.fit(X_train, y_train)

# end the timer after we're done fitting
end_time = time.time()
# calculate elapsed time
elapsed_time_grid = end_time - start_time

# Extract the best model from random search
best_alpha_grid = grid_search.best_params_['alpha']
y_test_pred_grid = grid_search.best_estimator_.predict(X_test)
mse_test_grid = mean_squared_error(y_test, y_test_pred_grid)

print(f""RandomizedSearchCV - Best alpha: {best_alpha_grid}"")
print(f""Ran"
lab,hyperparameters.ipynb,code,2,"domizedSearchCV - Test MSE: {mse_test_grid:.2f}"")
print(f""RandomizedSearchCV - Time elapsed: {elapsed_time_grid:.2f} seconds"")"
lab,hyperparameters.ipynb,markdown,1,"# Challenge

Create codeblocks below to generate and hypertune a Ridge regression model using both `GridSearchCV` and `RandomizedSearchCV` on your dataset. 

After creating your model, evaluate its' MSE and R^2. Calculate the models residual plot to evaluate the validity of your predictions. 

Answer the analytical questions listed below as well.

Utilize the code written above, as well as the [Ridge sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to inform your answers!"
lab,hyperparameters.ipynb,code,1,"# TODO: Implement your Ridge regression model and tune it using RandomizedSearchCV
..."
lab,hyperparameters.ipynb,markdown,1,"## Writeup

Answer the analytical questions below using the metrics you've calculated.

1) It appears that `GridSearchCV` often finds better hyperparameters than `RandomizedSearchCV`. Why might we tolerate this loss in accuracy when dealing with large datasets and still prefer to use `RandomizedSearchCV`? 

...

2) Is there any difference between the MSE of ridge regression trained with RandomizedSearchCV vs lasso trained with RandomizedSearchCV? If so, which model performs best (i.e. which model has the lowest MSE)? Which metrics lead you to this conclusion?  

..."
lab,lasso_ridge.ipynb,markdown,1,"# Lasso & Ridge Code-Along

Implement the code-blocks below in order to implement the lasso regressor. We will be using the `diabetes` toy dataset which we can directly load from sklearn."
lab,lasso_ridge.ipynb,code,1,"# toy diabetes dataset
from sklearn.datasets import load_diabetes

# regressor models
from sklearn.linear_model import Lasso, Ridge
from sklearn.linear_model import LinearRegression

# accuracy metrics
from sklearn.metrics import mean_squared_error, r2_score

import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns"
lab,lasso_ridge.ipynb,code,1,"# load dataset
X, y = load_diabetes(return_X_y=True, as_frame=True)

# TODO: select only the ""age"" and ""bmi"" predictors for your predictors
X = X[...]

# view first 5 rows
X.head()"
lab,lasso_ridge.ipynb,code,1,"# TODO: Perform exploratory analysis on your ""X"" dataframe. Create multiple codeblocks!

..."
lab,lasso_ridge.ipynb,code,1,"# generate a linear regression model and fit your data
model = LinearRegression()
model.fit(X, y)

print(""Learned coefficients"", model.coef_, ""\n"")
print(""Learned intercept"", model.intercept_, ""\n"")"
lab,lasso_ridge.ipynb,code,1,"# generate estimated predictions on your X dataset to evaluate your MSE
y_pred = model.predict(X)

mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f""MSE: {mse:.2f}"")
print(f""R^2: {r2:.2f}"")"
lab,lasso_ridge.ipynb,code,1,"# calculate residuals off of your predictions
residuals = y - y_pred

# plot a residual plot
plt.scatter(y_pred, residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()"
lab,lasso_ridge.ipynb,code,1,"# TODO: fit a LASSO model and evaluate your error metrics
lasso = Lasso(alpha=1.0)
lasso.fit(..., ...)

y_pred_lasso = lasso.predict(...)"
lab,lasso_ridge.ipynb,code,1,"mse_lasso = mean_squared_error(..., ...)
r2_lasso = r2_score(..., ...)

print(f""Lasso MSE: {mse:.2f}"")
print(f""Lasso R^2: {r2:.2f}"")"
lab,lasso_ridge.ipynb,code,1,"# TODO: generate a residual plot on your lasso calculations
residuals = y - y_pred_lasso

# plot a residual plot
plt.scatter(..., ...)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()"
lab,lasso_ridge.ipynb,markdown,1,"# Challenge

Create codeblocks below to generate OLS, Lasso, and Ridge regression models on your dataset. This time however, instead of only selecting ""age"", and ""bmi"" you will include **all** predictor variables.

After creating your models, evaluate their MSE and R^2. Calculate each models residual plot to evaluate the validity of your predictions. 

Answer the analytical questions listed below as well.

Utilize the code written above, as well as the [Ridge sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to inform your answers!"
lab,lasso_ridge.ipynb,code,1,"# TODO: Implement all 3 models (Linear, Lasso, Ridge) on all your predictors
..."
lab,lasso_ridge.ipynb,markdown,1,"## Writeup

Answer the analytical questions below using the metrics you've calculated.

1) Is there any difference between the MSE of the OLS model trained only on `bmi` and `age`, and the OLS model trained on all predictors? If so, which model performs best (i.e. which model has the lowest MSE)? Which metrics lead you to this conclusion?  

...

2) Does there seem to be any difference in the MSE of OLS, Lasso, and Ridge regression when evaluating all predictors? If so, which model performs best? Which metrics lead you to this conclusion?  

..."
lab,accuracy.ipynb,markdown,1,"# Logistic Regression Accuracy Code-Along

Implement the code-blocks below in order to evaluate the accuracy, precision, recall, specificity, F1 score and ROC curve of the regressor you trained yesterday. We will be using the `breast_cancer` toy dataset which we can directly load from sklearn."
lab,accuracy.ipynb,code,1,"from sklearn.datasets import load_breast_cancer

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns"
lab,accuracy.ipynb,code,1,"# load dataset
X, y = load_breast_cancer(return_X_y=True, as_frame=True)

# view first 5 rows of predictors
X.head()"
lab,accuracy.ipynb,code,1,"# TODO: select only the ""mean radius"", ""mean texture"", and ""mean perimeter"" predictors for your predictors
...

# TODO: split into test and training sets
...

# view first 5 rows of training data
X_train.head()"
lab,accuracy.ipynb,code,1,"# initialize and train the model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# generate a confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(""Confusion Matrix:\n"", cm)"
lab,accuracy.ipynb,code,1,"# calculate all measures of accuracy
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# calculate specificity by hand
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f""Accuracy: {accuracy:.2f}"")
print(f""Precision: {precision:.2f}"")
print(f""Recall (Sensitivity): {recall:.2f}"")
print(f""Specificity: {specificity:.2f}"")
print(f""F1 Score: {f1:.2f}"")"
lab,accuracy.ipynb,code,1,"# get probability scores for ROC curve
y_probs = model.predict_proba(X_test)[:, 1]

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc = roc_auc_score(y_test, y_probs)

# Plotting
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f""AUC = {auc:.2f}"")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel(""False Positive Rate"")
plt.ylabel(""True Positive Rate"")
plt.title(""ROC Curve"")
plt.legend()
plt.grid(True)
plt.show()"
lab,accuracy.ipynb,markdown,1,"# Challenge

Recreate your logistic regressor on the `hotel` dataset. Your target variable will be `is_canceled` column, while your predictor will be the `lead_time` column.

After creating your model extract best hyperparameters using both `RandomizedSearchCV` and `GridSearchCV`. Evaluate the accuracy, precision, recall, specificity, F1 Score, and ROC curve of both your predictions.

Answer the analytical questions listed below as well."
lab,accuracy.ipynb,code,1,"# TODO: Implement the logistic regression model on the `hotel.csv` dataset
..."
lab,accuracy.ipynb,markdown,1,"## Writeup

Answer the analytical questions below using the metrics you've calculated.

1) Did your `RandomizedSearchCV` model contain more precision or recall? Did this lead to greater false negatives or greater false positives? 

...

2) According to the AUC score, did `RandomizedSearchCV` and `GridSearchCV` generate a better model?

..."
lab,logistic_regression.ipynb,markdown,1,"# Logistic Regression Code-Along

Implement the code-blocks below in order to implement the logistic regressor. We will be using the `breast_cancer` toy dataset which we can directly load from sklearn."
lab,logistic_regression.ipynb,code,1,"from sklearn.datasets import load_breast_cancer

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import time"
lab,logistic_regression.ipynb,code,1,"# load dataset
X, y = load_breast_cancer(return_X_y=True, as_frame=True)

# view first 5 rows of predictors
X.head()"
lab,logistic_regression.ipynb,code,1,"# view first 5 random samples of target data
y.sample(5)"
lab,logistic_regression.ipynb,code,1,"# TODO: select only the ""mean radius"", ""mean texture"", and ""mean perimeter"" predictors for your predictors
X = ...

# TODO: split into test and training sets
X_train, X_test, y_train, y_test = ...

# view first 5 rows of training data
X_train.head()"
lab,logistic_regression.ipynb,code,1,"# Randomly search for the best hyperparameters on a logistic regression model
param_dist = {
    'penalty': ['l1', 'l2'],
    'C': np.linspace(0.01, 1, 100),
    'solver': ['saga'], 
    'max_iter': [10000]
}

random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X_train, y_train)

# Best model from random search
best_params_random = random_search.best_params_
best_score_random = random_search.best_score_

print(f""RandomizedSearchCV - Best Params: {best_params_random}"")
print(f""RandomizedSearchCV - Cross-Val Accuracy: {best_score_random:.2f}"")"
lab,logistic_regression.ipynb,code,1,"# Use the best model found from RandomizedSearchCV to predict on unseen test data

# extract the best estimator
best_log = random_search.best_estimator_

# predict on testing data
log_predictions = best_log.predict(X_test)

# evaluate its accuracy
test_score = accuracy_score(log_predictions, y_test)

print(f""RandomizedSearchCV - Coefficients: {best_log.coef_}"")
print(f""RandomizedSearchCV - Test Accuracy: {test_score:.2f}"")"
lab,logistic_regression.ipynb,markdown,1,"The coefficients above relate to the predictors ""mean radius"", ""mean texture"", and ""mean perimeter.""

Note that positive values indicate a higher log-odds that a tumor is malignant. However, negative values indicate a lower log-odds that a tumor is malignant.

Which predictor seems to indicate a higher probability that a tumor is malignant?"
lab,logistic_regression.ipynb,code,1,"# Now we will see why we prefer to use RandomizedSearchCV
# Search the grid for the best hyperparameters on a logistic regression model (this will take at least 3 minutes!)
grid_search = GridSearchCV(LogisticRegression(), param_grid=param_dist, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best model from grid search
best_params_grid = grid_search.best_params_
best_score_grid = grid_search.best_score_

print(f""GridSearchCV - Best Params: {best_params_grid}"")
print(f""GridSearchCV - Cross-Val Accuracy: {best_score_grid:.2f}"")"
lab,logistic_regression.ipynb,markdown,1,"# Challenge

Create a logistic regressor on the `hotel` dataset. Your target variable will be `is_canceled` column, while your predictor will be the `lead_time` column.

After creating your model extract best hyperparameters using both `RandomizedSearchCV` and `GridSearchCV`. Evaluate the coefficients and the accuracy of both hyperparameter finding algorithms (we will learn more about classification accuracy metrics tomorrow). 

Answer the analytical questions listed below as well."
lab,logistic_regression.ipynb,code,1,"# TODO: Implement the logistic regression model on the `hotel.csv` dataset
..."
lab,logistic_regression.ipynb,markdown,1,"## Writeup

Answer the analytical questions below using the metrics you've calculated.

1) What was the accuracy of your hyperparameters extracted from `GridSearchCV`? Was this any different from `RandomizedSearchCV`? Did the extended run-time of `GridSearchCV` provide huge increases in accuracy?

...

2) According to your coefficients, how did `lead_time` influence the probability that a booking would be cancelled? 

..."
lab,bayes_theorem.ipynb,markdown,1,"# Lab: Machine Learning and Text Classification Fundamentals

Welcome to this lab session designed to introduce core machine learning concepts and text classification techniques. In this lab you will:

- Differentiate between supervised and unsupervised learning.
- Visualize and simulate the training process of a simple model.
- Discuss multi-class classification strategies, including ""one vs. one"" and ""one vs. all"" approaches.
- Apply basic probability concepts using Bayes theorem.
- Implement a plug and play example and then extend it to calculate word frequencies in text.

These skills are not just academic; they also mirror practical steps you might take on the job, such as debugging mod"
lab,bayes_theorem.ipynb,markdown,2,"el training issues, optimizing learning schedules, and processing textual data for insights.

Let's get started!"
lab,bayes_theorem.ipynb,markdown,1,"## Understanding Learning Paradigms

Supervised learning involves training a model on labeled data, where each input has a corresponding output label. In contrast, unsupervised learning uses data without explicit labels to try to identify inherent patterns or groupings.

We will learn more about unsupervised learning techniques in later parts of phase 2. This entails tasks such as:
* Recommending the next song/article/movie to watch for uncategorized users
* Reducing the size of your dataset to get better predictive capabilities
* Predicting the next word in a sentence."
lab,bayes_theorem.ipynb,markdown,1,"## Visualizing the Training Process (Plug and Play)

Below is a simple simulation of the training process of a supervised model. The code initializes some weights, updates them iteratively by subtracting a fraction of the current error, and decreases the error over iterations. Run the following code cell to see how the weights are updated step by step."
lab,bayes_theorem.ipynb,code,1,"# Initialize weights (starting with random values)

# think of these two values as your beta-0 and beta-1 coefficients (B0 + B1X)
# TODO: set these betas to different values, see how your MSE changes!
b0, b1 = 0.0, 0.0

# think of these as your ""true"" samples. First data point is X, second is Y
y = [
    [1, 2],
    [2, 3],
    [3, 5],
    [4, 7]
]
n = len(y)

# your starting error
# calculated by creating prediction and measuring error (MSE)
total_error = 0
for data in y:
    yi = data[1]
    xi = data[0]
 
    # B0 + B1 * X
    yhat_i = b0 + (b1 * xi)
    # calculate squared residual
    error = (yi - yhat_i)**2

    # add up error
    total_error += error
    

print(""Mean squared error i"
lab,bayes_theorem.ipynb,code,2,"s"", total_error / n )"
lab,bayes_theorem.ipynb,code,1,"# the learning rate of your model
# TODO: play around with this learning_rate, what happens to our training?
learning_rate = 0.1

print(""Training process simulation:"")

for i in range(5):
    # calculate adjustment based on current error and learning rate
    b0_gradient = 0
    b1_gradient = 0

    # calcuate gradient descent on all data-points1
    for data in y:
        xi = data[0]
        yi = data[1]
        yhat_i = b0 + b1 * xi 
        error = yhat_i - yi

        # Partial derivatives of the MSE loss function
        b0_gradient += error
        b1_gradient += error * xi

    # calculate the average of all gradients
    b0_gradient = (2 / n) * b0_gradient
    b1_gradient = (2 / n) "
lab,bayes_theorem.ipynb,code,2,"* b1_gradient

    # calculate your updated weights 
    b0 = b0 - learning_rate * b0_gradient
    b1 = b1 - learning_rate * b1_gradient
    
    # measure your new error
    total_error = 0
    for data in y:
        yi = data[1]
        xi = data[0]
    
        # B0 + B1 * X
        yhat_i = b0 + (b1 * xi)
        # calculate squared residual
        error = (yi - yhat_i)**2

        # add up error
        total_error += error / n

    print(""Iteration"", i+1, ""we get weights:"", b0, b1, ""with error:"", round(total_error, 4))"
lab,bayes_theorem.ipynb,markdown,1,"## Multi-Class Classification Strategies

In many classification problems, you have to decide how to handle scenarios with more than two classes. Two common strategies are:

- **One vs. One Classification:** A classifier is trained for every pair of classes.
- **One vs. All Classification:** A classifier is trained for each class against all the others.

Below, we demonstrate how we can create one v one classifiers using the `iris` dataset, which contains samples of 3 kinds of flowers. Using multiple logistic regression models, we can c

But first let's perform exploratory data analysis."
lab,bayes_theorem.ipynb,code,1,"import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""iris.csv"")

df.head()"
lab,bayes_theorem.ipynb,markdown,1,### Univariate Analysis
lab,bayes_theorem.ipynb,code,1,"# View distribution of petal.length
sns.histplot(data=df, x=""petal.length"", fill=True)
plt.xlabel(""petal.length"")
plt.ylabel('Density')
plt.show()"
lab,bayes_theorem.ipynb,code,1,# TODO: continue your univariate analysis!
lab,bayes_theorem.ipynb,markdown,1,### Bivariate Analysis
lab,bayes_theorem.ipynb,code,1,"species_colors = {'setosa': 'blue', 'versicolor': 'orange', 'virginica': 'green'}

sns.swarmplot(data=df, x='variety', y=""petal.length"", palette=species_colors.values())
plt.xlabel('variety')
plt.ylabel(""petal.length"")
plt.show()"
lab,bayes_theorem.ipynb,code,1,# TODO: continue your bivariate analysis!
lab,bayes_theorem.ipynb,markdown,1,### One vs One Machine Learning
lab,bayes_theorem.ipynb,code,1,"from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsOneClassifier
from sklearn.metrics import classification_report

# TODO: based on your analysis, which predictor shows clear differences amongst species?
X = df[[""petal.length""]]

y = df[""variety""]

# form train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# generate base logistic model
base_model = LogisticRegression()

# use base model in classifier
ovo_clf = OneVsOneClassifier(base_model)"
lab,bayes_theorem.ipynb,code,1,"# define hyperparameter grid
param_grid = {
    'estimator__C': [0.01, 0.1, 1, 10],
    'estimator__penalty': ['l1', 'l2'],
    'estimator__solver': ['saga'], 
    'estimator__max_iter': [10000]
}

# use GridSearchCV to search over hyperparameters
grid_search = GridSearchCV(ovo_clf, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best model after hyperparameter tuning
best_model = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_model.predict(X_test)
print(""Best Parameters:"", grid_search.best_params_)
print(""\nClassification Report:\n"", classification_report(y_test, y_pred))"
lab,bayes_theorem.ipynb,markdown,1,"## Bayes Theorem Calculation

Bayes theorem helps us update our belief about a hypothesis given new evidence. The formula is:

   Posterior = (Likelihood × Prior) / Evidence

The code below implements a simple calculation using this formula. Run the cell to see how the posterior probability is computed."
lab,bayes_theorem.ipynb,code,1,"# Trial 1
prior = 0.98         # Initial belief (prior probability) (P(H))
likelihood = 0.5    # Probability of observing the evidence given the hypothesis (P(E|H))

evidence = (prior * 0.5) + ((1 - prior) * 0.8)    # Overall probability of observing the evidence (P(H)P(E|H) + P(-H)P(E|-H))

# Calculate the posterior probability using Bayes Theorem
posterior = (likelihood * prior) / evidence

print(""Posterior probability for trial 1 (updated likelihood that coin is fair):"", posterior)"
lab,bayes_theorem.ipynb,code,1,"# Trial 2: reuse previous posterior probability to calculate updated metrics

prior = posterior
evidence = (posterior * 0.5) + ((1-posterior) * 0.8)

# Calculate the posterior probability using Bayes Theorem
posterior = (likelihood * prior) / evidence

print(""Posterior probability for trial 2 (updated likelihood that coin is fair):"", posterior)"
lab,bayes_theorem.ipynb,code,1,"# Trial 3: again, update probability to view updated likelihood that coin is fair

prior = posterior
evidence = (posterior * 0.5) + ((1-posterior) * 0.8)

# Calculate the posterior probability using Bayes Theorem
posterior = (likelihood * prior) / evidence

print(""Posterior probability for trial 3 (updated likelihood that coin is fair):"", posterior)"
lab,bayes_theorem.ipynb,code,1,"# Trial 4: evidence in the other direction (what if we flip a tails?)

prior = posterior
evidence = (posterior * 0.5) + ((1-posterior) * 0.2)

# Calculate the posterior probability using Bayes Theorem
posterior = (likelihood * prior) / evidence

print(""Posterior probability for trial 4 (updated likelihood that coin is fair):"", posterior)"
lab,bayes_theorem.ipynb,markdown,1,"## Word Frequency for Text Classification

A simple yet powerful method for text classification is counting the frequency of words in a text (as we will see in next weeks classifier). 

Let's get introduced to this idea by splitting a sample sentence into words, counts how often each word appears, and displays the result in a table using pandas.

Let's see if we can subsequently use this to calculate the probability of a text being spam or not spam."
lab,bayes_theorem.ipynb,code,1,"# Spam text data
spam_texts = [
    ""hello your USPS package was not able to be delivered. click here!"",
    ""FREE PHONE! click here!"",
    ""USPS package not delivered! click here!""
]

# Normal text data
normal_texts = [
    ""hey whats the central limit theorem"",
    ""hey my phone is about to die""
]

# Split the text into individual words and count up the frequencies
spam_frequencies = {}
normal_frequencies = {}

# count up frequencies for spam texts
for text in spam_texts:
    # remove all punctuation marks (can punctuation be used to classify spam?)
    no_punctuation = text.replace(""."", """").replace(""!"", """")

    # split text and count up frequencies
    words = no_punctuation.split()
    "
lab,bayes_theorem.ipynb,code,2,"for w in words:
        if w not in spam_frequencies:
            spam_frequencies[w] = 1
        else:
            spam_frequencies[w] += 1

# count up frequencies for normal texts
for text in normal_texts:
    # remove all punctuation marks (can punctuation be used to classify spam?)
    no_punctuation = text.replace(""."", """").replace(""!"", """")

    # split text and count up frequencies
    words = no_punctuation.split()
    for w in words:
        if w not in normal_frequencies:
            normal_frequencies[w] = 1
        else:
            normal_frequencies[w] += 1

print(""spam frequency"", spam_frequencies)
print(""normal word frequency"",normal_frequencies)"
lab,bayes_theorem.ipynb,code,1,"# normalize both frequency tables (divide by number of total texts)
for w in spam_frequencies:
    spam_frequencies[w] = round(spam_frequencies[w] / len(spam_frequencies), 2)

for w in normal_frequencies:
    normal_frequencies[w] = round(normal_frequencies[w] / len(normal_frequencies), 2)

print(""spam frequency"", spam_frequencies)
print(""normal word frequency"",normal_frequencies)"
lab,bayes_theorem.ipynb,code,1,"# we also need to include words that dont show up in both dictionaries and assign them a probability of 0
spam_words = list(spam_frequencies.keys())
normal_words = list(normal_frequencies.keys())

for w in spam_words:
    normal_frequencies[w] = 0

for w in normal_words:
    spam_frequencies[w] = 0

print(""spam frequency"", spam_frequencies)
print(""normal word frequency"",normal_frequencies)"
lab,bayes_theorem.ipynb,code,1,"# use these frequency tables to calculate probability that a new text is spam or not!

new_text = ""Do you want a job? Click here!""

# use bayes theorem to calculate if this text is a spam (or not spam)

no_punctuation = new_text.replace(""."", """").replace(""!"", """")
words = no_punctuation.split()

# use ratio of spam to not spam texts to calculate initial probability
spam_probability = len(spam_texts) / (len(spam_texts) + len(normal_texts))
not_spam_probability = len(normal_texts) / (len(spam_texts) + len(normal_texts))

for w in words:
    if w in normal_frequencies and w in spam_frequencies:
        # ignore 0's for now
        if normal_frequencies[w] == 0:
            continue
        spam_p"
lab,bayes_theorem.ipynb,code,2,"robability = spam_probability * normal_frequencies[w]
        not_spam_probability = not_spam_probability * normal_frequencies[w]

print(spam_probability)
print(not_spam_probability)"
lab,bayes_theorem.ipynb,markdown,1,"## Wrap-Up

In this lab, you explored several foundational topics in machine learning and text processing:

- You examined the differences between supervised and unsupervised learning and considered real-world applications.
- You simulated a training process and extended that simulation to include a variable learning rate, linking the practice to how model optimization might occur in industry.
- You discussed multi-class classification strategies and their pros and cons.
- You applied Bayes theorem for probability updating and enhanced the example to interact with user inputs.
- Finally, you implemented a word frequency counter for text classification, then extended it to process external te"
lab,bayes_theorem.ipynb,markdown,2,"xt files.

These exercises build a bridge between theoretical concepts and their practical applications, a crucial skill in any data-driven role."
lab,feature_engineering.ipynb,markdown,1,"<div style=""background: #000;
            color: #FFF;
            margin: 0px;
            padding: 10px 0px 20px 0px;
            text-align: center; 
                "">
    <h1>Data: Types, Integrity and Accuracy</h1>
</div>"
lab,feature_engineering.ipynb,markdown,1,"### Numerical
**Numerical data** is information that is measurable, and it is, of course, data represented as numbers and not words or text.

**Continuous numbers** are numbers that don’t have a logical end to them. Examples include variables that represent money or height.

**Discrete numbers** are the opposite; they have a logical end to them. Some examples include variables for days in the month, or number of bugs logged.

# Categorical  

For categorical data, this is any data that isn’t a number, which can mean a string of text or date. These variables can be broken down into nominal and ordinal values, though you won’t often see this done.

**Ordinal values** are values that have a set"
lab,feature_engineering.ipynb,markdown,2," order to them. Examples of ordinal values include having a priority on a bug such as “Critical” or “Low” or the ranking of a race as “First” or “Third”. Nominal values are the opposite of ordinal values, and they represent values with no set order to them. Nominal value examples include variables such as “Country” or “Marital Status”.

In addition to ordinal and nominal values, there is a special type of categorical data called **binary**. Binary data types only have two values – yes or no. This can be represented in different ways such as “True” and “False” or 1 and 0. Binary data is used heavily for classification machine learning models. Examples of binary variables can include whether a"
lab,feature_engineering.ipynb,markdown,3," person has stopped their subscription service or not, or if a person bought a car or not."
lab,feature_engineering.ipynb,markdown,1,"### Where to get data?

There are lots of way to get data, and lots of places to get it from. Typically, most of this data will be accessed through the internet, in one way or another, especially when pursuing indepent research projects.
Institutional Access

If you are working with data as part of an institution, such as a company of research lab, the institution will typically have data it needs analyzing, that it collects in various ways. Keep in mind that even people working inside institutions, with access to local data, will data still seek to find and incorporate external datasets.
Data Repositories

Data repositories are databases from which you can download data. Some data repositor"
lab,feature_engineering.ipynb,markdown,2,"ies allow you to explore available datasets and download datasets in bulk. Others may also offer APIs, through which you can request specific data from particular databases.
Web Scraping

The web itself is full of unstructured data. Web scraping can be done to directly extract and collect data directly from websites.
Asking People for Data

Not all data is indexed or accessible on the web, at least not publicly. Sometimes finding data means figuring out if any data is available, figuring out where it might be, and then reaching out and asking people directly about data access. If there is some particular data you need, you can try to figure out who might have it, and get in touch to see if i"
lab,feature_engineering.ipynb,markdown,3,"t might be available.
Data Gathering Skills

Depending on your gathering method, you will likely have to do some combination of the following:
* Direct download data files from repositories
* Query databases & use APIs to extract and collect data of interest
* Ask people for data, and going to pick up data with a harddrive

Ultimately, the goal is collect and curate data files, hopefully structured, that you can read into Python."
lab,feature_engineering.ipynb,markdown,1,"### Structured and Unstructured data

**Structured Data**

Structured data is the data which conforms to a data model, has a well define structure, follows a consistent order and can be easily accessed and used by a person or a computer program.

Structured data is usually stored in well-defined schemas such as Databases. It is generally tabular with column and rows that clearly define its attributes.

SQL (Structured Query language) is often used to manage structured data stored in databases.

Characteristics of Structured Data:
* Data conforms to a data model and has easily identifiable structure
* Data is stored in the form of rows and columns (example : Database)
* Data is well organised"
lab,feature_engineering.ipynb,markdown,2," so, Definition, Format and Meaning of data is explicitly known
* Data resides in fixed fields within a record or file
* Similar entities are grouped together to form relations or classes
* Entities in the same group have same attributes
* Easy to access and query, So data can be easily used by other programs
* Data elements are addressable, so efficient to analyse and process

**Unstructured Data**

Unstructured data is everywhere. In fact, most individuals and organizations conduct their lives around unstructured data. Just as with structured data, unstructured data is either machine generated or human generated.

Here are some examples of machine-generated unstructured data:
* Satellite i"
lab,feature_engineering.ipynb,markdown,3,"mages: This includes weather data or the data that the government captures in its satellite surveillance imagery. Just think about Google Earth, and you get the picture.
* Scientific data: This includes seismic imagery, atmospheric data, and high energy physics.
* Photographs and video: This includes security, surveillance, and traffic video.
* Radar or sonar data: This includes vehicular, meteorological, and oceanographic seismic profiles.

The following list shows a few examples of human-generated unstructured data:
* Text internal to your company: Think of all the text within documents, logs, survey results, and e-mails. Enterprise information actually represents a large percent of the te"
lab,feature_engineering.ipynb,markdown,4,"xt information in the world today.
* Social media data: This data is generated from the social media platforms such as YouTube, Facebook, Twitter, LinkedIn, and Flickr.
* Mobile data: This includes data such as text messages and location information.
* website content: This comes from any site delivering unstructured content, like YouTube, Flickr, or Instagram.

### Why does unstructured data matter?

**Because 80% of the data in companies is unstructured**, organizations need to understand the types of unstructured data they are accumulating and the best ways to process and store this data for business advantages. Without data management strategies and guidance in these areas, companies run"
lab,feature_engineering.ipynb,markdown,5," the risks of not capitalizing on unstructured data, failing to keep up with competitors, or storing more unstructured data than they really need, thereby running up data center costs.

In a majority of cases, unstructured data is ultimately related back to the company's structured data records. As an example, every x-ray or MRI image for a patient is related back to the patient's record in the hospital's record system. The patient record in the record system is enriched with unstructured data that is linked to it, and the doctor gets a more complete picture of the patient.

This is the value of unstructured data: It enriches corporate data and enables leaders to work smarter.

### Ways you "
lab,feature_engineering.ipynb,markdown,6,"can deal with unstructured data:
[source](https://www.ironmountain.com/blogs/2020/4-ways-to-deal-with-unstructured-data)

1. Throw It Away

The reality is that much of the data organizations collect isn’t very interesting or useful, but it still takes up a lot of storage space. Devices such as smart cameras and machine sensors create huge amounts of data, little of which is needed if everything is operating normally. Rather than storing all that data, a better solution is evaluating it before it hits the network and discarding what isn’t needed. Edge computing, a type of distributed processing that makes decisions close to where data is gathered, is a promising way to do just that.

Edge AI "
lab,feature_engineering.ipynb,markdown,7,"is a special category of artificial intelligence that’s specifically intended to make decisions requiring immediate attention, such as controlling the brakes of a car or determining the likelihood that a machine is about to fail, according to Forbes. Edge AI can also be used to scour data streams and quickly identify what:
* Can be discarded.
* Requires immediate attention.
* Should be stored for analysis.

There are high hopes for edge computing; many researchers expect the market to grow more than 30% annually for the next several years, according to Allied Market Research.

2. Deduplicate It

Have you ever been on the distribution list of a mass email that included a 15-megabyte PowerPoin"
lab,feature_engineering.ipynb,markdown,8,"t attachment? Organizations generate an enormous amount of duplicate data, so much so that a 2013 IDC study estimated that companies spend $44 billion annually to store copies of data they already have. This situation has likely only grown since then.

High-speed, in-line deduplication can flag duplicate records and either hold them for review or delete them automatically. While savings vary when using this method, it led to more than a 10 times reduction in storage space for two-thirds of the companies in an Enterprise Strategy Group survey shared by TechTarget.

3. Tier It

If your organization treats all data the same, it’s flushing money down the drain. Only a tiny percentage of data is "
lab,feature_engineering.ipynb,markdown,9,"typically mission-critical enough that it needs to be instantly available on expensive storage media. Most data can be relegated to spinning disks or tape. Tiered storage automatically assigns data to the most appropriate storage medium based upon policies, which often results in significant savings.

Horison Information Strategies estimates that between 63% and 85% of a typical organization’s data can be moved to secondary or long-term storage without impacting operations. If your organization isn’t moving infrequently accessed data from disk to tape or from cloud to tape, you should take a fresh look at this low-cost archival option.

Tape is the most cost-effective storage medium and its "
lab,feature_engineering.ipynb,markdown,10,"retrieval times are approaching that of disk, thanks to recent advances in technology such as redundant arrays of independent tape. What’s more, tape is stored offline, making it nearly impervious to malware attacks. Additionally, cloud services can archive data to tape automatically, which can help you save big while still retaining ready access to your data.

4. Structure It

Machine learning algorithms are great at finding patterns, including those in unstructured data sources such as text documents and images. By repeatedly scanning similar documents with some human oversight, machines can quickly figure out that a sequence of digits is more likely to be a Social Security number than a p"
lab,feature_engineering.ipynb,markdown,11,"hone number or the identities of people in a photo or video. This semi-structured data can then be loaded into databases for analytical processing.

When deciding how to deal with unstructured data, look to cloud and tape for ideas. Deduplicating data helps minimize how much data is stored and AI helps with information processing and analysis, but cloud to tape storage ensures your data is protected in the most secure, accessible and cost-effective manner possible."
lab,feature_engineering.ipynb,markdown,1,"### Dirty Data

Dirty data, also known as rogue data, are inaccurate, incomplete or inconsistent data, especially in a computer system or database.

Dirty data can contain such mistakes as spelling or punctuation errors, incorrect data associated with a field, incomplete or outdated data, or even data that has been duplicated in the database. They can be cleaned through a process known as data cleansing.

### Data cleansing

Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, mo"
lab,feature_engineering.ipynb,markdown,2,"difying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.

After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data. 

### Requisites of qual"
lab,feature_engineering.ipynb,markdown,3,"ity data

High-quality data needs to pass a set of quality criteria. Those include:
* **Validity**: The degree to which the measures conform to defined business rules or constraints (see also Validity (statistics)). When modern database technology is used to design data-capture systems, validity is fairly easy to ensure: invalid data arises mainly in legacy contexts (where constraints were not implemented in software) or where inappropriate data-capture technology was used (e.g., spreadsheets, where it is very hard to limit what a user chooses to enter into a cell, if cell validation is not used). Data constraints fall into the following categories:
  + Data-Type Constraints – e.g., values i"
lab,feature_engineering.ipynb,markdown,4,"n a particular column must be of a particular data type, e.g., Boolean, numeric (integer or real), date, etc.
  + Range Constraints: typically, numbers or dates should fall within a certain range. That is, they have minimum and/or maximum permissible values.
  + Mandatory Constraints: Certain columns cannot be empty.
  +  Unique Constraints: A field, or a combination of fields, must be unique across a dataset. For example, no two persons can have the same social security number.
  + Set-Membership constraints: The values for a column come from a set of discrete values or codes. For example, a person's gender may be Female, Male or Unknown (not recorded).
  + Foreign-key constraints: This is "
lab,feature_engineering.ipynb,markdown,5,"the more general case of set membership. The set of values in a column is defined in a column of another table that contains unique values. For example, in a US taxpayer database, the ""state"" column is required to belong to one of the US's defined states or territories: the set of permissible states/territories is recorded in a separate State table. The term foreign key is borrowed from relational database terminology.
  + Regular expression patterns: Occasionally, text fields will have to be validated this way. For example, phone numbers may be required to have the pattern (999) 999-9999.
  + Cross-field validation: Certain conditions that utilize multiple fields must hold. For example, in "
lab,feature_engineering.ipynb,markdown,6,"laboratory medicine, the sum of the components of the differential white blood cell count must be equal to 100 (since they are all percentages). In a hospital database, a patient's date of discharge from the hospital cannot be earlier than the date of admission.
* **Accuracy**: The degree of conformity of a measure to a standard or a true value - see also Accuracy and precision. Accuracy is very hard to achieve through data-cleansing in the general case because it requires accessing an external source of data that contains the true value: such ""gold standard"" data is often unavailable. Accuracy has been achieved in some cleansing contexts, notably customer contact data, by using external dat"
lab,feature_engineering.ipynb,markdown,7,"abases that match up zip codes to geographical locations (city and state) and also help verify that street addresses within these zip codes actually exist.
* **Completeness**: The degree to which all required measures are known. Incompleteness is almost impossible to fix with data cleansing methodology: one cannot infer facts that were not captured when the data in question was initially recorded. (In some contexts, e.g., interview data, it may be possible to fix incompleteness by going back to the original source of data, i.e. re-interviewing the subject, but even this does not guarantee success because of problems of recall - e.g., in an interview to gather data on food consumption, no one"
lab,feature_engineering.ipynb,markdown,8," is likely to remember exactly what one ate six months ago. In the case of systems that insist certain columns should not be empty, one may work around the problem by designating a value that indicates ""unknown"" or ""missing"", but the supplying of default values does not imply that the data has been made complete.)
* **Consistency**: The degree to which a set of measures are equivalent in across systems (see also Consistency). Inconsistency occurs when two data items in the data set contradict each other: e.g., a customer is recorded in two different systems as having two different current addresses, and only one of them can be correct. Fixing inconsistency is not always possible: it requires"
lab,feature_engineering.ipynb,markdown,9," a variety of strategies - e.g., deciding which data were recorded more recently, which data source is likely to be most reliable (the latter knowledge may be specific to a given organization), or simply trying to find the truth by testing both data items (e.g., calling up the customer).
* **Uniformity**: The degree to which a set data measures are specified using the same units of measure in all systems ( see also Unit of measure). In datasets pooled from different locales, weight may be recorded either in pounds or kilos and must be converted to a single measure using an arithmetic transformation.

The term **integrity** encompasses accuracy, consistency and some aspects of validation  but"
lab,feature_engineering.ipynb,markdown,10," is rarely used by itself in data-cleansing contexts because it is insufficiently specific. (For example, ""referential integrity"" is a term used to refer to the enforcement of foreign-key constraints above.) 


### Data Integrity

Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle and is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context – even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation "
lab,feature_engineering.ipynb,markdown,11,"is a pre-requisite for data integrity. Data integrity is the opposite of data corruption. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended (such as a database correctly rejecting mutually exclusive possibilities). Moreover, upon later retrieval, ensure the data is the same as when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties."
lab,feature_engineering.ipynb,markdown,1,"<div style=""background: #000;
            color: #FFF;
            margin: 0px;
            padding: 10px 0px 20px 0px;
            text-align: center; 
                "">
    <h1>Code Along</h1>
    <h3>Cleaning up dirty data</h3>
</div>"
lab,feature_engineering.ipynb,markdown,1,"Some new things we'll cover in this code along:

[pandas.DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) - applies a function to rows of a col  
[pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) - drops rows or columns  
[pandas.DataFrame.drop_duplicates](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) - drops duplicates  
[pandas.DataFrame.duplicated](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) - returns True if duplicate  
[pandas.DataFrame.fillna](https://pandas.pydata.org/pa"
lab,feature_engineering.ipynb,markdown,2,"ndas-docs/stable/reference/api/pandas.DataFrame.fillna.html) - fills NaN values  
[pandas.merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) - merges two dfs"
lab,feature_engineering.ipynb,code,1,"import numpy as np 
import pandas as pd 
import datetime"
lab,feature_engineering.ipynb,markdown,1,Let's open the `ted_talks.csv` file and generate a dataframe. Then we'll check the top of the `df` to see what we have.
lab,feature_engineering.ipynb,code,1,"df = pd.read_csv(""ted_talks.csv"")
df.head()"
lab,feature_engineering.ipynb,markdown,1,Checking the shape of the `df`
lab,feature_engineering.ipynb,code,1,df.shape
lab,feature_engineering.ipynb,markdown,1,Checking info to see other information (like how much RAM our dataset uses and how many non-null entries we have)
lab,feature_engineering.ipynb,code,1,df.info()
lab,feature_engineering.ipynb,markdown,1,We see we have 2555 rows but 2531 non-null rows for 'main_speaker'. So let's check that out.
lab,feature_engineering.ipynb,code,1,"df[df['main_speaker'].isna()][[""description"", ""main_speaker"",""name""]]"
lab,feature_engineering.ipynb,markdown,1,"Before we fix the ""main_speaker"" column, let's shrink our dataset by removing duplicates. 
First we need to see how many duplicates we have:"
lab,feature_engineering.ipynb,code,1,df['url'].duplicated().sort_values(ascending=False)[:6]
lab,feature_engineering.ipynb,code,1,df[(df['url'].duplicated() == False)]
lab,feature_engineering.ipynb,code,1,df[df['url'].duplicated()]
lab,feature_engineering.ipynb,markdown,1,"Then we'll drop the duplicates using the `df.drop_duplicates` method. 

Remember, the keyword `inplace` means it will mutate the original `df`."
lab,feature_engineering.ipynb,code,1,help(df.drop_duplicates)
lab,feature_engineering.ipynb,code,1,"df.drop_duplicates('url', inplace=True)"
lab,feature_engineering.ipynb,markdown,1,"If we look at `published_date` and `film_date`, they're formatted as unix timestamps. We might need a human-readable format.

[more on unix timestamps](https://en.wikipedia.org/wiki/Unix_time)"
lab,feature_engineering.ipynb,code,1,"df[[""published_date"", ""film_date""]]"
lab,feature_engineering.ipynb,markdown,1,Let's see how we can turn a unix timestamp into a human-readable format
lab,feature_engineering.ipynb,code,1,"ts = 1151367060

s = datetime.datetime.fromtimestamp(ts)

s.strftime(""%m/%d/%Y, %H:%M:%S"")"
lab,feature_engineering.ipynb,markdown,1,"Putting that together, we can create a function that does this for any timestamp"
lab,feature_engineering.ipynb,code,1,"def transform_date(timestamp):
    _date = datetime.datetime.fromtimestamp(timestamp)
    return _date.strftime(""%m/%d/%Y, %H:%M:%S"")"
lab,feature_engineering.ipynb,markdown,1,And then we can test it with a random timestamp
lab,feature_engineering.ipynb,code,1,transform_date(1151367060)
lab,feature_engineering.ipynb,markdown,1,We can use the `df.apply` method to apply that function to each row of a specific column.
lab,feature_engineering.ipynb,code,1,df['film_date'].apply(transform_date)
lab,feature_engineering.ipynb,markdown,1,"Seeing that it works, we can now use that `transform_date` method along with `apply` to create two new columns of human-readable timestamps"
lab,feature_engineering.ipynb,code,1,"df['film_date_hr'] = df['film_date'].apply(transform_date)
df['published_date_hr'] = df['published_date'].apply(transform_date)"
lab,feature_engineering.ipynb,markdown,1,"If we wanted to change the ordering of the columns, we can use `df.columns.tolist()` to list our columns and then `df.reindex()` to reindex the columns by passing in a list of columns in the order we want"
lab,feature_engineering.ipynb,code,1,df.columns.tolist()
lab,feature_engineering.ipynb,code,1,"df = df.reindex(['comments',
 'description',
 'duration',
 'event',
 'film_date',
 'film_date_hr',
 'languages',
 'main_speaker',
 'name',
 'num_speaker',
 'published_date',
 'published_date_hr',
 'ratings',
 'related_talks',
 'speaker_occupation',
 'tags',
 'title',
 'url',
 'views',
],axis=1)"
lab,feature_engineering.ipynb,markdown,1,We can see that works by checking the head of the `df`
lab,feature_engineering.ipynb,code,1,df.head()
lab,feature_engineering.ipynb,markdown,1,"Next, we'll fix the durations, which are in seconds, by converting them to mins and rounding the result to 2 places after the decimal"
lab,feature_engineering.ipynb,code,1,"df['duration'] = round( df['duration'] / 60, 2)"
lab,feature_engineering.ipynb,markdown,1,We'll fix incorrect values for `languages` and `num_speakers` by replacing erroneous values with `1` since we know there's at least one language translated (english) and there's at least one speaker.
lab,feature_engineering.ipynb,code,1,df[df['languages'] == 0]
lab,feature_engineering.ipynb,code,1,"df['languages'].replace(0, 1, inplace=True)
df"
lab,feature_engineering.ipynb,code,1,df[df['languages'] == 0]
lab,feature_engineering.ipynb,code,1,df[(df['num_speaker'] <= 0)]
lab,feature_engineering.ipynb,code,1,"df[""num_speaker""].replace(0, 1, inplace=True)
df[""num_speaker""].replace(-1, 1, inplace=True)"
lab,feature_engineering.ipynb,markdown,1,"Finally, we're able to get back to the first issue of `main_speaker` having blanks. We can use `df['main_speaker'].isna()` to check the rows of `main_speaker` that are empty"
lab,feature_engineering.ipynb,code,1,df['main_speaker'].isna()
lab,feature_engineering.ipynb,markdown,1,"If we look at the `description`, `main_speaker` and `name` columns for all of the blank rows, we see some useful information"
lab,feature_engineering.ipynb,code,1,"df[df['main_speaker'].isna()][[""description"", ""main_speaker"",""name""]]"
lab,feature_engineering.ipynb,markdown,1,"Without that useful information, we can always just replace empty values with a default value like so:"
lab,feature_engineering.ipynb,code,1,"df.fillna(""unknown"", inplace=True)"
lab,feature_engineering.ipynb,code,1,df
lab,feature_engineering.ipynb,code,1,df[df['main_speaker'] == 'unknown'].shape
lab,feature_engineering.ipynb,markdown,1,"But here, `name` also contains the speaker's name. So we can rip it and then adjust `main_speaker` and `name` accordingly"
lab,feature_engineering.ipynb,code,1,df['name'].head()
lab,feature_engineering.ipynb,code,1,"df['main_speaker'] = df['name'].str.split(':').str[0]
df['main_speaker']"
lab,feature_engineering.ipynb,code,1,"df['name'] = df['name'].str.split(':').str[1]
df['name']"
lab,feature_engineering.ipynb,code,1,df
lab,feature_engineering.ipynb,markdown,1,"Finally, let's explore how we can apply one-hot-encoding and dummy encoding on our sample csv file (which is a sample of )"
lab,feature_engineering.ipynb,code,1,from sklearn.preprocessing import OneHotEncoder
lab,feature_engineering.ipynb,code,1,"df = pd.read_csv(""sample.csv"")

cat_features = [""type""]                              
num_features = [""amount"", ""oldbalanceOrg"", ""newbalanceOrig"", ""oldbalanceDest"", ""newbalanceDest""]    

X_cat = df[cat_features]
X_num = df[num_features]

X_cat.head()"
lab,feature_engineering.ipynb,code,1,X_num.head()
lab,feature_engineering.ipynb,markdown,1,Full one-hot encoding on categorical dataset
lab,feature_engineering.ipynb,code,1,"ohe = OneHotEncoder()
X_cat_full = ohe.fit_transform(X_cat).toarray()

X_cat_full"
lab,feature_engineering.ipynb,code,1,"# we can also get our new column names
ohe.get_feature_names_out(['type'])"
lab,feature_engineering.ipynb,markdown,1,"Notice that this a matrix of 1's and 0's that correspond to our categories in `type`. Next, we need to re-introduce these values back into our dataframe."
lab,feature_engineering.ipynb,code,1,"cat_names = ohe.get_feature_names_out(['type'])

encoded_df = pd.DataFrame(X_cat_full, columns=cat_names, index=df.index)

encoded_df.head()"
lab,feature_engineering.ipynb,markdown,1,"Next, we concatenate this dataframe back to our original dataframe."
lab,feature_engineering.ipynb,code,1,"full_df = pd.concat([X_num, encoded_df], axis=1)

full_df"
lab,feature_engineering.ipynb,markdown,1,"To implement dummy-encoding, we simply nee dinclude `drop='first'`"
lab,feature_engineering.ipynb,code,1,"ohe = OneHotEncoder(drop='first')
X_cat_full = ohe.fit_transform(X_cat).toarray()

X_cat_full"
lab,knn.ipynb,markdown,1,"# Lab: k-Nearest Neighbors (kNN) Classification

In this lab, you will:

- Manually compute L1 (Manhattan) and L2 (Euclidean) distances.
- Build and explore a toy dataset using kNN.
- Apply kNN to the Iris dataset with hyperparameter tuning.
- Learn the importance of feature scaling using MinMaxScaler.
- Investigate the curse of dimensionality by analyzing pairwise distances in synthetic data.
- Address class imbalance with SMOTE."
lab,knn.ipynb,markdown,1,"## Manual Distance Computation

In this section, you will implement functions to compute the L1 (Manhattan) and L2 (Euclidean) distances between two points. 

These distance metrics are fundamental to understanding how kNN makes predictions by comparing the similarity between data points."
lab,knn.ipynb,code,1,"import math

p1 = [1, 0]
p2 = [4, 5]

# function to calculate the Manhattan (L1) distance between two points
def l1_distance(point1, point2):
    distance = 0
    for i in range(len(point1)):
        a_point = point1[i]
        b_point = point2[i]
        distance += abs(a_point - b_point)
    return distance

print(""L1 distance:"", l1_distance(p1, p2))"
lab,knn.ipynb,code,1,"# function to calculate the Euclidean (L2) distance between two points
def l2_distance(point1, point2):
    sum_sq = 0
    for i in range(len(point1)):
        a_point = point1[i]
        b_point = point2[i]
        sum_sq += (a_point - b_point) ** 2

    return (sum_sq) ** (1/2)

print(""L2 distance:"", l2_distance(p1, p2))"
lab,knn.ipynb,code,1,"# expand this to the third dimension
p1 = [1, 0, 3]
p2 = [4, 5, 2]

print(""L1 distance:"", l1_distance(p1, p2)) 
print(""L2 distance:"", l2_distance(p1, p2))"
lab,knn.ipynb,markdown,1,"## kNN on a Toy Dataset

In this section, you'll create a small toy dataset with two features (e.g., `kibble_grams` and `noise_dB`) and three classes (e.g., dog, cat, hamster). Using scikit-learn's `KNeighborsClassifier`, you'll train a kNN model with `n_neighbors=3` on this dataset and evaluate its training performance."
lab,knn.ipynb,code,1,"import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score

from sklearn.model_selection import train_test_split, GridSearchCV

# create a toy dataset with 15 samples (5 per class)
data = {
    ""kibble_grams"": [200, 250, 115, 300, 50, 138, 128, 142, 280, 317, 282, 270, 69, 32, 31],
    ""noise_dB"":     [ 40,  60,  45,  80, 75,  38,  49,  42,  76,  88,  80,  73, 77, 75, 77],
    ""animal"": [
        ""cat"", ""dog"", ""cat"", ""dog"", ""hamster"",
        ""cat"", ""cat"", ""cat"",
        ""dog"", ""dog"", ""dog"", ""dog"",
        ""hamster"", ""hamster"", ""hamster""
    ]
}

# convert to DataFrame
d"
lab,knn.ipynb,code,2,"f = pd.DataFrame(data)
df.head()"
lab,knn.ipynb,code,1,"# TODO: Create predictor & target
X = df.drop(columns=[""animal""])
y = df[""animal""]

# TODO: Split into train/test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
lab,knn.ipynb,code,1,"# initialize and train the kNN classifier with k=3, & a distance metric of 'cityblock'
# DOCS: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
knn_toy = KNeighborsClassifier(n_neighbors=3, metric='cityblock')
knn_toy.fit(X_train, y_train)"
lab,knn.ipynb,code,1,"# predict the class for a new sample
predicted_class = knn_toy.predict(X_test)
print(""Predicted class:"", predicted_class)
print(""Actual class:"", y_test.tolist())"
lab,knn.ipynb,markdown,1,"## kNN on the Maternal Risk Dataset & Hyperparameter Tuning

In this section, we'll apply the kNN algorithm to the maternal risk dataset. You will:

- Load the maternal risk dataset using pandas.
- Perform EDA.
- Split the dataset into training and testing sets.
- Iteratively tune the hyperparameter k (number of neighbors) to determine the best value based on test accuracy."
lab,knn.ipynb,code,1,"from sklearn import datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# TODO: load the ""maternal"" dataset
# docs: https://archive.ics.uci.edu/dataset/863/maternal+health+risk
maternal_risk = pd.read_csv(""maternal.csv"")

maternal_risk.head()"
lab,knn.ipynb,code,1,"# TODO: perform EDA
..."
lab,knn.ipynb,code,1,"# TODO: select your predictors & target
X = maternal_risk.drop(columns=[""RiskLevel""])

# TODO: the dataset into training and testing sets
y = maternal_risk[""RiskLevel""]

# Split the dataset into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
lab,knn.ipynb,code,1,"# create a basic knn model with 3 neighbors
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Evaluate the classifier on the scaled test data
yhat = knn.predict(X_test)
accuracy = accuracy_score(y_test, yhat)

print(""Testing accuracy on non-scaled data:"", accuracy)"
lab,knn.ipynb,code,1,"# define range of k values to test (1 to 30)
k_values = range(1, 30)  

train_accuracies = []
test_accuracies = []

# evaluate kNN for each k in the specified range
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    train_pred = knn.predict(X_train)
    test_pred = knn.predict(X_test)
    
    train_accuracies.append(1 - accuracy_score(y_train, train_pred))
    test_accuracies.append(1 - accuracy_score(y_test, test_pred))

print(""Average training accuracy"", sum(train_accuracies) / len(train_accuracies))
print(""Average testing accuracy"", sum(test_accuracies) / len(test_accuracies))"
lab,knn.ipynb,code,1,"# Plot the performance for each k
plt.figure(figsize=(8, 5))
plt.plot(k_values, train_accuracies, marker='o', label=""Training Error Rate"")
plt.plot(k_values, test_accuracies, marker='s', label=""Testing Error Rate"")

plt.xlabel(""Number of Neighbors (k)"")
plt.ylabel(""Error Rate"")
plt.title(""kNN Hyperparameter Tuning on Maternal Dataset"")
plt.legend()
plt.show()

# Determine best k based on highest test accuracy
best_k = k_values[test_accuracies.index(min(test_accuracies))]
print(""Best k for test accuracy:"", best_k)"
lab,knn.ipynb,markdown,1,"## Data Scaling with MinMaxScaler

Scaling features is crucial for distance-based algorithms like kNN. In this section, you will:

- Scale the maternal dataset using scikit-learn's `MinMaxScaler` so that all features lie in the range [0, 1].
- Retrain the kNN model on the scaled data and observe any improvements in performance.

On the job, ensuring that your data is appropriately scaled can significantly improve model performance."
lab,knn.ipynb,code,1,"from sklearn.preprocessing import MinMaxScaler

# Initialize the MinMaxScaler to scale data to the range [0, 1]
scaler = MinMaxScaler(feature_range=(0, 1))

# Fit on the training data, then transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled training data to a DataFrame for a quick visualization
df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
print(""First five rows of the scaled training data:"")
print(df_scaled.head())"
lab,knn.ipynb,code,1,"# Retrain kNN on the scaled data using k=3 for illustration
knn_scaled = KNeighborsClassifier(n_neighbors=3)
knn_scaled.fit(X_train_scaled, y_train)

# Evaluate the classifier on the scaled test data
test_pred_scaled = knn_scaled.predict(X_test_scaled)
scaled_accuracy = accuracy_score(y_test, test_pred_scaled)

print(""Testing accuracy on scaled data:"", scaled_accuracy)"
lab,knn.ipynb,code,1,"# define range of k values to test (1 to 30)
k_values = range(1, 30)  

train_accuracies_scaled = []
test_accuracies_scaled = []

# evaluate kNN for each k in the specified range
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)

    train_pred = knn.predict(X_train_scaled)
    test_pred = knn.predict(X_test_scaled)
    
    train_accuracies_scaled.append(1 - accuracy_score(y_train, train_pred))
    test_accuracies_scaled.append(1 - accuracy_score(y_test, test_pred))

print(""Average training accuracy"", sum(train_accuracies_scaled) / len(train_accuracies_scaled))
print(""Average testing accuracy"", sum(test_accuracies_scaled) / len(test_accura"
lab,knn.ipynb,code,2,cies_scaled))
lab,knn.ipynb,code,1,"# Plot the performance for each k
plt.figure(figsize=(8, 5))
plt.plot(k_values, train_accuracies_scaled, marker='o', label=""Training Error Rate"")
plt.plot(k_values, test_accuracies_scaled, marker='s', label=""Testing Error Rate"")

plt.xlabel(""Number of Neighbors (k)"")
plt.ylabel(""Error Rate"")
plt.title(""kNN Hyperparameter Tuning on Maternal Dataset"")
plt.legend()
plt.show()

# Determine best k based on highest test accuracy
best_k = k_values[test_accuracies.index(min(test_accuracies))]
print(""Best k for test accuracy:"", best_k)"
lab,knn.ipynb,code,1,"# TODO: Implement full grid-search on the knn model to find best hyperparams
params = {
    ...
}

knn = KNeighborsClassifier()

# TODO: set up GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(...)

# TODO: fit this model on your training data
..."
lab,knn.ipynb,markdown,1,"## Exploring the Curse of Dimensionality

As the number of features (dimensions) increases, the distance between data points tends to converge, making it difficult for algorithms like kNN to distinguish between nearby and distant points.

In this section, you will explore the concept of the curse of dimensionality by computing the average pairwise distances in synthetic data as the number of dimensions increases."
lab,knn.ipynb,code,1,"import numpy as np

def average_pairwise_distance(data):
    n = len(data)
    distances = []
    for i in range(n):
        for j in range(i+1, n):
            diff = np.array(data[i]) - np.array(data[j])
            distances.append(np.sqrt(np.sum(diff**2)))
    return np.mean(distances)

num_points = 50
np.random.seed(42)

print(""Average Pairwise Distances for Increasing Dimensions:"")

for dim in range(1, 11):  # From 1 to 10 dimensions
    data = np.random.rand(num_points, dim)
    avg_dist = average_pairwise_distance(data)
    print(""Dimension {}: Average distance = {:.3f}"".format(dim, avg_dist))"
lab,knn.ipynb,markdown,1,"## Handling Class Imbalance with SMOTE

Class imbalance can negatively impact the performance of classification algorithms. In this section, you will:

- Create a synthetic imbalanced dataset where one class is underrepresented.
- Train a kNN classifier on the imbalanced data to obtain a baseline accuracy.
- Apply SMOTE (Synthetic Minority Over-sampling Technique) to rebalance the training set.
- Retrain the kNN model and compare the performance after applying SMOTE.

This process reflects real-world scenarios where class imbalance is common, such as fraud detection or medical diagnosis."
lab,knn.ipynb,code,1,"# Run me!
!pip install imblearn"
lab,knn.ipynb,code,1,"import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# import the imbalanced sample dataset that you'll be working with for your TLAB
sample = pd.read_csv(""sample.csv"")

sample.head()"
lab,knn.ipynb,code,1,"# TODO: explore your ratio of fraud vs non-fraudulent cases (visually or otherwise)
..."
lab,knn.ipynb,code,1,"# TODO: split the data into features and labels, select 2 numerical columns
X = sample[[""oldbalanceOrg"", ""amount""]]
y = sample[""isFraud""]

# split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# train kNN on the imbalanced data
knn_imb = KNeighborsClassifier(n_neighbors=3)
knn_imb.fit(X_train, y_train)

yhat = knn_imb.predict(X_test)
baseline_acc = accuracy_score(y_test, yhat)

print(""Baseline testing accuracy (imbalanced) (WOW AMAZING!):"", baseline_acc)"
lab,knn.ipynb,code,1,"# but let's see the entire picture...
print(precision_score(y_test, yhat))"
lab,knn.ipynb,code,1,"# Apply SMOTE to rebalance the training set (number of neighbors needs to be less than number of minority class samples)
smote = SMOTE(k_neighbors=2, random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(""Class distribution after SMOTE:"")
print(y_train_smote.value_counts())"
lab,knn.ipynb,code,1,"# Retrain kNN on the balanced data
knn_smote = KNeighborsClassifier(n_neighbors=3)
knn_smote.fit(X_train_smote, y_train_smote)

yhat_pred = knn_smote.predict(X_test)
smote_acc = accuracy_score(y_test, yhat_pred)

print(""Testing accuracy after applying SMOTE:"", smote_acc)"
lab,knn.ipynb,code,1,"# but let's see the entire picture...
print(precision_score(y_test, yhat_pred))"
lab,knn.ipynb,markdown,1,"## Conclusion

In this lab, you explored the k-Nearest Neighbors algorithm from multiple angles:

- You started by manually calculating distance metrics.
- You built a toy dataset and implemented a kNN classifier on it.
- You applied kNN on the Iris dataset and tuned the hyperparameter k.
- You learned the importance of feature scaling on distance-based models.
- You investigated the curse of dimensionality and addressed class imbalance with SMOTE.

These fundamental techniques are critical in real-world scenarios where data preparation, model tuning, and handling imbalances can have a significant impact on the performance of machine learning models.

Great job working through these exercise"
lab,knn.ipynb,markdown,2,s!
lab,naive_bayes.ipynb,markdown,1,"# Lab: Naive Bayes Classifier and Bayesian Statistics

By the end of this lab you will be able to:

- Explain and apply Bayes Theorem within a classification context.
- Implement a Naive Bayes Classifier using basic Python (without sklearn) to analyze a spam text example.
- Apply Laplace smoothing to adjust probability estimates when encountering zero counts.
- Implement a Gaussian Naive Bayes Classifier using sklearn on the iris dataset, including a simple hyperparameter search."
lab,naive_bayes.ipynb,markdown,1,"## Review Bayes Theorem & Naive Bayes Classifier (Plug and Play)

In this exercise, we calculate the posterior probability that a text is spam, given the presence of a specific word (""USPS""). 

We start with a dataset of `spam` and `non-spam` texts, then use Bayes' Theorem to compute the desired probability. We first calculate our prior probability of spam and normal text messages by simply calculating the ratio of text messages received. 

This is the core of the Naive Bayes classifier."
lab,naive_bayes.ipynb,code,1,"# Spam text data
spam_texts = [
    ""Hello, your USPS package was not able to be delivered. Click here!"",
    ""USPS package not able to be delivered!"",
    ""This is the USPS. Give us $200""
]

# Non-spam text data
normal_texts = [
    ""It’s Farukh. Quick tell me what the central limit theorem is."",
    ""I'm stopping by the USPS, do you want any stamps?"",
]

# use ratio of spam to not spam texts to calculate initial probability
spam_probability = len(spam_texts) / (len(spam_texts) + len(normal_texts))
not_spam_probability = len(normal_texts) / (len(spam_texts) + len(normal_texts))

print(f""Initial probability of spam {spam_probability}"")
print(f""Initial probability of not spam {not_spam_probab"
lab,naive_bayes.ipynb,code,2,"ility}"")"
lab,naive_bayes.ipynb,markdown,1,"By simply iterating through each word that **shows up in every single text**, we can count up the frequency for each word that shows up in our entire [corpus](https://en.wikipedia.org/wiki/Text_corpus).

We save this data in our dictionary, where each word is a unique key, and the associated value is the number of times this word shows up in a specific category of text."
lab,naive_bayes.ipynb,code,1,"import re

# Prepare dictionaries to count up the frequency of words in both categories
spam_frequencies = {}
nospam_frequencies = {}

# count up frequencies for spam texts
for text in spam_texts:
    # remove all punctuation marks (can punctuation be used to classify spam?)
    no_punctuation = re.sub(r""[.?!,']"", """", text)

    # lower all characters and split text and count up frequencies
    words = no_punctuation.lower().split()
    for w in words:
        if w not in spam_frequencies:
            spam_frequencies[w] = 1
        else:
            spam_frequencies[w] += 1

# add up all frequencies to get number of words in spam
total_spam = sum(spam_frequencies.values())

print(f""{total_s"
lab,naive_bayes.ipynb,code,2,"pam} words in spam"")"
lab,naive_bayes.ipynb,code,1,"# count up frequencies for normal texts
for text in normal_texts:
    # remove all punctuation marks (can punctuation be used to classify spam?)
    no_punctuation = re.sub(r""[.?!,']"", """", text)

    # split text and count up frequencies
    words = no_punctuation.lower().split()
    for w in words:
        if w not in nospam_frequencies:
            nospam_frequencies[w] = 1
        else:
            nospam_frequencies[w] += 1

total_not_spam = sum(nospam_frequencies.values())

print(f""{total_not_spam} words in non-spam"")"
lab,naive_bayes.ipynb,code,1,"total_words = total_not_spam + total_spam

print(f""{total_words} words across both classes"")"
lab,naive_bayes.ipynb,markdown,1,"However, this is not a sufficient spam classifier until we can calculate the frequencies of words that **don't** show up in another category.

Here, we iterate through both dictionaries. If we find a word in one dictionary that does not exist in another, we create a key value pairing where the word points to a value of `0` (aka `0%` likelihood that this word will show up in a separate category)."
lab,naive_bayes.ipynb,code,1,"# we also need to include words that dont show up in both dictionaries and assign them a probability of 0
spam_words = list(spam_frequencies.keys())
normal_words = list(nospam_frequencies.keys())

for w in spam_words:
    if w not in nospam_frequencies:
        nospam_frequencies[w] = 0

for w in normal_words:
    if w not in spam_frequencies:
        spam_frequencies[w] = 0

print(""spam probabilities"", spam_frequencies)
print(""normal word probabilities"",nospam_frequencies)"
lab,naive_bayes.ipynb,markdown,1,We then calculate the probability of a word showing up in a category of text (`spam` vs `non-spam`) by dividing each frequency by the total number of texts for that category. This normalizes our data and gives us a conditional probability that a word shows up in either a `spam` or `non-spam` text.
lab,naive_bayes.ipynb,code,1,"spam_prob = {}
nospam_prob = {}

# normalize both frequency tables (divide by number of words that show up in both categories
for w in spam_frequencies:
    spam_prob[w] = round(spam_frequencies[w] / total_spam, 2)

for w in nospam_frequencies:
    nospam_prob[w] = round(nospam_frequencies[w] / total_not_spam, 2)

print(""spam frequency"", spam_prob)
print(""normal word frequency"", nospam_prob)"
lab,naive_bayes.ipynb,markdown,1,"Now that we finally have a dictionary that represents our frequency table, we are able to calculate the probability that a new text is either spam or not spam! 

Let's start with the simple text, `USPS`. Since this is just one word, we do not need to apply any sort of text splitting or punctuation removal. However, we still need to make this lowercase for this code to work, and then we can calculate our likelihood.

We can calculate the likelihood that this text message is a spam text by multiplying our prior probability of spam (0.6) by the probability of the word `USPS` showing up in a spam text (0.12).

$$ P(Spam|""USPS"") = P(Spam)P(""USPS""|Spam) $$

As we established in the slides, it is n"
lab,naive_bayes.ipynb,markdown,2,"ot necessary to divide this value by the probability of `USPS` throughout all texts, as this is a constant amongst all classes."
lab,naive_bayes.ipynb,code,1,"new_text1 = ""USPS""

new_text1 = new_text1.lower()

spam_likelihood = spam_probability * spam_prob[new_text1]

print(f""Likelihood of spam is {spam_likelihood}"")"
lab,naive_bayes.ipynb,markdown,1,"Recall that this probability doesn't have any importance until we compare it to the probability of this text coming from a non-spam number.

$$ P(Not Spam|""Phone"") = P(Not Spam)P(""Phone""|Not Spam) $$"
lab,naive_bayes.ipynb,code,1,"not_spam_likelihood = not_spam_probability * nospam_prob[new_text1]

print(f""Likelihood of not spam is {not_spam_likelihood}"")"
lab,naive_bayes.ipynb,markdown,1,"As we note 0.07 > 0.02, so we **must state this text is spam**. 

Let's now observe a longer example where we must loop through all relevant probabilities in our frequency table."
lab,naive_bayes.ipynb,code,1,"new_text2 = ""USPS $200""

# apply text transformation
new_text2 = new_text2.replace(""."", """").replace(""!"", """")
new_text2 = new_text2.lower().split()

spam_likelihood = spam_probability
not_spam_likelihood = not_spam_probability

for w in new_text2:
    spam_likelihood *= spam_prob[w]
    not_spam_likelihood *= nospam_prob[w]

print(f""Likelihood of spam is {spam_likelihood}"")
print(f""Likelihood of not spam is {not_spam_likelihood}"")"
lab,naive_bayes.ipynb,markdown,1,"## Laplace Smoothing

When counting word frequencies, it often happens that some words do not appear in a dataset for one of the classes (i.e., zero counts). This leads to incorrect estimations."
lab,naive_bayes.ipynb,code,1,"new_text3 = ""Farukh USPS $200""

# apply text transformation
new_text3 = new_text3.replace(""."", """").replace(""!"", """")
new_text3 = new_text3.lower().split()

spam_likelihood = spam_probability
not_spam_likelihood = not_spam_probability

for w in new_text3:
    # for simplicity, we will ignore words we haven't seen
    if w in spam_prob and w in nospam_prob:
        spam_likelihood *= spam_prob[w]
        not_spam_likelihood *= nospam_prob[w]

print(f""Likelihood of spam is {spam_likelihood}"")
print(f""Likelihood of notspam is {not_spam_likelihood}"")"
lab,naive_bayes.ipynb,markdown,1,"Even though this is a spam text, note that one word throws off the entire estimation.

Laplace smoothing addresses this by adding a constant to all counts. The cell below demonstrates a basic plug-and-play implementation using word counts from spam messages."
lab,naive_bayes.ipynb,code,1,"# TODO: play around with this value, see what happens to your frequency table
alpha = 1

# calculate vocab size (number of unique words across both sets)
vocab_size = len(set(spam_prob).union(set(nospam_prob)))

# recalculate each probability using laplace smoothing
smoothed_spam_prob = {}
smoothed_nospam_prob = {}

# calculate smoothed probabilities
for w in spam_frequencies:
    smoothed_spam_prob[w] = round((spam_frequencies[w] + alpha) / (total_spam + (alpha * vocab_size)), 2)

for w in nospam_frequencies:
    smoothed_nospam_prob[w] = round((nospam_frequencies[w] + alpha) / (total_not_spam + (alpha * vocab_size)), 2)

print(""smoothed spam probabilities"", smoothed_spam_prob)
print(""smoot"
lab,naive_bayes.ipynb,code,2,"hed normal word probabilities"",smoothed_nospam_prob)"
lab,naive_bayes.ipynb,code,1,"import matplotlib.pyplot as plt

# display smoothed values of spam texts
plt.figure(figsize=(14, 8), dpi=80)
plt.bar(smoothed_spam_prob.keys(), smoothed_spam_prob.values())
plt.xticks(rotation=45)
plt.title(""Probability of Words in Spam Text"")
plt.show()"
lab,naive_bayes.ipynb,code,1,"# display smoothed values
plt.figure(figsize=(14, 8), dpi=80)
plt.bar(smoothed_nospam_prob.keys(), smoothed_nospam_prob.values())
plt.xticks(rotation=45)
plt.title(""Probability of Words in Non-Spam Text"")
plt.show()"
lab,naive_bayes.ipynb,markdown,1,Let's re-calculate the probability of our previous spam text using our smoothed values.
lab,naive_bayes.ipynb,code,1,"new_text3 = ""Farukh USPS $200""

# apply text transformation
new_text3 = new_text3.replace(""."", """").replace(""!"", """")
new_text3 = new_text3.lower().split()

spam_likelihood = spam_probability
not_spam_likelihood = not_spam_probability

for w in new_text3:
    # for simplicity, we will ignore words we haven't seen
    if w in spam_prob and w in nospam_prob:
        spam_likelihood *= smoothed_spam_prob[w]
        not_spam_likelihood *= smoothed_nospam_prob[w]

print(f""Likelihood of spam is {spam_likelihood}"")
print(f""Likelihood of notspam is {not_spam_likelihood}"")"
lab,naive_bayes.ipynb,markdown,1,"## Exploring the Naive Assumption of Independence 

In a Naive Bayes Classifier, the features (or predictors) are assumed to be independent. This exercise demonstrates that multiplying likelihoods yields the same result regardless of the order of the factors. Such properties are often leveraged in production models when working with complex datasets.

Let's say we get a ""different"" set of text messages, which are just our original text messages in a different order.

Replace your original text messages with this new code block. Do we get the same probabilities all over again?"
lab,naive_bayes.ipynb,code,1,"# Plug and Play Code Block for Exercise 3
spam_texts = [
    ""Click here! Hello, your USPS package was not able to be delivered."",
    ""to be delivered! not package able USPS"",
    ""Give us $200. This is the USPS.""
]

# Non-spam text data
normal_texts = [
    ""Tell me what the central limit theorem is. Quick, it’s Farukh"",
    ""Do you want any stamps? I'm stopping by the USPS."",
]"
lab,naive_bayes.ipynb,markdown,1,"## Gaussian Naïve Bayes Classifier with sklearn on our Dog/Cat/Hamster Dataset

Assume we're working for a startup that uses ""AI"" to find the best pet. We start with a dataset that describes the amount of kibble an animal eats, as well as the amount of decibel noise it emits.

Let's use our Naive Bayes classifier to identify types of animals based on these numerical features."
lab,naive_bayes.ipynb,code,1,"import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import classification_report"
lab,naive_bayes.ipynb,code,1,"data = {
    ""kibble_grams"": [200, 250, 115, 300, 50, 138, 128, 142, 280, 317, 282, 270, 69, 32, 31],
    ""noise_dB"":     [ 40,  60,  45,  80, 75,  38,  49,  42,  76,  88,  80,  73, 77, 75, 77],
    ""animal"": [
        ""cat"", ""dog"", ""cat"", ""dog"", ""hamster"",
        ""cat"", ""cat"", ""cat"",
        ""dog"", ""dog"", ""dog"", ""dog"",
        ""hamster"", ""hamster"", ""hamster""
    ]
}

# Convert to DataFrame
df = pd.DataFrame(data)

df.head()"
lab,naive_bayes.ipynb,code,1,"# select predictors & target
X = df.drop(columns=[""animal""])
y = df[""animal""]

# create a train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# create a Gaussian NB classifier
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# generate predictions and display
y_pred = gnb.predict(X_test)
y_pred"
lab,naive_bayes.ipynb,code,1,"# compare this to the actual target test data
y_test"
lab,naive_bayes.ipynb,markdown,1,"## Gaussian Naïve Bayes Classifier with sklearn (Challenge)

Next, let's implement a Gaussian Naïve Bayes Classifier using the Iris dataset.

We also perform a hyperparameter search using GridSearchCV. 

This exercise reflects practical workflows where model tuning and validation are essential for deploying accurate machine learning models in industry."
lab,naive_bayes.ipynb,code,1,"# TODO: load the Iris dataset
iris = ...

# TODO: select your predictors & target
...

# TODO: the dataset into training and testing sets
..."
lab,naive_bayes.ipynb,code,1,"# TODO: find the name of the smoothing hyperparam using the sklearn docs
# DOCS: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html
param_grid = {
    '...': ...
}

# Initialize the Gaussian Naive Bayes classifier
gnb = GaussianNB()"
lab,naive_bayes.ipynb,code,1,"# TODO: set up GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(...)

# TODO: fit this model on your training data
..."
lab,naive_bayes.ipynb,code,1,"# retrieve the best model and hyperparameters
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# TODO: Use the best model to make predictions on the test set
y_pred = ...

# create a classification report and evaluate
class_report = classification_report(y_test, y_pred)

print(""Best hyperparameters:"", best_params)
print(class_report)"
lab,naive_bayes.ipynb,markdown,1,"# Conclusion

In this lab, you have explored several key components of the Naive Bayes Classifier:

- Computing posterior probabilities using Bayes' Theorem
- Applying Laplace smoothing to correct for zero counts in word frequencies
- Demonstrating the independence assumption by showing that the product of likelihoods remains unchanged regardless of their ordering
- Implementing and tuning a Gaussian Naïve Bayes Classifier on the Iris dataset

Understanding these techniques is critical not only for academic exercises but also in practical data science roles, such as filtering spam emails or classifying data in real-world applications. Continue to experiment with these examples and extend the"
lab,naive_bayes.ipynb,markdown,2,m to better adapt to the challenges you will face on the job.
lab,classification_challenge.ipynb,markdown,1,"# Machine Learning Competition Lab

Today, you'll work with your pod members to build a machine learning model that will predict if a customer will cancel their phone plan. Your goal is to complete a machine learning pipeline on a novel dataset using the techniques and models that you've learned about throughout Phase 2. Additionally, if your model achieves a higher F1-score than other teams, your team will be awarded additional points on the Kahoot leaderboard.

Your team should complete the listed idiomatic steps:
1. EDA: Extract quick insights from your dataset using visualizations. Use the insights you've gained to inform your machine learning step.
2. Machine learning: Select, train, an"
lab,classification_challenge.ipynb,markdown,2,"d find hyperparameters for a 2-3 machine learning models.

While we will not explicitly state which code you should write to accomplish each step, we will ask you to consider the lessons & code-alongs that we've completed over the past couple of weeks to inform your analysis. 

More details are listed below:

**Data-Dictionary**
You will use a dataset pulled from the following UCI repository [link](https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset). More information on this dataset can be found at the following link.

**Prizes**
* 🥇 1st place recall: 1000 points per team member  
* 🥈 2nd place recall: 500 points per team member  
* 🥉 3rd place recall: 250 points per team member  "
lab,classification_challenge.ipynb,markdown,3,"

**Rules**
* You must submit your entry before class ends (9:30).  
* You're encouraged to use your notes, lecture slides, previous labs, and your peers to strategize and debug.  
* This is a chance to apply **everything**: feature engineering, over-sampling, model selection, hyperparameter tuning, regularization, and non-linear models.  

**Submission**

Each team member must submit this notebook with the completed steps to Canvas by 9:30 pm."
lab,classification_challenge.ipynb,code,1,"# TODO: Explore your dataset
..."
lab,classification_challenge.ipynb,code,1,"# TODO: Implement your machine learning model!
..."
lab,decision_trees.ipynb,markdown,1,"# Decision Trees & Entropy Code‐Along Lab

Within this lab, we will review:
* the decision tree structure
* calculating the Gini impurity
* building a decision tree with scikit‐learn
* assessing the performance of decision trees

In each section, you will see code examples with detailed commentary and visualization."
lab,decision_trees.ipynb,markdown,1,"## Understanding Trees

In this section we illustrate a simple representation of a  tree. The tree is represented as a Python dictionary where a node contains a 'question' and two branches for 'yes' and 'no' answers."
lab,decision_trees.ipynb,code,1,"from pprint import pprint

simple_tree = {
    'question': 'Do you have a question on trees?',
    'yes': {
        'question': 'Did you consult the notes?',
        'yes': {
            'question': 'Did you ask a staff member?',
            'yes': {
                'question': 'Was your question answered?',
                'yes': 'Great! Go forward with confidence.',
                'no': 'Google documentation & articles.'
            },
            'no': 'Ask a staff member.'
        },
        'no': 'Look at the notes.'
    },
    'no': {
        'question': 'Are you sure?',
        'yes': {
            'question': 'Are you reeaallly sure?',
            'yes': 'Ok! Tell me what a tree lea"
lab,decision_trees.ipynb,code,2,"f is then.',
            'no': 'Great, ask a question.'
        },
        'no': 'Great, ask a question.'
    }
}

print('Simple decision tree defined as a dictionary:')
pprint(simple_tree)"
lab,decision_trees.ipynb,markdown,1,"The code above creates a Python dictionary called `simple_tree` that mimics a decision tree. The root node asks a question; based on the answer, it branches into a nested dictionary for the 'yes' case or directly returns an action for the 'no' case. Notice how each key represents a decision point.

![img](tree.png)"
lab,decision_trees.ipynb,markdown,1,"## Understanding the Gini Index and Node Purity

In this section we calculate the Gini impurity, which measures the purity of a node. A lower Gini index indicates a purer node (aka a node is more homogenous)."
lab,decision_trees.ipynb,code,1,"def compute_gini(counts):
    """"""
    Compute the Gini impurity for a node given the counts of classes.

    Parameters
    -----------
    counts: a list of counts for each class

    Returns
    -----------
    Gini impurity value (float)
    """"""
    total = sum(counts)
    # for each count of classes
    gini = 0
    for c in counts:
        # add the squared ratio to the gini index
        gini += (c / total) ** 2
    # calculate the impurity
    impurity = 1 - gini
    return impurity"
lab,decision_trees.ipynb,markdown,1,"The `compute_gini` function accepts a list of counts for each class present at a node. It first computes the total count, then calculates the impurity by subtracting the sum of squared probabilities from 1. This function is critical to understanding how decision trees choose the best split."
lab,decision_trees.ipynb,code,1,"# Calculate Gini impurity for a node with an impure split [10, 5]
gini_value = compute_gini([10, 5])
print('Gini impurity for counts [10, 5]:', gini_value)"
lab,decision_trees.ipynb,code,1,"# let's see what happens when the ratio mostly leans towards one class [12, 3]
gini_value = compute_gini([12, 3])
print('Gini impurity for counts [12, 3]:', gini_value)"
lab,decision_trees.ipynb,code,1,"# the split is getting more pure! let's see what happens when we have a ""pure"" split [15, 0]
gini_value = compute_gini([15, 0])
print('Gini impurity for counts [15, 0]:', gini_value)"
lab,decision_trees.ipynb,markdown,1,The code cell above calculates the Gini impurity for a node with two classes with varying counts. The output shows how pure (or impure) this distribution is.
lab,decision_trees.ipynb,markdown,1,"## Growing a Decision Tree with scikit‐learn

In this section we use a real dataset and scikit‐learn's `DecisionTreeClassifier` to build and inspect a decision tree model. We will to continue our exploration of the bank dataset from yesterday."
lab,decision_trees.ipynb,code,1,"import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report

from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV

# TODO: load the bank dataset
...

# TODO: select predictors & target variables
...
...

# TODO: perform a train test split
..."
lab,decision_trees.ipynb,code,1,"from sklearn.tree import DecisionTreeClassifier

# instantiate the DecisionTreeClassifier with a maximum depth of 3 and a fixed random state for reproducibility
dt = DecisionTreeClassifier(max_depth=3, random_state=42)

# Fit the classifier to the data
dt.fit(X_train, y_train)"
lab,decision_trees.ipynb,markdown,1,This cell creates a decision tree classifier using scikit‐learn’s `DecisionTreeClassifier`. The classifier is configured with a maximum depth of 3 and a fixed random state to ensure reproducibility. The model is then fitted to the bank dataset features and target values.
lab,decision_trees.ipynb,code,1,"from sklearn.tree import export_text

# Export the tree structure as text
tree_rules = export_text(dt, feature_names=X_train.columns)
print('Decision tree structure:')
print(tree_rules)"
lab,decision_trees.ipynb,markdown,1,"The cell above uses `export_text` to generate a textual representation of the decision tree. This output details the splits and decision criteria at each node, making it easier to understand how the model makes predictions.

Let's use matplotlib and the tree object from sklearn to visualize what this decision tree looks like."
lab,decision_trees.ipynb,code,1,"from sklearn import tree
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
tree.plot_tree(dt, feature_names=X_train.columns, filled=True, class_names=[""1"", ""0""], rounded=True)
plt.show()"
lab,decision_trees.ipynb,code,1,"yhat = dt.predict(X_test) 

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,decision_trees.ipynb,markdown,1,"As always, let's use hyperparameter search to see if we can discover better performance."
lab,decision_trees.ipynb,code,1,"# TODO: implement random search on the DecisionTree model to find best hyperparams
# DOCS: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier
...

dt = DecisionTreeClassifier(random_state=42)

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
...

# TODO: fit this model on your training data
..."
lab,decision_trees.ipynb,code,1,"# extract the best classifier
best_dt = random_search.best_estimator_

yhat = best_dt.predict(X_test) 

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,decision_trees.ipynb,code,1,random_search.best_params_
lab,decision_trees.ipynb,markdown,1,"## Variance in Decision Trees and Model Evaluation

Even though we found a set of ""best"" hyperparameters, decision trees are known for their high variance. In this section, we illustrate this by training models with different random states and evaluating their performance using cross-validation."
lab,decision_trees.ipynb,code,1,"from sklearn.model_selection import cross_val_score

# instantiate 2 DecisionTreeClassifiers for the train set to display variance
dt1 = DecisionTreeClassifier()
dt1.fit(X_train, y_train)

scores1 = cross_val_score(dt1, X_test, y_test, cv=5)
print('Cross-validation scores for first tree:', scores1)"
lab,decision_trees.ipynb,code,1,"dt2 = DecisionTreeClassifier()
dt2.fit(X_train, y_train)

scores2 = cross_val_score(dt2, X_test, y_test, cv=5)
print('Cross-validation scores for second tree:', scores2)"
lab,decision_trees.ipynb,markdown,1,"This cell demonstrates variance by training two decision tree models . The `cross_val_score` function is used to evaluate each model with 5-fold cross-validation. Comparing the scores helps illustrate how sensitive decision trees can be to variations in training processes.

Notice that the same does not occur for the previous classifier's we've discussed.
* kNN
* Naive Bayes"
lab,decision_trees.ipynb,markdown,1,### kNN Variance Check
lab,decision_trees.ipynb,code,1,"from sklearn.neighbors import KNeighborsClassifier

knn1 = KNeighborsClassifier()
knn1.fit(X_train, y_train)

scores1 = cross_val_score(knn1, X_test, y_test, cv=5)
print('Cross-validation scores for first tree:', scores1)"
lab,decision_trees.ipynb,code,1,"knn2 = KNeighborsClassifier()
knn2.fit(X_train, y_train)

scores2 = cross_val_score(knn1, X_test, y_test, cv=5)
print('Cross-validation scores for first tree:', scores2)"
lab,decision_trees.ipynb,markdown,1,### Naive Bayes Check
lab,decision_trees.ipynb,code,1,"from sklearn.naive_bayes import GaussianNB

gnb1 = GaussianNB()
gnb1.fit(X_train, y_train)

scores1 = cross_val_score(gnb1, X_test, y_test, cv=5)
print('Cross-validation scores for first tree:', scores1)"
lab,decision_trees.ipynb,code,1,"gnb2 = GaussianNB()
gnb2.fit(X_train, y_train)

scores2 = cross_val_score(gnb1, X_test, y_test, cv=5)
print('Cross-validation scores for first tree:', scores2)"
lab,random_forests.ipynb,markdown,1,"# Ensemble Learning Lab

By the end of this lab you will review:
- Bootstrapping and Aggregating (Bagging)
- Random Forests with Feature Sub–Sampling
- Boosting with AdaBoost
- Additional concepts on reproducibility and cross–validation"
lab,random_forests.ipynb,markdown,1,"## Bootstrapping and Aggregating (Bagging)

In this section we demonstrate how to generate bootstrap samples from a dataset and aggregate predictions by computing summary statistics."
lab,random_forests.ipynb,code,1,"import numpy as np
import pandas as pd

# create a sample dataset representing model outcomes
data = np.array([10, 12, 11, 13, 12, 9, 15, 12, 10, 11])

data"
lab,random_forests.ipynb,markdown,1,"The above cell creates a NumPy array (or vector) named `data` that contains a series of numerical values. These numbers simulate target values from a simple model.

Let's measure the variance of this vector."
lab,random_forests.ipynb,code,1,np.var(data)
lab,random_forests.ipynb,markdown,1,Let's observe as to what happens when we apply the bootstrap. First we will impelement a function that randomly samples from our dataset.
lab,random_forests.ipynb,code,1,"def bootstrap_sample(data, sample_size):
    # sample with replacement using numpy's random.choice function
    return np.random.choice(data, size=sample_size, replace=True)

bootstrap_sample(data, 5)"
lab,random_forests.ipynb,code,1,"# we will be creating a new sample of the same length as our original sample. Notice that we still get a different set from our original!
bootstrap_sample(data, len(data))"
lab,random_forests.ipynb,markdown,1,"This cell defines a function called `bootstrap_sample` that takes in the original data and a desired sample size. The function uses `np.random.choice` with `replace=True` to randomly sample data points with replacement, simulating the bootstrapping process. It then generates a bootstrap sample equal in length to the original data and prints it. 

Using this function, we will resample our dataset, calculate the average of each sample, and re-evaluate variance."
lab,random_forests.ipynb,code,1,"# generate bootstrap samples and compute averages
resampled_means = []
for i in range(5):
    # TODO: re-sample using the `bootstap_sample` function (use the same length of your original sample)
    sample = ...
    # TODO: calculate its' mean and append to list
    mean = ...
    ...

resampled_means"
lab,random_forests.ipynb,markdown,1,"In this cell, we generate 5 bootstrap samples using a for loop. Think of this as the `map` pattern from decades ago (phase 1). For each bootstrap sample, we compute the average value. The resulting list of averages is printed. This demonstrates how aggregating predictions (using the mean) over multiple bootstrapped datasets can help reduce variance in estimates.

Let's observe if variance was reduced in our new dataset."
lab,random_forests.ipynb,code,1,np.var(resampled_means)
lab,random_forests.ipynb,markdown,1,Amazing! We went from 2.65 to 0.34. Let's see how we apply this to our decision trees.
lab,random_forests.ipynb,markdown,1,"## Random Forests: Extending Bagging with Feature Sub–Sampling

In this section we move beyond basic bootstrapping by applying the Random Forest algorithm. Random forests extend bagging by training each decision tree on a random subset of features, which further decorrelates the models and improves prediction robustness."
lab,random_forests.ipynb,code,1,"from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import confusion_matrix, classification_report

from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV

import matplotlib.pyplot as plt
import seaborn as sns"
lab,random_forests.ipynb,code,1,"# TODO: load the bank dataset
...

# TODO: select predictors & target variables
...
...

# TODO: perform a train test split
..."
lab,random_forests.ipynb,code,1,"# instantiate a RandomForestClassifier with a fixed random seed for reproducibility
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)"
lab,random_forests.ipynb,code,1,"# let's generate a classification report to see how well our random forest performed
yhat = rf.predict(X_test) 

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,random_forests.ipynb,markdown,1,"While it seems like we have comparable accuracy to our decision tree, let's see what happens when we perform hyperparameter search."
lab,random_forests.ipynb,code,1,"# TODO: implement random search on the RandomForestClassifier model to find best hyperparams
# DOCS: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
param_dist = {
    ""criterion"": ...,
    ""max_depth"": ...,
    ""min_samples_split"": ...,
    ""max_features"": ...
}

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
...

# TODO: fit this model on your training data
..."
lab,random_forests.ipynb,code,1,"# TODO: extract the best rf estimator
rf_best = ...

# TODO: use this estimator to generate ""yhat"" on your X_test dataset
yhat = ...

# TODO: generate a confusion matrix and a classification report
confusion = ...
class_report = ...

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,random_forests.ipynb,markdown,1,"Wow, 26% recall on ""yes"" instances. This seems to be highest we've gotten so far! 

Keep in mind that there are other techniques we can employ to try for higher success metrics. This involves:
* scaling your data
* using grid-search
* feature engineering

Some of which you will employ in tonights challenge."
lab,random_forests.ipynb,markdown,1,"But before we get to that, let's evaluate our ""impurity"" removed metric to evaluate the most important features in our dataset."
lab,random_forests.ipynb,code,1,"# extract feature importances from the trained RandomForest model
importances = rf_best.feature_importances_
feature_names = X.columns

# Create a bar plot to visualize feature importances
plt.figure(figsize=(8, 4))
plt.bar(feature_names, importances)
plt.title('Feature Importances from Random Forest')
plt.xlabel('Feature')
plt.ylabel('Importance Score')
plt.show()"
lab,random_forests.ipynb,markdown,1,"This cell first retrieves the feature importance scores from the fitted Random Forest model and then plots these scores using a bar plot. The x-axis shows the feature names and the y-axis indicates the importance score for each feature. The plot helps visualize how much each feature contributes to the model's decision-making process.

It appears that performing splits on the ""duration"" variable leads to more impurity removed, so we therefore reason that ""duration"" is more important than ""balance"" in terms of converting outreach into paying customers."
lab,random_forests.ipynb,markdown,1,"## Boosting with AdaBoost

Boosting is a sequential ensemble method where each new model attempts to correct the errors of its predecessor. In this section, we work with AdaBoost, which uses decision stumps (trees with a single split) as weak learners and iteratively adjusts the weights of training samples."
lab,random_forests.ipynb,code,1,"from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# create a decision stump (a tree with max_depth=1) to use as the weak learner
stump = DecisionTreeClassifier(max_depth=1)

# initialize the AdaBoostClassifier with 50 weak learners and a fixed random state
ada = AdaBoostClassifier(estimator=stump, random_state=42)

ada.fit(X_train, y_train)"
lab,random_forests.ipynb,markdown,1,"In this cell, we first import `AdaBoostClassifier` and `DecisionTreeClassifier` from scikit-learn. A decision stump is created as a weak learner by setting `max_depth=1`. The AdaBoost classifier is then initialized with 50 estimators (`n_estimators=50`) and a fixed random state for reproducibility. Finally, the AdaBoost model is trained on the bank marketing dataset.

Let's evaluate its' accuracy."
lab,random_forests.ipynb,code,1,"# let's generate a classification report to see how well our random forest performed
yhat = ada.predict(X_test) 

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,random_forests.ipynb,markdown,1,"37% recall on ""yes"" cases?!?! With no scaling? No feature selection? No hyperparameter search? Just plain out-of-the-box machine learning?

![amazing](https://i.pinimg.com/736x/3d/8e/9d/3d8e9d8286222e3705e0636b94832335.jpg)

You should immediately be heading to Kaggle right now to try to win a competition or two.

Let's follow this up with some actual hyperparameter search."
lab,random_forests.ipynb,code,1,"# TODO: implement random search on the AdaBoostClassifier model to find best hyperparams
# DOCS: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
...

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
...

# TODO: fit this model on your training data
..."
lab,random_forests.ipynb,code,1,"# TODO: extract the best adaboost estimator
...

# TODO: use this estimator to generate ""yhat"" on your X_test dataset
...

# TODO: generate a confusion matrix and a classification report
...
...

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,random_forests.ipynb,markdown,1,"It seems like we increased precision at the cost of our recall. This reveals that while adaboost is a powerful machine learning model, it will be poorly trained unless we specify the measure it should optimize.

Let's focus our training efforts on the `f1_score` metric by creating a customized scorer via the `make_scorer` method. This is a second-order function which takes in other functions (such as f1_score, or precision_score) along with additional parameters which specify what we should consider a positive case (`pos_label`) and lastly how to handle cases when we get 0 in the denominator (`zero_division`)."
lab,random_forests.ipynb,code,1,"from sklearn.metrics import f1_score, make_scorer

# set up an f1 scorer
f1_scorer = make_scorer(f1_score, pos_label='yes', zero_division=0)"
lab,random_forests.ipynb,markdown,1,"We also increase the number of training iterations to `n_iter=100` so that we can explore a wider ""area"" of hyperparameter combinations in our `RandomizedSearchCV` object."
lab,random_forests.ipynb,code,1,"# set up RandomizedSearchCV with 5-fold cross-validation, n_iter of 50, and an f1 scoring parameter
random_search = RandomizedSearchCV(ada, param_distributions=param_dist, cv=5, n_iter=100, scoring=f1_scorer, random_state=42)

# fit this model on your training data
random_search.fit(X_train, y_train)"
lab,random_forests.ipynb,code,1,"# TODO: extract the best adaboost estimator
...

# TODO: use this estimator to generate ""yhat"" on your X_test dataset
...

# TODO: generate a confusion matrix and a classification report
...
...

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"", class_report)"
lab,random_forests.ipynb,markdown,1,"In the above cell, we may have failed to find hyperparameters that resulted in a meaningful increase of f1-score. This highlights the importance of continuously iterating on your models using the feature engineering, regularizing, and oversampling techniques that we've explored throughout the past modules."
lab,svm.ipynb,markdown,1,"# Support Vector Machines Lab

Within this lab, we will review:
* linear algebra fundamentals 
* the hard margin & the maximal margin classifier
* the soft margin & support vectors
* the kernel trick & support vector machines

In each section, you will see code examples with detailed commentary and visualization."
lab,svm.ipynb,markdown,1,"##  Linear Algebra Fundamentals

In this section, we will work with numpy arrays to demonstrate the creation of vectors and matrices, and perform basic linear algebra operations such as dot products and transposition."
lab,svm.ipynb,code,1,"import numpy as np

# previously in Python we used lists to express ordered arrangements of data
lst = [1, 2, 3]

# in linear algebra, we call this a vector
# we can express this as a one-dimensional numpy array (vector)
vector = np.array([1, 2, 3])

vector"
lab,svm.ipynb,code,1,"# previously in Python we used lists of lists to express highly dimensional data
lst_of_lst = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]

# similarly we can express this using a 2-dimensional numpy array (matrix)
matrix = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
    [10, 11, 12]
])

matrix"
lab,svm.ipynb,markdown,1,"The code above starts by importing the numpy library. We create a one-dimensional array named `vector` and a two-dimensional array named `matrix`. 

The matrix is defined with three rows and three columns (4 x 3 matrix). Note that we always define a matrix or vector by the number of its' rows first and then its' columns (RC cola).

We also often apply mathematical operations to these structures."
lab,svm.ipynb,code,1,"# we can apply a scalar value (a single value) to our arrays
matrix * 2"
lab,svm.ipynb,code,1,vector * 2
lab,svm.ipynb,code,1,vector + 2
lab,svm.ipynb,code,1,vector - 2
lab,svm.ipynb,markdown,1,Note that we get the same output as the above when we express this as a multiplication operation between two vectors.
lab,svm.ipynb,code,1,"vector * np.array([2, 2, 2])"
lab,svm.ipynb,code,1,"# as well as addition
vector + np.array([2, 2, 2])"
lab,svm.ipynb,code,1,"# and subtraction
vector - np.array([2, 2, 2])"
lab,svm.ipynb,markdown,1,"This hints that there's something more going on underneath the hood of numpy. This is what we call [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html).

When we apply arithmetic operations to arrays of different dimensions, `numpy` vectorizes array operation to save us computational time. If you're interested as to how data scientists use this to save time (and money) check out this [talk by Dr.Burchell](https://www.youtube.com/watch?v=9_mhjjlKjDo).

Next, let's learn about matrix operations."
lab,svm.ipynb,code,1,"# we can calculate a transpose on our matrix, notice how this ""tilts"" our original matrix
matrix_transposed = matrix.T
print(matrix_transposed)"
lab,svm.ipynb,markdown,1,"This operation becomes highly relevant when calculating something called the [dot product](https://en.wikipedia.org/wiki/Dot_product) between two matrices.

The dot product is defined as the ""inner product"" between two matrices. This is algebraically defined as the sum of the products of corresponding rows & columns.

![dot product](https://algebra1course.wordpress.com/wp-content/uploads/2013/02/slide10.jpg)

As we will see later, we use this measure to calculate the ""similarity"" between two vectors.

We can only calculate a dot-product between two matrices if the number of columns in the first matrix lines up with the number of rows in the second. So while this will work..."
lab,svm.ipynb,code,1,"# 3 x 3 matrix
matrix1 = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])
# 3 x 2 matrix
matrix2 = np.array([
    [1, 2],
    [3, 4],
    [5, 6]
])

dot_product = np.dot(matrix1, matrix2)

print(dot_product)"
lab,svm.ipynb,markdown,1,... this won't
lab,svm.ipynb,code,1,"# 3 x 3 matrix
matrix3 = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])
# 2 x 3 matrix
matrix4 = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

dot_product = np.dot(matrix3, matrix4)

print(dot_product)"
lab,svm.ipynb,markdown,1,"To enable the dot product calculation between these two matrices, we can take the transpose of the second matrix to flip our rows & columns."
lab,svm.ipynb,code,1,"dot_product = np.dot(matrix3, matrix4.T)

print(dot_product)"
lab,svm.ipynb,markdown,1,"While it is not necessary to take the transpose of our vector when applying a dot-product in `numpy`, we express the dot product operation with the transpose of the beta vector to ensure dimensions match."
lab,svm.ipynb,code,1,"# (1 x 3) vector
betas = np.array([1, 2, 3])

# (1 x 3) vector
X = np.array([4, 5, 6])

# Compute the dot product of the vector and the extracted column
dot_product = np.dot(betas.T, X)
print(""B^T * X ="", dot_product)"
lab,svm.ipynb,markdown,1,"## Understanding Hyperplanes and the Concept of Margins

In this section, we will visualize data points from two classes and illustrate a hyperplane that acts as a decision boundary. The hyperplane and margins are visualized on a 2D plot to help you understand how they separate data points."
lab,svm.ipynb,code,1,"import matplotlib.pyplot as plt
import numpy as np

# define two sets of 2D data points for two classes
class0 = np.array([
    [1, 2],
    [2, 3],
    [3, 3]
])

class1 = np.array([
    [6, 5],
    [7, 8],
    [8, 6]
])

# create a scatter plot for both classes
plt.figure(figsize=(6, 4))
plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Class 0')
plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Class 1')

# define a hyperplane using a simple linear equation (e.g., x2 = 5 - 0.5*x1)
x_vals = np.linspace(0, 10, 100)
# pseudo-coefficients
hyperplane = 5 - 0.5 * x_vals

# plot the hyperplane as a dashed line
plt.plot(x_vals, hyperplane, 'k--', label='Hyperplane')

plt.xlabe"
lab,svm.ipynb,code,2,"l('Feature 1')
plt.ylabel('Feature 2')
plt.title('Data Points with Hyperplane')
plt.legend()
plt.show()"
lab,svm.ipynb,markdown,1,Let's see what a potential hyperplane would look like in 3-dimensions.
lab,svm.ipynb,code,1,"# define two sets of 3D data points for two classes
class0 = np.array([
    [1, 2, 1],
    [2, 3, 2],
    [3, 3, 2]
])

class1 = np.array([
    [6, 5, 6],
    [7, 8, 8],
    [8, 6, 7]
])

# create a 3D scatter plot
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(class0[:, 0], class0[:, 1], class0[:, 2], color='blue', label='Class 0')
ax.scatter(class1[:, 0], class1[:, 1], class1[:, 2], color='red', label='Class 1')

# define the hyperplane: x3 = w0 + w1*x1 + w2*x2
# pseudo-coefficients
w0, w1, w2 = 1, 0.5, -0.4 
x1_range = np.linspace(0, 10, 20)
x2_range = np.linspace(0, 10, 20)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
x3_grid = w0 + w1 * x1_"
lab,svm.ipynb,code,2,"grid + w2 * x2_grid

# Plot the hyperplane
ax.plot_surface(x1_grid, x2_grid, x3_grid, alpha=0.5, color='gray', label='Hyperplane')

# Label axes
ax.set_xlabel('Feature 1 (x1)')
ax.set_ylabel('Feature 2 (x2)')
ax.set_zlabel('Feature 3 (x3)')
ax.set_title('3D Data Points with Separating Hyperplane')

ax.legend()
plt.show()"
lab,svm.ipynb,markdown,1,"Here, we define two sets of points, each representing a different class(`class0` and `class1`). These points are plotted using `plt.scatter` with different colors. 

A hyperplane is defined by the **linear equation** x2 = 5 - 0.5 * x1 and is plotted as a dashed line using `plt.plot`."
lab,svm.ipynb,markdown,1,"## Implementing a Linear Support Vector Classifier (SVC) on the Bank Dataset

Now that we are familiar with the underpinnings of SVC, let's train a classifier using scikit-learn's `LinearSVC` on the bank marketing dataset (unrelated to TLAB 2). For more information on this dataset, check out the following [link](https://archive.ics.uci.edu/dataset/222/bank+marketing).

We will:
- Load the bank marketing dataset using pandas.
- Perform EDA.
- Split the dataset into training and testing sets.
- Iteratively tune the hyperparameter to determine the best value based on test accuracy for our Linear SVC.
- Evaluate results."
lab,svm.ipynb,code,1,"import pandas as pd

# TODO: import the bank dataset
bank = pd.read_csv(""bank.csv"")

bank.head()"
lab,svm.ipynb,code,1,"bank.groupby(""y"")[""age""].count()"
lab,svm.ipynb,code,1,"# TODO: perform EDA!
..."
lab,svm.ipynb,code,1,"from sklearn.preprocessing import OneHotEncoder

# perform one-hot-encoding on a set of categorical columns

# TODO: select your choice of categorical columns
cat_features = [...]                              
num_features = [""balance"", ""duration""]    

X_cat = bank[cat_features]
X_num = bank[num_features]

X_cat.head()"
lab,svm.ipynb,code,1,"ohe = OneHotEncoder()
X_cat_full = ohe.fit_transform(X_cat).toarray()

X_cat_full"
lab,svm.ipynb,code,1,ohe.get_feature_names_out(cat_features)
lab,svm.ipynb,code,1,"cat_names = ohe.get_feature_names_out(cat_features)

encoded_df = pd.DataFrame(X_cat_full, columns=cat_names, index=bank.index)

encoded_df.head()"
lab,svm.ipynb,code,1,"X = pd.merge(X_num, encoded_df, left_index=True, right_index=True)

X"
lab,svm.ipynb,code,1,"# select the target
y = bank[""y""]"
lab,svm.ipynb,code,1,"import numpy as np
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix, classification_report

from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV

# TODO: perform a train test split
...

# initialize LinearSVC with regularization parameter C=1.0 and set max_iter for convergence
lin_svc = LinearSVC(C=1.0, max_iter=10000, random_state=42)

# train the classifier on the dataset
lin_svc.fit(X, y)

# make predictions on the same dataset
yhat = lin_svc.predict(X_test) 

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report\n"""
lab,svm.ipynb,code,2,", class_report)"
lab,svm.ipynb,markdown,1,"88%, not bad!

But how can we acheive better accuracy? 

That's right! Good old hyperparameter search."
lab,svm.ipynb,code,1,"# TODO: implement random search on the LinearSVC model to find best hyperparams
# DOCS: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html
param_grid = {
    ...
}

svc = LinearSVC(max_iter=10000)

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
...

# TODO: fit this model on your training data
..."
lab,svm.ipynb,markdown,1,Let's see how our best seperating linear hyperplane looks like on 2-dimensions of our highly-dimensional dataset.
lab,svm.ipynb,code,1,"from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Only use 2 numeric features
X_vis = bank[[""balance"", ""duration""]]
y_vis = (bank[""y""] == ""yes"").astype(int)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_vis)

# Fit SVC on 2D data
best_svc = random_search.best_estimator_
best_svc.fit(X_scaled, y_vis)

# Plot
xx, yy = np.meshgrid(
    np.linspace(X_scaled[:, 0].min(), X_scaled[:, 0].max(), 100),
    np.linspace(X_scaled[:, 1].min(), X_scaled[:, 1].max(), 100)
)
zz = best_svc.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.contourf(xx, yy, zz > 0, alpha=0.3, cmap='coolwarm')
plt.contour(xx, yy, zz, levels="
lab,svm.ipynb,code,2,"[0], colors='k', linestyles='--')  # Decision boundary
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_vis, cmap='coolwarm', edgecolors='k')
plt.xlabel(""Balance (scaled)"")
plt.ylabel(""Duration (scaled)"")
plt.title(""LinearSVC Decision Boundary (2D)"")
plt.show()"
lab,svm.ipynb,markdown,1,It seems like assuming a linear seperation is unrealistic (even when we find best hyperparameters). Let's explore non-linear decision boundaries.
lab,svm.ipynb,markdown,1,"## Exploring Non-Linear SVM and the Kernel Trick

This section introduces a non-linear Support Vector Machine. We explore the kernel trick using scikit-learn's `SVC` on a non-linearly separable dataset (an XOR pattern). A contour plot visualizes the decision boundary produced by the chosen kernel."
lab,svm.ipynb,code,1,"import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

# initialize a Support Vector Classifier with RBF kernel to handle non-linearity
svc_non_linear = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)

# Train the classifier on the XOR dataset
svc_non_linear.fit(X_train, y_train)

# make predictions on the same dataset
yhat = svc_non_linear.predict(X_test)

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report:\n"", class_report)"
lab,svm.ipynb,markdown,1,Look's like we get a pretty abysmal f1-score on `yes` cases. Let's do some hyperparameter search to try out different kernel functions.
lab,svm.ipynb,code,1,"# implement random search on the LinearSVC model to find best hyperparams
param_grid = {
    'C': np.linspace(0.1, 10, 100),
    'kernel': ['rbf', 'poly', 'sigmoid'],
    'gamma': ['scale', 'auto'],
    'degree': [2, 3, 4, 5]  
}

svc = SVC(max_iter=10000, random_state=42)

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
...

# TODO: fit this model on your training data
..."
lab,svm.ipynb,code,1,"best_svc = random_search.best_estimator_

# make predictions on the same dataset
yhat = best_svc.predict(X_test)

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report:\n"", class_report)"
lab,svm.ipynb,markdown,1,"Not only is our f1-score remaining low, but it also looks like we're getting some kind of warning. Let's inspect this message closer.

```
ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
```

Looks like we're failing to converge, which essentially indicates that our model is [failing to learn](https://www.reddit.com/r/learnmachinelearning/comments/1f8fjek/what_is_convergence/). However, the `sklearn` package is providing us a recommendation: `Consider pre-processing your data with StandardScaler or MinMaxScaler`. Let's use our knowledge of code from the knn lab to implement a min-max scaler on our dataset.
"
lab,svm.ipynb,markdown,2,"
According to this [stackoverflow](https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati) link, we also have a couple of other options when it comes to solving this type of error."
lab,svm.ipynb,code,1,"from sklearn.preprocessing import MinMaxScaler

# TODO: intialize the MinMaxScaler to scale data to the range [0, 1]
...

# TODO: fit on the training data, then transform both training and testing data
...
..."
lab,svm.ipynb,markdown,1,Now we can re-implement our SVC classifier on our scaled dataset. For good measure let's increase our `max_iter` as well.
lab,svm.ipynb,code,1,"svc = SVC(max_iter=100000, random_state=42)

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
...

# TODO: fit this model on your training data
..."
lab,svm.ipynb,code,1,"best_svc = random_search.best_estimator_

# make predictions on the same dataset
yhat = best_svc.predict(X_test_scaled)

confusion = confusion_matrix(y_test, yhat)
class_report = classification_report(y_test, yhat)

print(""Confusion Matrix \n"", confusion)
print(""\nClassification Report:\n"", class_report)"
lab,svm.ipynb,code,1,"from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Only use 2 numeric features
X_vis = bank[[""balance"", ""duration""]]
y_vis = (bank[""y""] == ""yes"").astype(int)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_vis)

# Fit SVC on 2D data
best_svc = random_search.best_estimator_
best_svc.fit(X_scaled, y_vis)

# Plot
xx, yy = np.meshgrid(
    np.linspace(X_scaled[:, 0].min(), X_scaled[:, 0].max(), 100),
    np.linspace(X_scaled[:, 1].min(), X_scaled[:, 1].max(), 100)
)
zz = best_svc.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.contourf(xx, yy, zz > 0, alpha=0.3, cmap='coolwarm')
plt.contour(xx, yy, zz, levels="
lab,svm.ipynb,code,2,"[0], colors='k', linestyles='--')  # Decision boundary
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_vis, cmap='coolwarm', edgecolors='k')
plt.xlabel(""Balance (scaled)"")
plt.ylabel(""Duration (scaled)"")
plt.title(""Kernel Decision Boundary (2D)"")
plt.show()"
lab,eda_rental.ipynb,markdown,1,"# Apartment Price Prediction Case Study

You are a data scientist at Willoz, a real-estate online marketplace based in Albuquerque, NM. 

Your team has been contracted by a property aggregator to improve user engagement by offering a personalized recommendation widget. This feature will allow users to enter their apartment preferences (size, pet-policy) and receive a prediction of a typical apartment price. You've been provided with a historical dataset of 10,000 rental listings scraped from various sources containing predictors such as bedrooms, price, location, and descriptions.

We will demo

Your task is to:
* Explore and clean the dataset,
* Engineer new features using the insights you'"
lab,eda_rental.ipynb,markdown,2,"ve discovered from your EDA,
* Deploy it locally using a light-weight streamlit app that takes in apartment features and returns a price.

Let's use the patterns we've learned about in class to complete this case study together."
lab,eda_rental.ipynb,markdown,1,"## EDA

Let's get started with exploratory data analysis. 

* Load the dataset and identify the structure and content of each column. 
* Identify analytical & predictive questions to determine valuable analyses. 
* Generate summary statistics and visualizations on features such as price, bedrooms, bathrooms, square_feet, and cityname.  
* Investigate missing or inconsistent values and decide how to address them.  
* Identify correlations or relationships that might impact housing price or desirability."
lab,eda_rental.ipynb,code,1,"import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt"
lab,eda_rental.ipynb,markdown,1,### Identifying the Structure of the Dataset
lab,eda_rental.ipynb,code,1,"rentals = pd.read_csv(""apartments_for_rent.csv"")"
lab,eda_rental.ipynb,code,1,"# identify the shape
..."
lab,eda_rental.ipynb,code,1,"# identify the columns
..."
lab,eda_rental.ipynb,code,1,"# take a look at the first few rows
..."
lab,eda_rental.ipynb,markdown,1,"## Forming Good Exploratory Questions

Let's consider good exploratory questions given the structure of our dataset and the types of variables our dataset contains. Consider these questions and how we can apply them to our dataset to ask specific questions about our columns.

* Which column are we attempting to predict?
* What numerical variables do we have? Which visualization techniques can we apply to these variables?
* What categorical variables do we have? Which visualization techniques can we apply to these variables?
* How can we explore the relationships between numeric vs numeric? What about categorical vs numeric?"
lab,eda_rental.ipynb,markdown,1,"### Generate Visualizations

Let's perform both univariate and bivariate analysis."
lab,eda_rental.ipynb,markdown,1,"### Investigate Missing, Incosistent, or Outlier Values. Identify the ""Shape"" of your Data (Univariate Analysis)

Let's identify the distributions of our numeric columns, as well as our categorical columns. Additionally, let's observe if we have any null values."
lab,eda_rental.ipynb,code,1,...
lab,eda_rental.ipynb,markdown,1,"### Identify correlations or relationships (Bivariate Analysis)

Now, let's observe if we have any clear relationships between our predictors & our target (price). Remember, our predictors could be categorical as well! Furthermore, even if our predictors are expressed as numerical, they can still *manifest* themselves as discrete if we don't expect decimal values, and we are limited to a range of numeric values."
lab,eda_rental.ipynb,code,1,...
lab,eda_rental.ipynb,markdown,1,"### Observations

Looking at our distributions and null values, it appears that we do have outlier values as well as missing data. Furthermore, it appears that there are specific columns which **should not** be included in our prediction step as it could potentially bias our data to specific samples. This includes columns like `id`, `category`, `title`, `body`, `address`, `latitude`, `longitude`, `source`, `time`, and others.

Remember, we want to be able to predict the price of a rental property given the independent features of an apartment that we have at the ready. Features such as `id`, `title` and `body` are already expressed in our sample through other columns such as `bathrooms` and "
lab,eda_rental.ipynb,markdown,2,"`bedrooms`. 

For our predictive columns, must make an executive decision as to what we want to do with missing values. We could either:
* Drop columns with missing values,
* Drop rows with missing values (and remove roughly half of our dataset),
* Impute missing values,
* Or perform all 3 steps in some specific order. The order in which you apply these steps result in different datasets!

Take a close look at your dataset however! A ""None"" might not always mean `NA` in this dataset. For example, what might ""None"" mean in the context of the `pets_allowed` or `amenities` columns?

We should also decide what to do with outlier values. We could either:
* Transform columns to fit a normal distri"
lab,eda_rental.ipynb,markdown,3,"bution
* Filter your dataset to not include atypical values in your analysis.

More information on possible data transformations are listed here: https://developers.google.com/machine-learning/crash-course/numerical-data/normalization.

Furthermore it looks like we have data that could be transformed to ensure we have as much as data as available for greater predictive capabilities. This includes:
* Encoding specific categorical predictors in our analysis,
* Converting values with alternative units of measurement into 1 standard

Which data transformation technique have we learned about before which can assist us in this?

Remember, there is no one concrete methodology. You must be able to c"
lab,eda_rental.ipynb,markdown,4,hoose transformation techiques and justify them!
lab,kmeans.ipynb,markdown,1,"# Unsupervised Learning – Understanding K-Means Clustering Notebook

In this lab you will explore unsupervised learning by building up the K-Means clustering algorithm from first principles. By the end of this lab you will review:
- Clustering algorithms
- Euclidean distance
- The Kmeans clustering algorithm
- Recommendation Systems"
lab,kmeans.ipynb,markdown,1,"## Introduction to Unsupervised Learning

Unsupervised learning methods discover inherent patterns in data without using labeled outcomes. In this section, we illustrate these ideas with a simple visualization."
lab,kmeans.ipynb,code,1,"import numpy as np
import matplotlib.pyplot as plt"
lab,kmeans.ipynb,code,1,"# define a toy dataset of 2D points
points = np.array([
    [20, 20], 
    [30, 30], 
    [40, 20], 
    [50, 20], 
    [50, 50], 
    [30, 50], 
    [40, 10], 
    [50, 10], 
    [60, 30], 
    [60, 50]
])"
lab,kmeans.ipynb,code,1,"# Create a scatter plot to visualize these clusters
plt.scatter(points[:, 0], points[:, 1], color='blue')
plt.title('Visualization of Two Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,markdown,1,"Let's say we have the following unlabeled dataset. Could we sufficiently assign labels that cluster these data points together? 

Let's assume two clusters in our dataset and apply the kmeans clustering algorithm."
lab,kmeans.ipynb,code,1,"# set the number of clusters
k = 2

# randomly assign a cluster for each data point
np.random.seed(42) 
clusters = np.random.randint(0, k, size=points.shape[0])

# print out initial cluster assignments
print('Initial cluster assignments:', clusters)"
lab,kmeans.ipynb,code,1,"# Create a scatter plot to visualize these clusters
colors = {0:'tab:blue', 1:'tab:orange'}

plt.scatter(points[:, 0], points[:, 1],  c=np.vectorize(colors.get)(clusters))
plt.title('Visualization of Two Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,markdown,1,"We've created two manually defined clusters and visualized them using a scatter plot. How well did we capture groupings however?

To answer this question, we must calculate centroids and utilize our squared euclidean distance formula."
lab,kmeans.ipynb,markdown,1,"## Conceptual Walkthrough of K-Means Clustering

In this section we build foundational components for the K-Means algorithm starting with distance computation and centroid calculation."
lab,kmeans.ipynb,code,1,"def squared_euclidean_distance(a, b):
    """"""Compute and return the squared Euclidean distance between two points a and b.""""""
    return np.sum((a - b) ** 2)"
lab,kmeans.ipynb,markdown,1,"The function defined above computes the squared Euclidean distance between two points. It subtracts one point from the other, squares the differences, and sums them. This metric is useful for determining how similar or different two points are in Euclidean space.

In conjunction with the squared_euclidean_distance function, let's calculate the centroids of our respective classes."
lab,kmeans.ipynb,code,1,"# get clusters
cluster0 = points[clusters == 0]
cluster1 = points[clusters == 1]

print(""cluster 0\n"", cluster0)
print(""cluster 1\n"", cluster1)"
lab,kmeans.ipynb,markdown,1,We can calculate the centroids of our clusters by simply getting the average data point across all points within one cluster.
lab,kmeans.ipynb,code,1,"centroid0 = cluster0.mean(axis=0)
centroid1 = cluster1.mean(axis=0)

print(""centroid 0\n"", centroid0)
print(""centroid 1\n"", centroid1)"
lab,kmeans.ipynb,code,1,"plt.scatter(points[:, 0], points[:, 1],  c=np.vectorize(colors.get)(clusters))
plt.plot(centroid0[0],centroid0[1],marker='s', c='black', label='Centroid 0') 
plt.plot(centroid1[0],centroid1[1],marker='s', c='red', label='Centroid 1') 
plt.title('Visualization of Two Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,markdown,1,"Lastly, let's define logic which will iterate over each data point, measure the euclidean distance from centroids, and update data point clusters by choosing the class with the nearest centroid."
lab,kmeans.ipynb,code,1,"def assign_clusters(data, centroids):
    """"""Assign each data point to the closest centroid using squared Euclidean distance.""""""
    clusters = []
    for point in data:
        distances = [squared_euclidean_distance(point, c) for c in centroids]
        clusters.append(np.argmin(distances))
    return np.array(clusters)"
lab,kmeans.ipynb,markdown,1,"The function `assign_clusters` iterates through each data point, computes its squared Euclidean distance to each centroid, and assigns the point to the cluster with the minimum distance.

Additionally, we need logic which will iteratively calculate new centroids based on our new cluster assignment."
lab,kmeans.ipynb,code,1,"def update_centroids(data, clusters, K):
    """"""Calculate new centroids as the mean of data points in each cluster.""""""
    new_centroids = []
    for k in range(K):
        points = data[clusters == k]
        new_centroids.append(points.mean(axis=0) if len(points) > 0 else np.array([0, 0]))
    return np.array(new_centroids)"
lab,kmeans.ipynb,markdown,1,"The `update_centroids` function computes the new centroid for each cluster by taking the mean of all data points assigned to that cluster. It also handles the case where a cluster might have no points.

Let's execute one iteration of our kmean algorithm and observe which new assignments emerge."
lab,kmeans.ipynb,code,1,"# one iteration: assign clusters and update centroids
centroids = [centroid0, centroid1]

clusters = assign_clusters(points, centroids)
new_centroids = update_centroids(points, clusters, K=2)

print('Cluster assignments:', clusters)
print('Updated centroids:\n', new_centroids)"
lab,kmeans.ipynb,code,1,"# plot updated centroids and clusters
plt.scatter(points[:, 0], points[:, 1],  c=np.vectorize(colors.get)(clusters))
plt.plot(new_centroids[0][0],new_centroids[0][1],marker='s', c='black', label='Centroid 0') 
plt.plot(new_centroids[1][0],new_centroids[1][1],marker='s', c='red', label='Centroid 1') 
plt.title('Visualization of Two Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,markdown,1,Let's try for another iteration and see what happens.
lab,kmeans.ipynb,code,1,"# second iteration: assign clusters and update centroids
centroids = [centroid0, centroid1]

clusters = assign_clusters(points, centroids)
new_centroids = update_centroids(points, clusters, K=2)

print('Cluster assignments:', clusters)
print('Updated centroids:\n', new_centroids)"
lab,kmeans.ipynb,code,1,"# plot updated centroids and clusters
plt.scatter(points[:, 0], points[:, 1],  c=np.vectorize(colors.get)(clusters))
plt.plot(new_centroids[0][0],new_centroids[0][1],marker='s', c='black', label='Centroid 0') 
plt.plot(new_centroids[1][0],new_centroids[1][1],marker='s', c='red', label='Centroid 1') 
plt.title('Visualization of Two Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,markdown,1,"It seems like we reached stopped cluster assignment! We can state that these are our ""best"" cluster assignments if we assume only 2 clusters. However...
* How do we know if k=2 is the best cluster number?
* What if we changed our random initial assignment? Would that result in different clusters?"
lab,kmeans.ipynb,code,1,"# set the number of clusters
k = 3

# randomly assign a cluster for each data point
np.random.seed(42) 
clusters = np.random.randint(0, k, size=points.shape[0])

# get clusters
cluster0 = points[clusters == 0]
cluster1 = points[clusters == 1]
cluster2 = points[clusters == 2]

# get centroids
centroid0 = cluster0.mean(axis=0)
centroid1 = cluster1.mean(axis=0)
centroid2 = cluster2.mean(axis=0)

centroids = [centroid0, centroid1, centroid2]

# Create a scatter plot to visualize these clusters
colors = {0:'tab:blue', 1:'tab:orange', 2:'tab:green'}

plt.scatter(points[:, 0], points[:, 1],  c=np.vectorize(colors.get)(clusters))
plt.plot(centroids[0][0],centroids[0][1],marker='s', c='black', label="
lab,kmeans.ipynb,code,2,"'Centroid 0') 
plt.plot(centroids[1][0],centroids[1][1],marker='s', c='red', label='Centroid 1') 
plt.plot(centroids[2][0],centroids[2][1],marker='s', c='purple', label='Centroid 2') 
plt.title('Visualization of Three Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,code,1,"# iteration 1
clusters = assign_clusters(points, centroids)
new_centroids = update_centroids(points, clusters, K=3)

print('Cluster assignments:', clusters)
print('Updated centroids:\n', new_centroids)"
lab,kmeans.ipynb,code,1,"plt.scatter(points[:, 0], points[:, 1],  c=np.vectorize(colors.get)(clusters))
plt.plot(new_centroids[0][0],new_centroids[0][1],marker='s', c='black', label='Centroid 0') 
plt.plot(new_centroids[1][0],new_centroids[1][1],marker='s', c='red', label='Centroid 1') 
plt.plot(new_centroids[2][0],new_centroids[2][1],marker='s', c='purple', label='Centroid 2') 
plt.title('Visualization of Three Clusters')

plt.xlim([0, 70])
plt.ylim([0, 70])
plt.xlabel('noise_dB')
plt.ylabel('kibble_grams')
plt.legend()
plt.show()"
lab,kmeans.ipynb,markdown,1,"This code executes a single iteration of K-Means, where data points are first assigned to the nearest centroid and then new centroids are calculated based on these assignments.

Challenge: Integrate these steps inside a loop to run multiple iterations until convergence, or try varying the number of clusters."
lab,kmeans.ipynb,markdown,1,"## Determining the Optimal Number of Clusters

In this section we explore how to evaluate clustering performance. The elbow method assists in selecting the optimal number of clusters by analyzing the inertia (sum of squared distances)."
lab,kmeans.ipynb,markdown,1,### Elbow Plot
lab,kmeans.ipynb,code,1,"from sklearn.cluster import KMeans

inertias = []
ks = range(1, 10)
for k in ks:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(points)
    inertias.append(kmeans.inertia_)
    print('K:', k, 'Inertia:', kmeans.inertia_)"
lab,kmeans.ipynb,markdown,1,This block uses Scikit-Learn's KMeans to compute and print the inertia for cluster counts ranging from 1 to 4. Observing the inertia values helps in determining the 'elbow' point beyond which additional clusters do not significantly reduce the error.
lab,kmeans.ipynb,code,1,"plt.figure(figsize=(6, 4))
plt.plot(list(ks), inertias, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()"
lab,kmeans.ipynb,markdown,1,"The plot generated above shows the relationship between the number of clusters and the corresponding inertia. An 'elbow' in this plot suggests a good choice for k.

Here we could state an ""elbow"" exists at k=3 or k=4."
lab,kmeans.ipynb,markdown,1,"### Silhouette Analysis

We can also view the silhouette score of our clusters to determine how 'tightly grouped' together clusters are. A greater value indicates better clusters, while values close to -1 indicates poorly created clusters."
lab,kmeans.ipynb,code,1,"from sklearn.metrics import silhouette_score

silhouette_scores = []
ks = range(2, 10)

for k in ks:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(points)

    labels = kmeans.labels_
    score = silhouette_score(points, labels)
    silhouette_scores.append(score)

    print('K:', k, 'Silhouette:', score)"
lab,kmeans.ipynb,code,1,"plt.plot(ks, silhouette_scores, marker='o')
plt.title('Silhouette Scores vs Number of Clusters')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()"
lab,kmeans.ipynb,markdown,1,Most measures of cluster tightness point to around the same number of clusters. Note that our silhouette score points to an optimal amount of clusters at `k=2` or `k=3`.
lab,kmeans.ipynb,markdown,1,"### Gap Statistic

Again, there are around 30 metrics to choose from when evaluating best clusters. We'll stop at 3.

The last metric is the gap statistic. Think of this as a hypothesis test where we measure the difference between the actual WSS & the expected WSS under the null distribution of our data. Our goal here is to find the smallest `k` such that we have the largest gap statistic.

Unfortunately there is no default gap statistic method in sklearn, we have to pull code from Kaggle: https://www.kaggle.com/code/mallikarjunaj/gap-statistics"
lab,kmeans.ipynb,code,1,"def gap_stat(data, maxClusters):
    """"""
    Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie
    Params:
        data: ndarry of shape (n_samples, n_features)
        nrefs: number of sample reference datasets to create
        maxClusters: Maximum number of clusters to test for
    Returns: (gaps)
    """"""
    nrefs=3
    gaps = np.zeros((len(range(1, maxClusters)),))
    resultsdf = []
    for gap_index, k in enumerate(range(1, maxClusters)):

        # Holder for reference dispersion results
        refDisps = np.zeros(nrefs)

        # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop
        for i in ran"
lab,kmeans.ipynb,code,2,"ge(nrefs):
            
            # Create new random reference set
            randomReference = np.random.random_sample(size=data.shape)
            
            # Fit to it
            km = KMeans(k)
            km.fit(randomReference)
            
            refDisp = km.inertia_
            refDisps[i] = refDisp
            
        # Fit cluster to original data and create dispersion
        km = KMeans(k)
        km.fit(data)
        
        origDisp = km.inertia_

        # Calculate gap statistic
        gap = np.log(np.mean(refDisps)) - np.log(origDisp)

        # Assign this loop's gap statistic to gaps
        gaps[gap_index] = gap
        
        resultsdf.append({'clusterC"
lab,kmeans.ipynb,code,3,"ount':k, 'gap':gap})

    return resultsdf"
lab,kmeans.ipynb,code,1,"gap_stats = gap_stat(points, maxClusters=10)
gap_stats"
lab,kmeans.ipynb,code,1,"ks = [g[""clusterCount""] for g in gap_stats]
gap_measures = [g[""gap""] for g in gap_stats]

plt.plot(ks, gap_measures, marker='o')
plt.title('Gap Statistic vs Number of Clusters')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Gap Statistic')
plt.grid(True)
plt.show()"
lab,kmeans.ipynb,markdown,1,Our gap statistic points to an optimal amount of clusters at `k=2`.
lab,kmeans.ipynb,markdown,1,"## Application to a Realistic Dataset: Music Features

Here we apply the K-Means algorithm to a realistic dataset containing music track features. This exercise simulates a real-world scenario where clustering can aid in tasks like recommendation system development."
lab,kmeans.ipynb,code,1,"import pandas as pd

# Load the music features dataset (ensure 'sample_music.csv' is in the working directory)
songs = pd.read_csv('sample_music.csv')

songs.sample(n=5)"
lab,kmeans.ipynb,markdown,1,"The code above reads the music features dataset from a CSV file and prints the first five rows so you can inspect its structure.

Let's explore this dataset through some light EDA."
lab,kmeans.ipynb,code,1,"# TODO: Find out what differentiates songs
..."
lab,kmeans.ipynb,code,1,"import seaborn as sns

sns.scatterplot(songs, x=""sadness"", y=""dating"", hue=""genre"")"
lab,kmeans.ipynb,markdown,1,Let's apply the same Kmeans steps to our dataset of actual music to check if we can find clusters.
lab,kmeans.ipynb,code,1,songs.columns
lab,kmeans.ipynb,code,1,"# Select numeric features assumed to represent music track attributes
predictor_cols = ['dating', 'violence', 'world/life', 'night/time', 'shake the audience',
       'family/gospel', 'romantic', 'communication', 'obscene', 'music',
       'movement/places', 'light/visual perceptions', 'family/spiritual',
       'sadness', 'feelings']
X = songs[predictor_cols]
# Look! No Y!

# Apply K-Means clustering with k=3 on the selected features
kmeans_music = KMeans(n_clusters=3, random_state=42)
kmeans_music.fit(X)"
lab,kmeans.ipynb,markdown,1,Let's see the clusters we generate for a sample of 10 unlabeled songs.
lab,kmeans.ipynb,code,1,"sample_songs = songs.sample(10)

selected_predictors = sample_songs[predictor_cols]
sample_labels = kmeans_music.predict(selected_predictors)"
lab,kmeans.ipynb,code,1,"sample_songs[""predicted_class""] = sample_labels

sample_songs[[""artist_name"", ""track_name"", ""predicted_class""]]"
lab,kmeans.ipynb,markdown,1,"What do you think about these labels? Did it seem like they cluster songs well?

If not, let's try out different cluster labels instead."
lab,kmeans.ipynb,code,1,"# Evaluate clustering quality using the elbow method on the music dataset
inertias_music = []
ks_music = range(1, 10)

for k in ks_music:
    km = KMeans(n_clusters=k, random_state=42).fit(X)
    inertias_music.append(km.inertia_)
    print('K:', k, 'Inertia:', km.inertia_)"
lab,kmeans.ipynb,code,1,"plt.figure(figsize=(6, 4))
plt.plot(list(ks_music), inertias_music, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()"
lab,kmeans.ipynb,code,1,"silhouette_scores = []
ks = range(2, 10)

for k in ks:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)

    labels = kmeans.labels_
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)

    print('K:', k, 'Silhouette:', score)

plt.plot(ks, silhouette_scores, marker='o')
plt.title('Silhouette Scores vs Number of Clusters')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()"
lab,kmeans.ipynb,markdown,1,"## Practical Considerations and Extended Concepts

In our final section, we discuss the importance of feature scaling and other practical considerations that impact clustering outcomes."
lab,kmeans.ipynb,code,1,"from sklearn.preprocessing import StandardScaler

# Standardize the numeric features so that each has mean=0 and variance=1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print('First five rows of scaled features:')
print(X_scaled[:5])"
lab,kmeans.ipynb,code,1,"kmeans_music = KMeans(n_clusters=3, random_state=42)
kmeans_music.fit(X_scaled)"
lab,kmeans.ipynb,code,1,"scaled_labels = kmeans_music.predict(selected_predictors)

sample_songs[""scaled_labels""] = scaled_labels

sample_songs[[""artist_name"", ""track_name"", ""scaled_labels""]]"
lab,kmeans.ipynb,code,1,"# Evaluate clustering quality using the elbow method on the music dataset
inertias_music = []
ks_music = range(1, 10)

for k in ks_music:
    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)
    inertias_music.append(km.inertia_)
    print('K:', k, 'Inertia:', km.inertia_)

plt.figure(figsize=(6, 4))
plt.plot(list(ks_music), inertias_music, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()"
lab,kmeans.ipynb,code,1,"silhouette_scores = []
ks = range(2, 10)

for k in ks:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_scaled)

    labels = kmeans.labels_
    score = silhouette_score(X_scaled, labels)
    silhouette_scores.append(score)

    print('K:', k, 'Silhouette:', score)

plt.plot(ks, silhouette_scores, marker='o')
plt.title('Silhouette Scores vs Number of Clusters')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()"
lab,kmeans.ipynb,markdown,1,"This code applies standard scaling to the numeric features, ensuring that all features contribute equally to distance measurements in K-Means. Scaling is a crucial step in preparing real-world data for clustering."
lab,kmeans.ipynb,markdown,1,"## Lab Conclusion

Throughout this lab, you built a K-Means clustering model from its basic components, starting from distance calculations and centroid updates through to applying the method on a real-world dataset. Review these code-blocks for your upcoming TLAB #3."
lab,lab.ipynb,markdown,1,"## Introduction to Unsupervised Learning

In this section we introduce unsupervised learning—a process where data is explored without predefined labels. Unsupervised learning is applied in customer segmentation, content recommendation, and many other fields. The examples in this lab follow the lab plan outlined in the attached lab plan document fileciteturn0file0."
lab,lab.ipynb,code,1,print('Welcome to the Unsupervised Learning Lab!')
lab,lab.ipynb,markdown,1,"The code above prints a welcome message to introduce the lab. 

Challenge: Modify the printed message to include additional details about unsupervised learning."
lab,lab.ipynb,markdown,1,"## Exploring the K-Means Clustering Algorithm

K-Means clustering partitions data into groups by iteratively assigning data points to the closest centroid and recalculating centroids using squared Euclidean distance. This section uses a simple toy dataset to illustrate these concepts, as described in the lab plan fileciteturn0file0."
lab,lab.ipynb,code,1,"import numpy as np

# Define a toy dataset of 2D points
X = np.array([[1, 2], [1, 4], [2, 3], [5, 8], [6, 9], [5, 6]])

# Set the number of clusters
k = 2

# Randomly assign a cluster for each data point
np.random.seed(42)  # for reproducibility
assignments = np.random.randint(0, k, size=X.shape[0])
print('Initial cluster assignments:', assignments)"
lab,lab.ipynb,markdown,1,"The code defines a small toy dataset and randomly assigns each point to one of two clusters. The use of a fixed random seed ensures consistent results upon each run. 

Challenge: Experiment with a different random seed or change the number of clusters to observe how the initial assignments vary."
lab,lab.ipynb,code,1,"def squared_distance(a, b):
    """"""Compute the squared Euclidean distance between two vectors.""""""
    return np.sum((a - b) ** 2)

# Calculate the squared distance between the first two points
dist = squared_distance(X[0], X[1])
print('Squared distance between first two points:', dist)"
lab,lab.ipynb,markdown,1,"The function above computes the squared Euclidean distance between two vectors. In the example, it calculates the distance between the first two points of the toy dataset. 

Challenge: Enhance the function by adding a print statement that shows the individual differences between vector elements during the computation."
lab,lab.ipynb,code,1,"# Compute centroids based on initial cluster assignments
centroids = []
for i in range(k):
    cluster_points = X[assignments == i]
    centroids.append(np.mean(cluster_points, axis=0))
centroids = np.array(centroids)
print('Computed centroids:', centroids)"
lab,lab.ipynb,markdown,1,"The above code calculates centroids by computing the mean of the data points assigned to each cluster. The resulting centroids are then printed. 

Challenge: Modify the centroid computation to use the median of the points instead of the mean, and compare the outcomes."
lab,lab.ipynb,code,1,"import matplotlib.pyplot as plt

# Visualize the toy dataset with the initial cluster assignments
plt.scatter(X[:, 0], X[:, 1], c=assignments, cmap='viridis', label='Data Points')

# Plot the computed centroids
for i, center in enumerate(centroids):
    plt.scatter(center[0], center[1], marker='x', color='red', s=100, label=f'Centroid {i}')

plt.title('Toy Dataset Cluster Visualization')
plt.legend()
plt.show()"
lab,lab.ipynb,markdown,1,"This visualization displays the toy dataset colored by their respective initial cluster assignments, with red 'x' markers indicating the centroids. 

Challenge: Modify the visualization by changing the colormap or marker style, and discuss the effect on your interpretation of the clusters."
lab,lab.ipynb,markdown,1,"## Code-Along Activity: Building K-Means from Scratch

In this section, we build the K-Means algorithm step by step. We begin by defining helper functions to compute cluster assignments and update centroids, and then combine these into the main K-Means function."
lab,lab.ipynb,code,1,"def compute_clusters(X, centroids):
    import numpy as np
    clusters = []
    for x in X:
        distances = [np.sum((x - c) ** 2) for c in centroids]
        clusters.append(np.argmin(distances))
    return np.array(clusters)

# Example usage:
# clusters = compute_clusters(X, centroids)"
lab,lab.ipynb,markdown,1,"The function compute_clusters assigns each data point in X to the closest centroid using squared Euclidean distance. 

Challenge: Modify this function to use an alternative distance metric, such as Manhattan distance."
lab,lab.ipynb,code,1,"def update_centroids(X, clusters, k):
    import numpy as np
    new_centroids = []
    for i in range(k):
        points = X[clusters == i]
        new_centroids.append(np.mean(points, axis=0))
    return np.array(new_centroids)

# Example usage:
# new_centroids = update_centroids(X, assignments, k)"
lab,lab.ipynb,markdown,1,"The update_centroids function recalculates the centroids by finding the mean of data points in each cluster. 

Challenge: Adjust the function to handle empty clusters (for example, by keeping the previous centroid if no points are assigned)."
lab,lab.ipynb,code,1,"def k_means(X, k, max_iter=10):
    import numpy as np
    # Initialize centroids by selecting k random data points
    indices = np.random.choice(len(X), k, replace=False)
    centroids = X[indices]
    
    for iteration in range(max_iter):
        clusters = compute_clusters(X, centroids)
        new_centroids = update_centroids(X, clusters, k)
        
        # Check for convergence using allclose for numerical stability
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return centroids, clusters

# Run K-Means on the toy dataset with a higher iteration limit
centroids_final, clusters_final = k_means(X, k, max_iter=20)
print('Final"
lab,lab.ipynb,code,2," centroids:', centroids_final)
print('Final cluster assignments:', clusters_final)"
lab,lab.ipynb,markdown,1,"The k_means function integrates the helper functions to perform clustering. It initializes centroids, iterates to update cluster assignments and centroids, and stops when the centroids converge. 

Challenge: Re-run the k_means function with a different value of k (for example, k=3) and observe how the final centroids and assignments differ."
lab,lab.ipynb,markdown,1,"## Working with a Real-World Inspired Dataset

In this section we apply our K-Means algorithm to a real-world dataset. The Iris dataset is used with its first two features to facilitate 2D visualization. This practical example mirrors real-world clustering challenges described in the lab plan fileciteturn0file0."
lab,lab.ipynb,code,1,"from sklearn.datasets import load_iris

data = load_iris()
# Extract the first two features (sepal length and sepal width)
X_real = data.data[:, :2]"
lab,lab.ipynb,markdown,1,"The above code loads the Iris dataset and selects its first two features to simplify the clustering visualization. 

Challenge: Try selecting a different pair of features (for example, the last two features) and analyze how the resulting clusters change."
lab,lab.ipynb,code,1,"centroids_real, clusters_real = k_means(X_real, 3, max_iter=20)

import matplotlib.pyplot as plt

plt.scatter(X_real[:, 0], X_real[:, 1], c=clusters_real, cmap='viridis', label='Data Points')
plt.scatter(centroids_real[:, 0], centroids_real[:, 1], marker='x', color='red', s=100, label='Centroids')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('K-Means Clustering on Iris Dataset (first 2 features)')
plt.legend()
plt.show()"
lab,lab.ipynb,markdown,1,"This visualization displays the Iris dataset with k=3 clusters. Data points are colored by cluster assignment and centroids are marked in red. 

Challenge: Experiment with different values of k and describe how the cluster visualization changes."
lab,lab.ipynb,markdown,1,"## Extensions: Filtering Techniques and Practical Applications

Clustering results enable filtering data for applications like collaborative and content-based filtering. In this section, we filter the Iris dataset based on a specific cluster assignment to simulate this process."
lab,lab.ipynb,code,1,"# Filter the Iris dataset for data points assigned to cluster 0
filtered_data = X_real[clusters_real == 0]
print('Data points in Cluster 0:')
print(filtered_data)"
lab,lab.ipynb,markdown,1,"The code filters the dataset to display only those data points that belong to cluster 0. This approach can be extended to various filtering techniques in real-world applications. 

Challenge: Modify the filtering criterion to display data points from a different cluster and analyze the differences."
lab,lab.ipynb,markdown,1,"## Reflection and Professional Relevance

Reflect on the clustering process and its practical implications. Consider the effects of random initialization, choice of k, and feature scaling on the clustering results, as well as how these aspects relate to real-world data science challenges."
lab,lab.ipynb,code,1,"def print_reflections():
    questions = [
        'How does random initialization affect the final clusters?',
        'What changes do you observe when you modify the number of clusters (k)?',
        'How might feature scaling impact the clustering outcome?'
    ]
    for q in questions:
        print(q)

print_reflections()"
lab,lab.ipynb,markdown,1,"The function above prints a set of reflective questions to help you consider the sensitive aspects of the K-Means algorithm, such as initialization and convergence criteria. 

Challenge: Extend the list with an additional question regarding the impact of convergence tolerance on algorithm performance."
lab,predict_rental.ipynb,markdown,1,"## Modeling

Lastly, let's go through the data prediction steps:

* Are we attempting to solve a regression or classification task?
* Which data transformation steps can we apply given our analytical outcomes?
* How should we measure error given our prediction task?
* What other models can we try out?"
lab,predict_rental.ipynb,code,1,"from sklearn.ensemble import RandomForestRegressor

# regressor models
from sklearn.linear_model import Lasso, Ridge
from sklearn.linear_model import LinearRegression

# accuracy metrics
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np"
lab,predict_rental.ipynb,code,1,"rentals = pd.read_csv(""transformed_rentals.csv"")
rentals.head()"
lab,predict_rental.ipynb,code,1,"# train a basic Linear Regression model
..."
lab,predict_rental.ipynb,markdown,1,"## Iteratively Choosing Predictors

It seems like we're coming to a minumum MSE when it comes to generating predictions on all of our predictors. Let's choose only the most **important** predictors and see if it leads to better predictive capabilities."
lab,predict_rental.ipynb,code,1,...
lab,transform_rental.ipynb,markdown,1,"## Data Preperation

Now that we've performed data exploration, let's perform our data transformation steps:

* Clean up missing or malformed entries
* Consider applying filtering or normalization techniques when dealing with extreme values
* Convert text based or categorical fields into numeric formats

Let's use the insights we've gained from our EDA to inform our transformations. Don't hesitate to regenerate data visualizations as you make data transformations!"
lab,transform_rental.ipynb,markdown,1,"## Data Clean-Up

Let's start out by dropping non-predictive columns. From there, we will move to missing & outlier values."
lab,transform_rental.ipynb,code,1,"import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt"
lab,transform_rental.ipynb,code,1,"rentals = pd.read_csv(""apartments_for_rent.csv"")
rentals.head()"
lab,transform_rental.ipynb,code,1,...
lab,transform_rental.ipynb,markdown,1,"## Data Transformations

Let's apply data encoding & feature engineering principles as well."
lab,transform_rental.ipynb,code,1,...
lab,fashion-mnist.ipynb,markdown,1,"# Fashion Detection

You are a senior data scientist at a LA-based online store. Your CEO has come up with an eccentric idea to analyze upcoming fashion trends not through marketplace analytics, nor through advertisement analysis.

Instead, they plan to release a swarm of drones above the streets of Los Angeles and utilize computer vision to recognize the kinds of clothes people are wearing. Based on this insight, your company plans to make reactive changes to marketplace desires & trends.

You are tasked with generating a few machine learning models to test out the validity of such a model. Namely you will create:
* A kNN model with PCA pre-processing
* A feed-forward neural network in Kera"
lab,fashion-mnist.ipynb,markdown,2,s
lab,fashion-mnist.ipynb,code,1,"import numpy as np

import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Flatten

from tensorflow.keras.datasets import fashion_mnist"
lab,fashion-mnist.ipynb,code,1,"# load in your shopping dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
lab,fashion-mnist.ipynb,code,1,"# TODO: view the shape of your training data
..."
lab,fashion-mnist.ipynb,code,1,"# TODO: view the content of your first row
..."
lab,fashion-mnist.ipynb,code,1,"# TODO:count the number of unique classes in your target variable
np.unique_counts(...)"
lab,fashion-mnist.ipynb,code,1,"# TODO: view one training example
plt.imshow(..., cmap='Blues')
plt.title(f""Clothing: {y_train[0]}"")
plt.axis('off')
plt.show()"
lab,fashion-mnist.ipynb,code,1,"# helper function to view images of clothes
def plot_gallery(title, images, n_col=4, n_row=5):
    fig, axs = plt.subplots(
        nrows=n_row,
        ncols=n_col,
        figsize=(2.0 * n_col, 2.3 * n_row),
        constrained_layout=True,
    )
    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)
    fig.set_edgecolor(""black"")
    fig.suptitle(title, size=16)

    for ax, vec in zip(axs.flat, images):
        im = ax.imshow(
            vec.reshape((28, 28)),
            cmap='Blues',
            interpolation=""nearest""
        )
        ax.axis(""off"")

    fig.colorbar(im, ax=axs, orientation=""horizontal"", shrink=0.99, aspect=40, pad=0.01)
    plt.show()

plot"
lab,fashion-mnist.ipynb,code,2,"_gallery(""Sample Training from Fashion MNIST"", x_train)"
lab,fashion-mnist.ipynb,markdown,1,"## kNN + PCA Implementation 

Utilize the PCA algorithm to decompose your highly dimensional dataset into fewer components and then utilize kNN to check if you've created a sufficient model."
lab,fashion-mnist.ipynb,code,1,"from sklearn.decomposition import PCA

import seaborn as sns

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

from sklearn.model_selection import train_test_split, RandomizedSearchCV"
lab,fashion-mnist.ipynb,code,1,"# flatten your data files
x_train_flat = x_train.reshape(x_train.shape[0], -1)
x_test_flat = x_test.reshape(x_test.shape[0], -1)

print(""new x_train shape"", x_train_flat.shape)
print(""new x_test shape"", x_test_flat.shape)"
lab,fashion-mnist.ipynb,code,1,"# visualize your flattened data
plt.figure(figsize=(18, 1))
plt.imshow(x_train_flat[0].reshape(1, -1), cmap='Blues', aspect='auto')
plt.title(""Flattened Image (1x784)"")
plt.xlabel(""Pixel index"")
plt.yticks([])
plt.tight_layout()
plt.show()"
lab,fashion-mnist.ipynb,code,1,"# TODO: initialize PCA to reduce your flattened data to 8 components for visualization
pca_estimator = ...

pca_estimator.fit(...)
plot_gallery(""Eigenclothes on 8 Components"", pca_estimator.components_, 4, 2)"
lab,fashion-mnist.ipynb,code,1,"# TODO: create 50 components using PCA
pca = ...

# TODO: convert our training & testing predictors variables to 100 basic components
X_train_pca = ...
X_test_pca = ...

X_train_pca.shape"
lab,fashion-mnist.ipynb,code,1,"# TODO: implement random-search on the knn model to find best hyperparams
params = {
    ""n_neighbors"": range(5, 50, 5),
    ""weights"": [""uniform"", ""distance""],
    ""metric"": [""cityblock"", ""cosine"", ""euclidean"", ""minkowski""]
}

knn = ...

# TODO: set up RandomizedSearchCV with 5-fold cross-validation
random_search = ...

# TODO: fit this model on your PCA training data
..."
lab,fashion-mnist.ipynb,code,1,"# TODO: extract the best estimator
best_knn = ...

# TODO: predict on testing data
yhat = ...

# TODO: evaluate its accuracy
confusion_mat = confusion_matrix(..., ...)
class_report = classification_report(..., ...)
accuracy = accuracy_score(..., ...)

print(""Accuracy Score\n"",accuracy)
print(""Confusion Matrix\n"", confusion_mat)
print(""Classification Report\n"", class_report)"
lab,fashion-mnist.ipynb,code,1,"n_samples_to_plot = 16

# Randomly sample indices from the test set
sample_idxs = np.random.choice(len(y_test), n_samples_to_plot, replace=False)

plt.figure(figsize=(12, 8))
for i, idx in enumerate(sample_idxs):
    img = x_test[idx]                      # original image (28x28)
    true_label = y_test[idx]
    pred_label = yhat[idx]

    # Subplot setup
    plt.subplot(4, 4, i + 1)
    plt.imshow(img, cmap='gray')
    title_color = 'green' if true_label == pred_label else 'red'
    plt.title(f""True: {true_label}\nPred: {pred_label}"", 
              color=title_color, fontsize=10)
    plt.axis('off')

plt.suptitle(""Fashion MNIST – Random Predictions"", fontsize=16)
plt.tight_layout()
plt.sho"
lab,fashion-mnist.ipynb,code,2,w()
lab,fashion-mnist.ipynb,markdown,1,## Keras Implementation
lab,fashion-mnist.ipynb,code,1,"# standardize your data before running your model
x_train_flat_norm = x_train_flat / 255.0
x_test_flat_norm = x_test_flat / 255.0"
lab,fashion-mnist.ipynb,code,1,"# TODO: Create a sequential model with at least 3 layers.
# Input layer: An input object with as many nodes as predictors
# Dense layer: a hidden layer with `relu` or `tanh` activation function
# Out layer: an output layer with as many nodes as classes and a `softmax` activation function
model_clothes = ..."
lab,fashion-mnist.ipynb,code,1,"# TODO: compile your model with the 'adam' optimizer, 'sparse_categorical_crossentropy' loss and 'accuracy' for metrics
model_clothes.compile(...)

# TODO: fit your model for 100 models and 20% validation data
model_clothes.fit(...)"
lab,fashion-mnist.ipynb,code,1,"# measure the accuract of your model on your test set
loss, accuracy = model_clothes.evaluate(x_test_flat_norm, y_test, verbose=0)
print(f""Loss: {loss}, Test Accuracy: {accuracy}"")"
lab,fashion-mnist.ipynb,code,1,"# generate predictions for later evaluation
yhat = model_clothes.predict(x_test_flat_norm)

predicted_classes = np.argmax(yhat, axis=1)
predicted_classes"
lab,fashion-mnist.ipynb,code,1,"n_samples_to_plot = 16

# Randomly sample indices from the test set
sample_idxs = np.random.choice(len(y_test), n_samples_to_plot, replace=False)

plt.figure(figsize=(12, 8))
for i, idx in enumerate(sample_idxs):
    img = x_test[idx]                      # original image (28x28)
    true_label = y_test[idx]
    pred_label = predicted_classes[idx]

    # Subplot setup
    plt.subplot(4, 4, i + 1)
    plt.imshow(img, cmap='Blues')
    title_color = 'green' if true_label == pred_label else 'red'
    plt.title(f""True: {true_label}\nPred: {pred_label}"", 
              color=title_color, fontsize=10)
    plt.axis('off')

plt.suptitle(""Fashion MNIST – Random Predictions"", fontsize=16)
plt.tight_la"
lab,fashion-mnist.ipynb,code,2,"yout()
plt.show()"
lab,fashion-mnist.ipynb,code,1,"# extract weights & balances
weights, biases = model_clothes.layers[0].get_weights()

print(""weights shape:"", weights.shape)  # (784, 128)
print(""biases shape:"", biases.shape)    # (128,)"
lab,fashion-mnist.ipynb,code,1,"# Visualize the first 20 neurons' weight vectors as 64x64 images
fig, axes = plt.subplots(4, 5, figsize=(15, 6))

for i, ax in enumerate(axes.flat):
    weight_vector = weights[:, i]
    weight_image = weight_vector.reshape((28, 28))  # reshape from 4096 to 64x64

    ax.imshow(weight_image, cmap='Blues')
    ax.set_title(f'Neuron {i}')
    ax.axis('off')

plt.suptitle(""Visualizations of First Layer Weights"")
plt.tight_layout()
plt.show()"
lab,fashion-mnist.ipynb,code,1,"# serialize your model
model_clothes.save(""model.keras"")"
lab,fashion-mnist.ipynb,markdown,1,"To run your clothing prediction algorithm in streamlit, install the dependencies below in your `ds` environment and then run `streamlit run cloth-predict.py` in your terminal!"
lab,fashion-mnist.ipynb,code,1,!pip install streamlit opencv-python pillow
lab,linear_algebra.ipynb,markdown,1,"# Linear Algebra Foundations

Understanding linear algebra is essential for grasping how machine learning especially neural networks operate. Here’s a concise review of key concepts:

--

## Vectors and Matrices

A **vector** is an ordered list of numbers, often written as a column or row. Formally, an $n$-dimensional vector is:

$$
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$

**Examples:**

- 2D vector:  
    $$
    \mathbf{a} = \begin{bmatrix} 5 \\ 7 \end{bmatrix}
    $$
- 3D vector:  
    $$
    \mathbf{b} = \begin{bmatrix} 2 \\ -1 \\ 4 \end{bmatrix}
    $$
- 4D vector:  
    $$
    \mathbf{c} = \begin{bmatrix} 0 \\ 3 \\ 8 \\ -2 \end{bmatrix}
    $$

Vectors "
lab,linear_algebra.ipynb,markdown,2,"can represent features, inputs, or outputs in neural networks, with the dimension corresponding to the number of features or neurons. Vectors and matrices are the building blocks of linear algebra and are fundamental to machine learning and neural networks.

- **Vectors** are ordered lists of numbers, often representing features, data points, or weights. For example, a vector can represent the input features to a neural network layer:
  
  $$
  \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
  $$
  
  In code, a vector might look like `np.array([5, 7])` for a 2D vector, or `np.array([2, -1, 4])` for a 3D vector.

- **Matrices** are rectangular arrays of numbers arrange"
lab,linear_algebra.ipynb,markdown,3,"d in rows and columns. They are used to organize data and perform transformations. In neural networks, the weights connecting layers are typically stored in matrices. For example, a matrix $A$ with shape $2 \times 3$:
  
  $$
  A = \begin{bmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6
  \end{bmatrix}
  $$
  
  In code, this is represented as `np.array([[1, 2, 3], [4, 5, 6]])`.

**Types of matrices commonly used:**
- **Square matrix:** Same number of rows and columns (e.g., $2 \times 2$).
  
  $$
  S = \begin{bmatrix}
  7 & 8 \\
  9 & 10
  \end{bmatrix}
  $$

- **Identity matrix:** Diagonal elements are 1, others are 0. Acts as a multiplicative identity.
  
  $$
  I = \begin{bmatrix}
  1 & 0 \\
  0 & 1
"
lab,linear_algebra.ipynb,markdown,4,"  \end{bmatrix}
  $$

- **Zero matrix:** All elements are zero.
  
  $$
  Z = \begin{bmatrix}
  0 & 0 \\
  0 & 0
  \end{bmatrix}
  $$

- **Diagonal matrix:** Only diagonal elements are nonzero.
  
  $$
  D = \begin{bmatrix}
  3 & 0 \\
  0 & 5
  \end{bmatrix}
  $$

**Why they matter:**
- Vectors represent data, weights, and activations.
- Matrices organize weights and data batches, and enable efficient computation through matrix multiplication.
- Understanding their properties (such as shape, transpose, and special types) is crucial for implementing and debugging neural networks.

- **Vectors** represent data points or features. In neural networks, inputs, weights, and outputs are often expre"
lab,linear_algebra.ipynb,markdown,5,"ssed as vectors.
- **Matrices** are used to organize collections of vectors and perform transformations. For example, the weights connecting layers in a neural network are typically stored in matrices.

---"
lab,linear_algebra.ipynb,markdown,1,"## Norm

### Norm

The norm of a vector or matrix is a measure of its ""size"" or ""length."" The most common is the Euclidean (L2) norm, which is the square root of the sum of the squares of the elements.

#### Example: Vector Norm

For a vector $\mathbf{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, the L2 norm is:

$$
\|\mathbf{u}\|_2 = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14} \approx 3.74
$$

#### Example: Matrix Norm

For a matrix $A$, the Frobenius norm is:

$$
\|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}
$$

For

$$
A = \begin{bmatrix}
1 & 2 & 3 \\
3 & 4 & 5
\end{bmatrix}
$$

the Frobenius norm is:

$$
\|A\|_F = \sqrt{1^2 + 2^2 + 3^2 + 3^2 + 4^2 + 5^2} = \sqrt{1 + 4 + 9 + 9 + 16 + 25} = \sqrt{64} ="
lab,linear_algebra.ipynb,markdown,2," 8
$$

**Interpretation:**  
- Norms are used to measure distances, regularize weights (prevent overfitting), and quantify errors in neural networks.  
- L1 norm (sum of absolute values) and L2 norm (Euclidean) are common in machine learning.

---"
lab,linear_algebra.ipynb,markdown,1,"## Element-wise Operations

In linear algebra and neural networks, **element-wise operations** refer to performing a specific operation on each individual element of a vector or matrix. These operations are fundamental for data transformation and are heavily used in tasks like normalization and activation functions.

### Scalars and Broadcasting

A **scalar** is a single numerical value. When a scalar is added to, subtracted from, multiplied by, or divided with a vector or matrix, the operation is applied **individually to each element** — a behavior known as **broadcasting**.

For example, multiplying a vector by a scalar:

$$
2 \cdot \begin{bmatrix} 1 \\\\ 2 \\\\ 3 \end{bmatrix} =
\begin{b"
lab,linear_algebra.ipynb,markdown,2,"matrix} 2 \cdot 1 \\\\ 2 \cdot 2 \\\\ 2 \cdot 3 \end{bmatrix} =
\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \end{bmatrix}
$$

Broadcasting is commonly used to **scale** or **shift** data, such as during normalization or when applying a bias term.

#### Example: Element-wise Vector Operations

Let the vectors:

$$
\mathbf{u} = \begin{bmatrix} 1 \\\\ 2 \\\\ 3 \end{bmatrix}, \quad
\mathbf{v} = \begin{bmatrix} 4 \\\\ 5 \\\\ 6 \end{bmatrix}
$$

#### 1. **Addition**

Each corresponding pair of elements is added:

$$
\mathbf{u} + \mathbf{v} =
\begin{bmatrix} 1 + 4 \\\\ 2 + 5 \\\\ 3 + 6 \end{bmatrix} =
\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \end{bmatrix}
$$

#### 2. **Element-wise Multiplication** (Hadamard Product)

"
lab,linear_algebra.ipynb,markdown,3,"Each element is multiplied with the corresponding element of the other vector:

$$
\mathbf{u} \circ \mathbf{v} =
\begin{bmatrix} 1 \cdot 4 \\\\ 2 \cdot 5 \\\\ 3 \cdot 6 \end{bmatrix} =
\begin{bmatrix} 4 \\\\ 10 \\\\ 18 \end{bmatrix}
$$

#### 3. **Applying a Function (e.g., Squaring)**

Functions such as square, square root, ReLU, sigmoid, or tanh are often applied element-wise:

$$
\mathbf{u}^2 =
\begin{bmatrix} 1^2 \\\\ 2^2 \\\\ 3^2 \end{bmatrix} =
\begin{bmatrix} 1 \\\\ 4 \\\\ 9 \end{bmatrix}
$$

This principle applies to **activation functions** in neural networks, where functions like ReLU, sigmoid, or tanh are applied to each neuron output individually."
lab,linear_algebra.ipynb,markdown,1,"## Vector-on-Vector Element-wise Operations

Element-wise operations between vectors apply a binary operation (such as addition or multiplication) to each corresponding pair of elements from two vectors of the same size. These are also called **Hadamard operations** when referring to multiplication.

Let the vectors be:

$$
\mathbf{u} = \begin{bmatrix} 1 \\\\ 2 \\\\ 3 \end{bmatrix}, \quad
\mathbf{v} = \begin{bmatrix} 4 \\\\ 5 \\\\ 6 \end{bmatrix}
$$


1. **Element-wise Addition**


Each corresponding pair of elements is added:

$$
\mathbf{u} + \mathbf{v} =
\begin{bmatrix}
1 + 4 \\\\
2 + 5 \\\\
3 + 6
\end{bmatrix}
=
\begin{bmatrix}
5 \\\\
7 \\\\
9
\end{bmatrix}
$$


2. **Element-wise Multipli"
lab,linear_algebra.ipynb,markdown,2,"cation** (Hadamard Product which is NOT the dot product)


Each element of $\mathbf{u}$ is multiplied by the corresponding element of $\mathbf{v}$:

$$
\mathbf{u} \circ \mathbf{v} =
\begin{bmatrix}
1 \cdot 4 \\\\
2 \cdot 5 \\\\
3 \cdot 6
\end{bmatrix}
=
\begin{bmatrix}
4 \\\\
10 \\\\
18
\end{bmatrix}
$$

3. **Element-wise Function Application (e.g., Squaring $\mathbf{u}$)**


You can also apply a function to each element of a single vector independently:

$$
\mathbf{u}^2 =
\begin{bmatrix}
1^2 \\\\
2^2 \\\\
3^2
\end{bmatrix}
=
\begin{bmatrix}
1 \\\\
4 \\\\
9
\end{bmatrix}
$$

---

These operations are fundamental in machine learning and neural networks, where transformations are often applied"
lab,linear_algebra.ipynb,markdown,3," element-wise to vectors or matrices (e.g., during activation, loss computation, or feature scaling)."
lab,linear_algebra.ipynb,markdown,1,"## Dot Product

The **dot product** (also called the **inner product**) of two vectors produces a **scalar** — a single number — by multiplying corresponding elements and summing the results. It is written using a **dot** notation: $\mathbf{a} \cdot \mathbf{b}$.

### Dot Product Formula

Given two vectors:

$$
\mathbf{a} = \begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \end{bmatrix}, \quad
\mathbf{b} = \begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \end{bmatrix}
$$

The dot product is:

$$
\mathbf{a} \cdot \mathbf{b} =
\begin{bmatrix}
a_1 \\\\
a_2 \\\\
a_3
\end{bmatrix}
\cdot
\begin{bmatrix}
b_1 \\\\
b_2 \\\\
b_3
\end{bmatrix}
$$

$$
\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3
$$

In general:

$"
lab,linear_algebra.ipynb,markdown,2,"$
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i
$$

### Example: Dot Product Calculation

Let:

$$
\mathbf{a} = \begin{bmatrix} 2 \\\\ 5 \\\\ 7 \end{bmatrix}, \quad
\mathbf{b} = \begin{bmatrix} 3 \\\\ 4 \\\\ 1 \end{bmatrix}
$$

Compute:

$$
\mathbf{a} \cdot \mathbf{b} = (2 \cdot 3) + (5 \cdot 4) + (7 \cdot 1) = 6 + 20 + 7 = 33
$$

**Result:**
$$
\mathbf{a} \cdot \mathbf{b} = 33
$$

This is a **scalar**, not a vector."
lab,linear_algebra.ipynb,markdown,1,"### Matrix Multiplication (Dot Product Generalized)

The **dot product** of vectors produces a scalar. When generalized to matrices, this becomes **matrix multiplication**, where each entry of the result is a dot product between a **row** of the first matrix and a **column** of the second matrix.

Let:

$$
A = \begin{bmatrix}
1 & 2 & 3 \\\\
4 & 5 & 6 \\\\
7 & 8 & 9
\end{bmatrix}, \quad
B = \begin{bmatrix}
9 & 8 & 7 \\\\
6 & 5 & 4 \\\\
3 & 2 & 1
\end{bmatrix}
$$

To compute the matrix product $C = AB$, each element $C_{ij}$ is calculated as the **dot product** of the $i$-th row of $A$ and the $j$-th column of $B$:

$$
C_{ij} = \sum_{k=1}^{3} A_{ik} \cdot B_{kj}
$$

---

#### Step-by-Step Exam"
lab,linear_algebra.ipynb,markdown,2,"ple

We compute each element of $C = AB$:

- **First row of A** $\cdot$ **First column of B**:
  $$
  C_{11} = (1 \cdot 9) + (2 \cdot 6) + (3 \cdot 3) = 9 + 12 + 9 = 30
  $$

- **First row of A** $\cdot$ **Second column of B**:
  $$
  C_{12} = (1 \cdot 8) + (2 \cdot 5) + (3 \cdot 2) = 8 + 10 + 6 = 24
  $$

- **First row of A** $\cdot$ **Third column of B**:
  $$
  C_{13} = (1 \cdot 7) + (2 \cdot 4) + (3 \cdot 1) = 7 + 8 + 3 = 18
  $$

- **Second row of A** $\cdot$ **First column of B**:
  $$
  C_{21} = (4 \cdot 9) + (5 \cdot 6) + (6 \cdot 3) = 36 + 30 + 18 = 84
  $$

- **Second row of A** $\cdot$ **Second column of B**:
  $$
  C_{22} = (4 \cdot 8) + (5 \cdot 5) + (6 \cdot 2) = 32 + 25 + 12 ="
lab,linear_algebra.ipynb,markdown,3," 69
  $$

- **Second row of A** $\cdot$ **Third column of B**:
  $$
  C_{23} = (4 \cdot 7) + (5 \cdot 4) + (6 \cdot 1) = 28 + 20 + 6 = 54
  $$

- **Third row of A** $\cdot$ **First column of B**:
  $$
  C_{31} = (7 \cdot 9) + (8 \cdot 6) + (9 \cdot 3) = 63 + 48 + 27 = 138
  $$

- **Third row of A** $\cdot$ **Second column of B**:
  $$
  C_{32} = (7 \cdot 8) + (8 \cdot 5) + (9 \cdot 2) = 56 + 40 + 18 = 114
  $$

- **Third row of A** $\cdot$ **Third column of B**:
  $$
  C_{33} = (7 \cdot 7) + (8 \cdot 4) + (9 \cdot 1) = 49 + 32 + 9 = 90
  $$

#### Final Result:

Putting it all together:

$$
C = AB = \begin{bmatrix}
30 & 24 & 18 \\\\
84 & 69 & 54 \\\\
138 & 114 & 90
\end{bmatrix}
$$


#### Key"
lab,linear_algebra.ipynb,markdown,4," Point

Matrix multiplication is **not** element-wise. It involves **dot products** between rows and columns, resulting in new combinations of values — not just simple multiplication."
lab,linear_algebra.ipynb,code,1,"import numpy as np

#a = np.array([2, 5, 7])
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
#b = np.array([3, 4, 1])
b = np.array([[9,8,7], [6,5,4], [3,2,1]])
dot_product = np.dot(a, b)
print(""Dot product:"", dot_product)"
lab,linear_algebra.ipynb,markdown,1,"### Summary: Dot Product vs. Matrix Multiplication

- The **dot product** takes two vectors and returns a scalar.
- **Matrix multiplication** generalizes this idea: each element in the result is a dot product between a row of the first matrix and a column of the second.



This is how the ""dot product"" generalizes to matrices: each entry in the result is a dot product of a row from $A$ and a column from $B$.

### Dot Product vs. Element-wise Multiplication

| Operation                  | Symbol     | Output Type | Description                                         |
|---------------------------|------------|---------------|-----------------------------------------------------|
| Dot product"
lab,linear_algebra.ipynb,markdown,2,"               | $\cdot$    | Scalar/Matrix | Multiply and sum: $a_1b_1 + a_2b_2 + \dots$         |
| Element-wise multiplication | $\circ$    | Vector      | Multiply each element separately: $[a_1b_1, a_2b_2, \dots]$ |

### Why It Matters

- The **dot product** is used in neural networks to compute the **weighted sum** of inputs before applying an activation function — this is the basic operation of a neuron.
- **Element-wise operations**, on the other hand, are used for scaling, normalization, and activation functions that apply independently to each element.

---"
lab,linear_algebra.ipynb,markdown,1,"## Transpose

The transpose of a matrix flips its rows and columns. It is often used to align dimensions for multiplication.

### Example: Matrix Transpose

Suppose we have a matrix:

$$
A = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
$$

The transpose of $A$, denoted $A^\top$, is:

$$
A^\top = \begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix}
$$

**Result:**  
Rows become columns and columns become rows."
lab,linear_algebra.ipynb,code,1,"# try changing the shape of the array
A = np.array([[1, 2, 3], [3, 4, 5]])
print(f""A\n{A}"")
B = A.reshape(6, 1)
print(f""A.reshape(6, 1):\n{B}"")
C = A.reshape(1, 6)
print(f""A.reshape(1, 6):\n{C}"")
D = A.reshape(2, 3)
print(f""A.reshape(2, 3):\n{D}"")
E = A.reshape(3, 2)
print(f""A.reshape(3, 2):\n{E}"")
print(f""A:\n{A}"")
transpose = A.T
print(f""transpose:\n{transpose}"")"
lab,linear_algebra.ipynb,markdown,1,"## Determinant

The determinant is a scalar value that can be computed from a square matrix. It provides important information about the matrix, such as whether it is invertible and how it scales space.

The determinant tells us important things about a matrix. In general, it helps us know if a matrix can be inverted (reversed), how it changes the size or orientation of shapes when used for transformations, and whether a system of linear equations has a unique solution. If the determinant is zero, the matrix cannot be inverted and the system may not have a unique solution.

The general formula for the determinant of an $n \times n$ matrix $A$ is:

$$
\det(A) = \sum_{\sigma \in S_n} \mathrm{s"
lab,linear_algebra.ipynb,markdown,2,"gn}(\sigma) \prod_{i=1}^n a_{i, \sigma(i)}
$$

where $S_n$ is the set of all permutations of $\{1, 2, \dots, n\}$ and $\mathrm{sgn}(\sigma)$ is the sign of the permutation $\sigma$."
lab,linear_algebra.ipynb,markdown,1,"### The determinant of a $2 \times 2$ matrix is calculated as:

$$
\det\left(\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}\right) = ad - bc
$$

**Example** Let's compute the determinant of a $2 \times 2$ matrix using an example:

$$
A = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$

The determinant is:

$$
\det(A) = (1 \times 4) - (2 \times 3) = 4 - 6 = -2
$$


### For a $3 \times 3$ matrix the general formula is:

$$
\det\left(\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}\right) = aei + bfg + cdh - ceg - bdi - afh
$$

### However, it is often easier to use the Cofactor Expansion formula:

For a $3 \times 3$ matrix:

$$
A = \begin{bmatrix}
a & b & c \\\\
d & e & f \\\\
g"
lab,linear_algebra.ipynb,markdown,2," & h & i
\end{bmatrix}
$$

We compute the determinant by expanding along the **first row**:

$$
\det(A) =
a \cdot \det\begin{bmatrix} e & f \\\\ h & i \end{bmatrix}
- b \cdot \det\begin{bmatrix} d & f \\\\ g & i \end{bmatrix}
+ c \cdot \det\begin{bmatrix} d & e \\\\ g & h \end{bmatrix}
$$

Each $2 \times 2$ determinant is computed using the rule: $ad - bc$.

Suppose we have:

$$
C = \begin{bmatrix}
1 & 2 & 3 \\
3 & 4 & 5 \\
6 & 7 & 8
\end{bmatrix}
$$


The determinant of $C$ is:
To derive the $3 \times 3$ determinant formula from the definition, expand along the first row:


$$
\det(C) = 1 \cdot
\begin{vmatrix}
4 & 5 \\
7 & 8
\end{vmatrix}
- 2 \cdot
\begin{vmatrix}
3 & 5 \\
6 & 8
\end{vmatri"
lab,linear_algebra.ipynb,markdown,3,"x}
+ 3 \cdot
\begin{vmatrix}
3 & 4 \\
6 & 7
\end{vmatrix}
$$


Each $2 \times 2$ determinant is:

$$
\begin{vmatrix}
a & b \\
c & d
\end{vmatrix}
= ad - bc
$$

So,

$$
\det(C) = 1 \cdot (4 \times 8 - 5 \times 7)
- 2 \cdot (3 \times 8 - 5 \times 6)
+ 3 \cdot (3 \times 7 - 4 \times 6) 
$$

So, the determinant is:

$$
\det(C) = 1 \cdot (4 \times 8 - 5 \times 7)
- 2 \cdot (3 \times 8 - 5 \times 6)
+ 3 \cdot (3 \times 7 - 4 \times 6) 
= 1 \cdot (32 - 35) - 2 \cdot (24 - 30) + 3 \cdot (21 - 24)
= 1 \cdot (-3) - 2 \cdot (-6) + 3 \cdot (-3)
= -3 + 12 - 9 = 0
$$

Therefore, $\det(C) = 0$."
lab,linear_algebra.ipynb,markdown,1,"#### Interpretation

- If the determinant is zero, the matrix is singular (not invertible).
- If the determinant is nonzero, the matrix is invertible.

Determinants are important for understanding the properties of transformations in neural networks and for solving systems of equations."
lab,linear_algebra.ipynb,markdown,1,"#### Using NumPy

You can compute determinants in code using NumPy:"
lab,linear_algebra.ipynb,code,1,"import numpy as np

C = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

det_C = np.linalg.det(C)
# Print the determinant with higher precision
print(f""Determinant of C: (should be 0)\n{det_C:.130f}"")  # Show 130 decimal places
print(f""is close to zero? {np.isclose(det_C, 0)}"") # Output: True"
lab,linear_algebra.ipynb,code,1,"def determinant(matrix):
    # Base case for 2x2 matrix
    if len(matrix) == 2 and len(matrix[0]) == 2:
        # Calculate determinant of 2x2 matrix
        # return the determinant using the formula ad - bc
        # ad - bc for matrix [[a, b], [c, d]]
        # matrix
        # ad = matrix[0][0] * matrix[1][1]
        # bc = matrix[0][1] * matrix[1][0]
        # return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]
        return matrix[0][0]*matrix[1][1] - matrix[0][1]*matrix[1][0]
    # Recursive case for larger matrices
    det = 0
    for col in range(len(matrix)):
        # Build minor matrix
        # minor = [row[:col] + row[col+1:] for row in matrix[1:]]
        # This"
lab,linear_algebra.ipynb,code,2," creates a new matrix excluding the first row and the current column
        # The minor is the matrix obtained by removing the first row and the current column
        # Calculate the cofactor
        # cofactor = ((-1) ** col) * matrix[0][col]
        # Multiply the cofactor by the determinant of the minor
        # det += cofactor * determinant(minor)
        # The determinant is the sum of the cofactors
        # det += ((-1) ** col) * matrix[0][col] * determinant([row[:col] + row[col+1:] for row in matrix[1:]])
        # We iterate over each column in the first row (matrix[0]) because the determinant is expanded along the first row.
        # For each column index 'col', we exclude that"
lab,linear_algebra.ipynb,code,3," column to form the minor matrix,
        # and recursively compute its determinant. This follows the Laplace expansion (cofactor expansion) formula.
        minor = [row[:col] + row[col+1:] for row in matrix[1:]]
        cofactor = ((-1) ** col) * matrix[0][col] * determinant(minor)
        det += cofactor
    return det

# Example usage:
det_C_py = determinant(C.tolist())
print(f""pure python determinant of C: {det_C_py}"")"
lab,linear_algebra.ipynb,markdown,1,"## Invertibility of Matrices

A matrix is **invertible** (also called **nonsingular**) if and only if its determinant is **nonzero**. If the determinant is zero, the matrix is **singular** and does **not** have an inverse.

## Why does the determinant matter?

- **If $\det(A) \neq 0$:** $A$ is invertible (there exists $A^{-1}$ such that $A A^{-1} = I$).
- **If $\det(A) = 0$:** $A$ is not invertible (no $A^{-1}$ exists).

## Example 1: Invertible $2 \times 2$ Matrix

Let
$$
M = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$

Compute the determinant:
$$
\det(M) = (1 \times 4) - (2 \times 3) = 4 - 6 = -2
$$

Since $\det(M) \neq 0$, $M$ is invertible.

## Example 2: Singular $3 \times 3$ Matrix"
lab,linear_algebra.ipynb,markdown,2,"

Let
$$
C = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
$$

Compute the determinant (as shown above):
$$
\det(C) = 0
$$

Since $\det(C) = 0$, $C$ is **not** invertible.

## Summary Table

| Matrix | Determinant | Invertible? |
|--------|-------------|-------------|
| $M$    | $-2$        | Yes         |
| $C$    | $0$         | No          |

**In practice:**  
- Always check the determinant before trying to compute a matrix inverse.
- In neural networks and data science, invertibility is important for solving systems of equations and for certain algorithms (e.g., finding unique solutions, matrix decompositions)."
lab,linear_algebra.ipynb,markdown,1,"## Inverse of a Matrix

The **inverse** of a square matrix $ A $, denoted $ A^{-1} $, is a matrix such that:

$$
A A^{-1} = A^{-1} A = I
$$

where $ I $ is the identity matrix. A matrix is **invertible if and only if** its determinant is nonzero.

### Formula for the Inverse (2×2 Matrix)

For a $ 2 \times 2 $ matrix:

$$
A = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$

If \( \det(A) = ad - bc \neq 0 \), then:

$$
A^{-1} = \frac{1}{ad - bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
$$

### Example: Inverse of a 2×2 Matrix

Let

$$
M = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$

Compute:

$$
\det(M) = 1 \cdot 4 - 2 \cdot 3 = -2 \\
M^{-1} = \frac{1}{-2} \begin{bmatrix} 4 & -2 \\ -"
lab,linear_algebra.ipynb,markdown,2,"3 & 1 \end{bmatrix}
= \begin{bmatrix} -2 & 1 \\ 1.5 & -0.5 \end{bmatrix}
$$

Verify:

$$
M^{-1}M = 
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
= I
$$

### Inverse of a 3×3 Matrix

For larger matrices, the inverse is found using:

1. **Minors** – Determinants of $ 2 \times 2 $ submatrices  
2. **Cofactors** – Apply a checkerboard of signs to the minors  
3. **Adjugate** – Transpose the cofactor matrix  
4. **Inverse** – Divide the adjugate by $ \det(A) $

Let:

$$
A = \begin{bmatrix}
6 & 1 & 1 \\
4 & -2 & 5 \\
2 & 8 & 7
\end{bmatrix}
$$

#### Step 1: Compute Determinant

$$
\det(A) = 6 \cdot (-54) - 1 \cdot 18 + 1 \cdot 36 = -306
$$

#### Step 2: Compute Minors and Cofactors

Minors matrix:"
lab,linear_algebra.ipynb,markdown,3,"

$$
\begin{bmatrix}
-54 & 18 & 36 \\
-1 & 40 & -16 \\
7 & 26 & -16
\end{bmatrix}
$$

Apply checkerboard signs (Cofactors matrix):

$$
\begin{bmatrix}
-54 & -18 & 36 \\
1 & 40 & 16 \\
7 & -26 & -16
\end{bmatrix}
$$

#### Step 3: Transpose to Get Adjugate

$$
\text{adj}(A) = 
\begin{bmatrix}
-54 & 1 & 7 \\
-18 & 40 & -26 \\
36 & 16 & -16
\end{bmatrix}
$$

#### Step 4: Compute Inverse

$$
A^{-1} = \frac{1}{-306}
\begin{bmatrix}
-54 & 1 & 7 \\
-18 & 40 & -26 \\
36 & 16 & -16
\end{bmatrix}
$$

#### Verification: \( A^{-1} A = I \)

To verify:

Let

$$
A^{-1} = \frac{1}{-306}
\begin{bmatrix}
-54 & 1 & 7 \\
-18 & 40 & -26 \\
36 & 16 & -16
\end{bmatrix}, \quad
A = \begin{bmatrix}
6 & 1 & 1 \\
4 & -"
lab,linear_algebra.ipynb,markdown,4,"2 & 5 \\
2 & 8 & 7
\end{bmatrix}
$$

Now compute the matrix product \( A^{-1} A \) (only showing final result):

$$
A^{-1} A =
\frac{1}{-306}
\begin{bmatrix}
-306 & 0 & 0 \\
0 & -306 & 0 \\
0 & 0 & -306
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
= I
$$

✅ This confirms that the computed inverse is correct."
lab,linear_algebra.ipynb,code,1,"# Compute the inverse of A using numpy
A = np.array([[1,2], [3,4]])
A = np.array([[6, 1, 1], [4, -2, 5], [2, 8, 7]])

try:
    print(""Matrix A:\n"", A)
    inv_A = np.linalg.inv(A)
    print(""Inverse of A:\n"", inv_A)
    # Verify the inverse by multiplying A and its inverse
    identity = np.dot(A, inv_A)
    print(""A * A^-1:\n"", identity)
    print(""Is A * A^-1 close to identity matrix?"", np.allclose(identity, np.eye(A.shape[0])))

except np.linalg.LinAlgError as e:
    print(""Matrix A is not invertible:"", e)"
lab,linear_algebra.ipynb,markdown,1,"---
## Eigenvalues and Eigenvectors

While not always directly used, understanding eigenvalues and eigenvectors helps in analyzing the stability and dynamics of learning algorithms.

Imagine you have a special kind of transformation—like stretching, squishing, or rotating—applied to vectors (arrows) in space. Most vectors will change direction and length when you do this. But some rare, special vectors only get stretched or squished (their direction stays the same). These are called **eigenvectors**.

The amount by which these special vectors are stretched or squished is called the **eigenvalue**.

- **Eigenvector:** A direction that stays the same after the transformation (except for gettin"
lab,linear_algebra.ipynb,markdown,2,"g longer or shorter).
- **Eigenvalue:** How much the eigenvector is stretched or squished.

**Example:**  
If you imagine pushing on a door, the axis the door rotates around doesn’t move—it’s like an eigenvector. The amount the door moves (how far it swings open) is like the eigenvalue.

In math, for a matrix $A$, an eigenvector $\mathbf{v}$ and eigenvalue $\lambda$ satisfy:
$$
A\mathbf{v} = \lambda \mathbf{v}
$$

This means applying $A$ to $\mathbf{v}$ just scales it by $\lambda$—the direction doesn’t change. Eigenvalues and eigenvectors help us understand what a matrix (or transformation) really does at its core.

To find the **eigenvalues** $\lambda$ of a square matrix $A$, solve the **ch"
lab,linear_algebra.ipynb,markdown,3,"aracteristic equation**:

$$
\det(A - \lambda I) = 0
$$

where $I$ is the identity matrix of the same size as $A$.

Once you have an eigenvalue $\lambda$, the corresponding **eigenvectors** $\mathbf{v}$ are the nonzero solutions to:

$$
(A - \lambda I)\mathbf{v} = 0
$$

That is, solve the homogeneous system above for $\mathbf{v}$ for each eigenvalue $\lambda$.


### Example: Eigenvalues and Eigenvectors

Suppose we have a square matrix:

$$
M = \begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}
$$

To find the eigenvalues $\lambda$, solve the characteristic equation:

$$
\det(M - \lambda I) = 0
$$
Where $I$ is the identity matrix:
First, write the matrix $M$ and the identity matrix $I$:

$$
M = \b"
lab,linear_algebra.ipynb,markdown,4,"egin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}, \quad
I = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

Multiply the identity matrix $I$ by the scalar $\lambda$:

$$
\lambda I = \begin{bmatrix}
\lambda & 0 \\
0 & \lambda
\end{bmatrix}
$$

Subtract $\lambda I$ from $M$:

$$
M - \lambda I = \begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}
-
\begin{bmatrix}
\lambda & 0 \\
0 & \lambda
\end{bmatrix}
=
\begin{bmatrix}
2 - \lambda & 1 \\
1 & 2 - \lambda
\end{bmatrix}
$$

The determinant is:

$$
(2 - \lambda)(2 - \lambda) - (1 \times 1) = (2 - \lambda)^2 - 1 = 0
$$

Expanding:

$$
(2 - \lambda)^2 = 1 \\
2 - \lambda = \pm 1 \\
\lambda_1 = 2 - 1 = 1 \\
\lambda_2 = 2 + 1 = 3
$$

So, the eigenvalues are $\la"
lab,linear_algebra.ipynb,markdown,5,"mbda_1 = 1$ and $\lambda_2 = 3$.

To find the eigenvector for $\lambda = 3$:

$$
(M - 3I)\mathbf{v} = 0 \\
\begin{bmatrix}
-1 & 1 \\
1 & -1
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
= \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

This gives $-v_1 + v_2 = 0$ and $v_1 - v_2 = 0$, so $v_1 = v_2$.

An eigenvector for $\lambda = 3$ is:

$$
\mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
$$

Similarly, for $\lambda = 1$:

$$
(M - I)\mathbf{v} = 0 \\
\begin{bmatrix}
1 & 1 \\
1 & 1
\end{bmatrix}

\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
= \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

This gives $v_1 + v_2 = 0$, so $v_1 = -v_2$.

An eigenvector for $\lambda = 1$ is:

$$
\mathbf{v} = \begin{bmatr"
lab,linear_algebra.ipynb,markdown,6,"ix} 1 \\ -1 \end{bmatrix}
$$

### Interpretation of Eigenvalues and Eigenvectors

- **Eigenvalues ($\lambda_1 = 1$, $\lambda_2 = 3$):**
    - The eigenvalues represent the amount of variance or ""stretching"" along their corresponding eigenvector directions.
    - The larger eigenvalue ($3$) indicates the direction along which the data (or transformation) has the greatest effect or variance.
    - The smaller eigenvalue ($1$) indicates a direction with less effect or variance.

- **Eigenvectors ($\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$):**
    - The eigenvector $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (for $\lambda = 3$) points along the line where both "
lab,linear_algebra.ipynb,markdown,7,"variables increase together. In the sensor example, this means both temperature and humidity rise or fall together—this is the main trend in the data.
    - The eigenvector $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ (for $\lambda = 1$) points along the line where one variable increases as the other decreases. This direction captures the contrast between temperature and humidity.

- **Geometric Meaning:**
    - If you transform a set of points using matrix $M$, points along the $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ direction will be stretched by a factor of $3$, while points along the $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ direction will be stretched by a factor of $1$ (i.e., unchanged in leng"
lab,linear_algebra.ipynb,markdown,8,"th).
    - In PCA, projecting data onto the eigenvector with the largest eigenvalue gives the principal component—showing the direction of maximum variance.

- **Practical Implication:**
    - In neural networks and data analysis, understanding these directions helps in reducing dimensionality, denoising data, and interpreting the underlying structure of datasets.
    - For the given matrix $M$, most of the ""action"" or information is along the $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ direction, so focusing on this can simplify analysis without losing much information.

**Summary:**  
- Eigenvalues: $1$ and $3$  
- Eigenvectors: $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ "
lab,linear_algebra.ipynb,markdown,9,"1 \end{bmatrix}$

## Real-Life Example: Using These Eigenvalues and Eigenvectors

Suppose you are analyzing a simple network of two sensors measuring temperature and humidity in a room. The readings are correlated, and you want to understand the main directions of variation in your data—this is a classic use case for Principal Component Analysis (PCA).

Given the covariance matrix:
$$
M = \begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}
$$

- The eigenvalues ($1$ and $3$) tell you the amount of variance captured along each principal direction.
- The eigenvectors ($\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$) give you the directions in feature space.

**How t"
lab,linear_algebra.ipynb,markdown,10,"o use them:**
- The direction $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (largest eigenvalue $3$) is the axis along which the data varies most—combining temperature and humidity together.
- The direction $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ (smaller eigenvalue $1$) is the axis along which the data varies least—contrasting temperature and humidity.

You can project your sensor data onto these axes to reduce dimensionality or to visualize the main trends, helping you identify patterns or anomalies in the room’s climate.

## Summary Table

| Concept         | Neural Network Role                      |
|-----------------|------------------------------------------|
| Vector          | Input, outpu"
lab,linear_algebra.ipynb,markdown,11,"t, weights                   |
| Matrix          | Layer weights, data batches              |
| Dot Product     | Weighted sum in neurons                  |
| Matrix Multiply | Layer transformations                    |
| Transpose       | Aligning dimensions for operations       |
| Determinant     | Invertibility, transformations           |
| Eigenvalues/Eigenvectors | Stability, dynamics analysis         |
| Element-wise    | Activation functions                     |

A solid grasp of these linear algebra concepts will make it easier to understand and implement neural networks."
lab,neural_networks.ipynb,markdown,1,"# Neural Networks – Perceptron Introduction Notebook

In this lab we will explore the basics of neural networks by building a simple perceptron from scratch using NumPy and then extending our work to experiment with gradient descent variants. Later, we will implement a simple feed‐forward neural network with Keras using the Olivetti faces dataset. Finally, we will explore how alternative activation functions and different layer configurations affect performance and intermediate outputs."
lab,neural_networks.ipynb,markdown,1,"## Basic Perceptron using NumPy

In this section we create a small dataset and define functions to compute the perceptron output, apply the Heaviside step activation, and update the weights using full-batch gradient descent."
lab,neural_networks.ipynb,code,1,"import numpy as np

# define a small dataset
# each row: [hours of exercise, cups of coffee] (with an additional bias term at the start)
# labels: 1 (insomnia) or 0 (no insomnia)
X = np.array([
    [1, 1, 3],
    [1, 2, 4],
    [1, 3, 6],
    [1, 3, 1],
    [1, 4, 3],
    [1, 5, 2],
    [1, 6, 1],
    [1, 2, 6],
    [1, 1, 5],
    [1, 0, 4],
    [1, 4, 5],
    [1, 5, 6],
    [1, 3, 3],
    [1, 6, 0],
    [1, 0, 1],
    [1, 2, 0],
    [1, 1, 0],
    [1, 0, 0],
    [1, 5, 3],
    [1, 4, 1]
])

y = np.array([
    1, 1, 1, 0, 0,
    0, 0, 1, 1, 1,
    0, 0, 1, 0, 1,
    1, 1, 1, 0, 0
])"
lab,neural_networks.ipynb,markdown,1,"The code above imports NumPy and creates a dataset with five samples. Each sample has two features (hours of exercise and cups of coffee), and an associated binary label indicating insomnia.

Next, let's create the perceptron which will take the dot product of our predictors and weights."
lab,neural_networks.ipynb,code,1,"def perceptron_output(x, weights):
    """"""Computes the weighted sum (dot product) for the perceptron.""""""
    return np.dot(x, weights)"
lab,neural_networks.ipynb,code,1,"# example usage of the perceptron
init_weights = np.array([0.0, 0.0, 0.0])

perceptron_output(X, init_weights)"
lab,neural_networks.ipynb,markdown,1,"We also need to implement a function to detect if our perceptron output ""fires"" via the heaviside function.

It returns an array where each element is 1 if the corresponding element in x is greater than or equal to zero, otherwise 0."
lab,neural_networks.ipynb,code,1,"def heaviside(x):
    """"""Applies the Heaviside step function element-wise on x. Returns 1 if x >= 0, else 0.""""""
    return np.where(x >= 0, 1, 0)"
lab,neural_networks.ipynb,code,1,"import matplotlib.pyplot as plt

x_axis = np.linspace(-1, 1, 100)
activations = [heaviside(x) for x in x_axis]

plt.plot(x_axis, activations)
plt.title(""Heaviside Function"")
plt.xlabel(""Perceptron Output"")
plt.ylabel(""Activation Function Output"")
plt.show()"
lab,neural_networks.ipynb,code,1,"# example usage of the heaviside function on our perceptron outputs on our initial weights

# everyone has insomnia!
heaviside(perceptron_output(X, init_weights))"
lab,neural_networks.ipynb,code,1,"yhat = heaviside(perceptron_output(X, init_weights))
error_rate = abs(sum(y - yhat) / len(y))

print(""Current error rate:"", error_rate)"
lab,neural_networks.ipynb,code,1,"import matplotlib.pyplot as plt 

# define a function to plot data for the decision boundary
def decision_boundary(X, y, weights):
    plt.figure(figsize=(8, 6))

    # plot each scatter point
    for label in np.unique(y):
        subset = X[y == label]
        plt.scatter(subset[:, 1], subset[:, 2], label=f""Class {label}"")

    # define the x & y vals of the decision boundary
    x_vals = np.linspace(0, 6, 100)
    y_vals = - (weights[0] + weights[1] * x_vals) / (weights[2] + 1e-6)
    plt.plot(x_vals, y_vals, 'k--', label=""Decision Boundary"")
    plt.xlabel(""Hours of Exercise"")
    plt.ylabel(""Cups of Coffee"")
    plt.title(""Scatter Plot with Decision Boundary"")
    plt.legend()
    plt.g"
lab,neural_networks.ipynb,code,2,"rid(True)
    plt.show()

decision_boundary(X, y, init_weights)"
lab,neural_networks.ipynb,markdown,1,"As we an see, our predictions are erroneous. We need a mechanism to make corrections on our weights, we need `gradient_descent`.

The `gradient_descent_update` function computes predictions using the current weights, calculates the error with respect to the true labels, and then updates the weights using full-batch gradient descent. The learning rate (`lr`) controls the step size for the update."
lab,neural_networks.ipynb,code,1,"def gradient_descent_update(X, y, weights, lr):
    """"""Performs one update of full-batch gradient descent on the weights.
    - X: Input feature matrix
    - y: True labels
    - weights: Current weight vector
    - lr: Learning rate
    Returns the updated weights.""""""
    preds = heaviside(perceptron_output(X, weights))
    error = y - preds
    delta = lr * np.dot(X.T, error) / X.shape[0]
    weights += delta
    return weights"
lab,neural_networks.ipynb,code,1,"# one iteration of full-batch gradient descent
learning_rate = 0.1

updated_weights = gradient_descent_update(X, y, init_weights, lr=learning_rate)
yhat = heaviside(perceptron_output(X, updated_weights))

print(""New weights"", updated_weights)
print(""New predictions"", yhat)"
lab,neural_networks.ipynb,code,1,"error_rate = abs(sum(y - yhat) / len(y))

print(""Current error rate:"", error_rate)"
lab,neural_networks.ipynb,code,1,"# plot decision boundary of updated weights
decision_boundary(X, y, updated_weights)"
lab,neural_networks.ipynb,markdown,1,"It seems like error rate isn't improving! Well, this is why gradient descent is an iterative proces..

Let's try out a few more runs (epochs) of gradient descent, and why don't we also try changing the `learning_rate` and `epochs` to see what happens to our final error rate."
lab,neural_networks.ipynb,code,1,"# Initialize weights to zeros (one weight per feature)
weights = np.array([0.0, 0.0, 0.0])

# TODO: Change this learning rate to see how this hyperparameter influences training
learning_rate = 0.01
# TODO: Change the number of epochs see how training duration influences final error rate
epoch_count = 100

epochs = []
learned_weights = []
errors = []

for epoch in range(epoch_count):
    new_weights = gradient_descent_update(X, y, weights, lr=learning_rate)
    yhat = heaviside(perceptron_output(X, new_weights))
    error_rate = abs(sum(y - yhat) / len(y))

    print(f""Epoch {epoch}: Weights = {weights}"")
    print(""error rate = "", error_rate)

    epochs.append(epoch)
    learned_weights.app"
lab,neural_networks.ipynb,code,2,"end(new_weights)
    errors.append(error_rate)"
lab,neural_networks.ipynb,code,1,"# plot decision boundary of final weights
decision_boundary(X, y, learned_weights[-1])"
lab,neural_networks.ipynb,markdown,1,"This training loop initializes the weight vector as zeros and then iteratively updates it using the full-batch gradient descent function. After each epoch, the updated weights are printed, illustrating how the model learns over time.

While this decision boundary might not be't perfect, note that training error is *largely* decreasing as we increase our epochs."
lab,neural_networks.ipynb,code,1,"def plot_errors(epochs, errors):
    plt.plot(epochs, errors)
    plt.xlabel(""Epoch #"")
    plt.ylabel(""% Misclassifications"")
    plt.title(""Training Error Rate vs Epoch"")

plot_errors(epochs, errors)"
lab,neural_networks.ipynb,code,1,"import matplotlib.pyplot as plt 

# create a function which plots the path of your full-batch gradient descent
def plot_descent(weights, errors):
    w1 = [w[0] for w in weights]
    w2 = [w[1] for w in weights]

    fig = plt.figure(figsize=(15, 9))
    ax = fig.add_subplot(111, projection='3d')

    # Plot the trajectory of gradient descent
    ax.plot(w1, w2, errors, marker='o', color='blue', label='Descent path')

    # Highlight start and end
    ax.scatter(w1[0], w2[0], errors[0], color='green', s=50, label='Start')
    ax.scatter(w1[-1], w2[-1], errors[-1], color='red', s=50, label='End')

    # Label axes
    ax.set_xlabel('Weight 1')
    ax.set_ylabel('Weight 2')
    ax.set_zlabel('"
lab,neural_networks.ipynb,code,2,"Error')
    ax.set_title('Gradient Descent Trajectory in 3D')
    ax.legend()

    plt.show()

# plot full batch gradient descent
plot_descent(learned_weights, errors)"
lab,neural_networks.ipynb,markdown,1,"## Gradient Descent Variants using NumPy

Here we explore alternative gradient descent methods, including mini-batch and stochastic gradient descent."
lab,neural_networks.ipynb,markdown,1,### Mini-Batch Gradient Descent
lab,neural_networks.ipynb,code,1,"def mini_batch_gradient_descent(X, y, weights, lr, batch_size):
    n, perm = X.shape[0], np.random.permutation(X.shape[0])
    X_shuf, y_shuf = X[perm], y[perm]
    for i in range(0, n, batch_size):
        xb, yb = X_shuf[i:i+batch_size], y_shuf[i:i+batch_size]
        p = heaviside(np.dot(xb, weights))
        e = yb - p
        dw = lr * np.dot(xb.T, e) / len(yb)
        weights += dw
    return weights"
lab,neural_networks.ipynb,markdown,1,"The `mini_batch_gradient_descent` function shuffles the dataset and divides it into mini-batches. For each mini-batch, it applies the gradient descent update. This approach can provide a balance between the stability of full-batch and the speed of stochastic gradient descent."
lab,neural_networks.ipynb,code,1,"# Initialize weights to zeros (one weight per feature)
weights = np.array([0.0, 0.0, 0.0])

epochs = []
learned_weights = []
errors = []

for epoch in range(epoch_count):
    new_weights = mini_batch_gradient_descent(X, y, weights, lr=learning_rate, batch_size=4)
    yhat = heaviside(perceptron_output(X, new_weights))
    error_rate = abs(sum(y - yhat) / len(y))

    print(f""Epoch {epoch}: Weights = {weights}"")
    print(""error rate = "", error_rate)

    epochs.append(epoch)
    learned_weights.append(new_weights)
    errors.append(error_rate)"
lab,neural_networks.ipynb,code,1,"# plot final decision boundary
decision_boundary(X, y, learned_weights[-1])"
lab,neural_networks.ipynb,code,1,"# plot errors of mini-batch
plot_errors(epochs, errors)"
lab,neural_networks.ipynb,code,1,"# plot the path of your mini-batch gradient descent
plot_descent(learned_weights, errors)"
lab,neural_networks.ipynb,markdown,1,### Stochastic Gradient Descent
lab,neural_networks.ipynb,code,1,"def stochastic_gradient_descent(X, y, weights, lr):
    n = X.shape[0]
    perm = np.random.permutation(n)
    X_shuf, y_shuf = X[perm], y[perm]
    for i in range(n):
        xi, yi = X_shuf[i], y_shuf[i]
        p = heaviside(np.dot(xi, weights))
        e = yi - p
        weights += lr * xi * e
    return weights"
lab,neural_networks.ipynb,markdown,1,The `stochastic_gradient_descent` function updates the weights one sample at a time after shuffling the data. This method can potentially converge faster but may be noisier than the full-batch approach.
lab,neural_networks.ipynb,code,1,"# Initialize weights to zeros (one weight per feature)
weights = np.array([0.0, 0.0, 0.0])

epochs = []
learned_weights = []
errors = []

for epoch in range(epoch_count):
    new_weights = stochastic_gradient_descent(X, y, weights, lr=learning_rate)
    yhat = heaviside(perceptron_output(X, new_weights))
    error_rate = abs(sum(y - yhat) / len(y))

    print(f""Epoch {epoch}: Weights = {weights}"")
    print(""error rate = "", error_rate)

    epochs.append(epoch)
    learned_weights.append(new_weights)
    errors.append(error_rate)"
lab,neural_networks.ipynb,code,1,"# plot final decision boundary
decision_boundary(X, y, learned_weights[-1])"
lab,neural_networks.ipynb,code,1,"# plot errors of mini-batch
plot_errors(epochs, errors)"
lab,neural_networks.ipynb,code,1,"# plot the path of your stochastic gradient descent
plot_descent(learned_weights, errors)"
lab,neural_networks.ipynb,markdown,1,"## Multi-Layer Perceptrons using Numpy

In this section we use numpy to implement a multi-layer perceptron network on a non-linearaly seperable dataset. We will explore how using different activation functions (and more layers) give us the ability to classify datasets with non-linear boundaries."
lab,neural_networks.ipynb,code,1,"import numpy as np

# define a non-linear dataset (with an added bias term)
X = np.array([
    [1, 2, 4], [1, 2.5, 4.5], [1, 3.0, 5.0], [1, 3.5, 4.5], [1, 4.0, 4.0],
    [1, 3.5, 3.5], [1, 4, 2], [1, 3.0, 3.0], [1, 2.5, 3.5], [1, 2.0, 4.0],
    [1, 3.0, 4.0], [1, 3.25, 4.25], [1, 2.75, 3.75], [1, 3.25, 3.75], [1, 2.75, 4.25]  
])

center = np.array([3, 4])
radius = 0.5

# label using circular rule: if inside radius → 1, else 0
features = X[:, 1:3]  
distances = np.linalg.norm(features - center, axis=1)
y = (distances < radius).astype(int)"
lab,neural_networks.ipynb,code,1,"plt.figure(figsize=(8, 6))

# plot each scatter point
for label in np.unique(y):
    subset = X[y == label]
    plt.scatter(subset[:, 1], subset[:, 2], label=f""Class {label}"")

plt.xlabel(""Hours of Exercise"")
plt.ylabel(""Cups of Coffee"")
plt.title(""Exercise, Coffee & Insomnia"")
plt.legend()
plt.grid(True)
plt.show()"
lab,neural_networks.ipynb,markdown,1,"Regardless of how many epochs we utilize or if our learning rate is ""good"" enough, we will never fit a linear boundary on a non-linear dataset.

To acheive non-linearity we must utilize a network of neurons, also known as multi-layer perceptron (MLP) to recognize the boundaries of our dataset and subsequently create one final non-linear decision boundaries.

Choosing the appropirate number of hidden layers & nodes within hidden layers is not a trivial task and cannot be answered simply. This often depends on your search space (the dataset) and how many classes we are aiming to categorize.

As a good rule of thumb, this [StackOverflow answer](https://stats.stackexchange.com/questions/181/how-"
lab,neural_networks.ipynb,markdown,2,"to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw) recommends the following rules:
* **The Input Layer**: This layer could be dependent on the number of predictor our dataset has. So for example, if we have 10 columns within our dataset, we will also have 11 input nodes (+1 for the bias term).  
* **The Output Layer**: This layer is completely dependent on the amount of categories we are looking to predict. So if we have two categories in our dataset (e.g. `fraud` vs `not fraud`), we will have only two output nodes.  
* **The Hidden Layers**: As expressed in the post, we only need hidden layers if our dataset contains non-linear boundaries. Once we determine that w"
lab,neural_networks.ipynb,markdown,3,"e do need a hidden layer (most prediction tasks can be sufficiently solved with only one hidden layer), we can arbitrarily choose a number of nodes somewhere between the input size and the output size (as expressed by Jeff Heaton).  

For this machine learning task, we will use 3 input nodes, 2 hidden node, and 1 output node."
lab,neural_networks.ipynb,code,1,"def forward_pass(X, W1, W2):
    """"""Performs a forward pass of the MLP""""""
    Z1 = heaviside(perceptron_output(X, W1))         # Hidden layer activation
    Z2 = heaviside(perceptron_output(Z1, W2))       # Output layer activation
    return Z2

def initialize_weights(input_size, hidden_size, output_size):
    """"""Randomly initialize weights for input → hidden and hidden → output layers""""""
    W1 = np.random.randn(input_size, hidden_size) * 0.1
    W2 = np.random.randn(hidden_size, output_size) * 0.1
    return W1, W2"
lab,neural_networks.ipynb,code,1,"input_nodes = 3
hidden_nodes = 2
output_size = 1

# initialize weights for both layers
W1, W2 = initialize_weights(input_nodes, hidden_nodes, output_size)

outputs = forward_pass(X, W1, W2)

print(outputs)"
lab,neural_networks.ipynb,markdown,1,"Just like before we need to train our model via gradient descent. However this time, since we are dealing with multiple layers of perceptrons, simple gradient descent will not work!

Iterating improvements on the output of our final layer of perceptrons does not gaurantee that our previous layer will see the same improvements. 

Instead, we must rely on the concept of [backpropagation](https://www.youtube.com/watch?v=IN2XmBhILt4&ab_channel=StatQuestwithJoshStarmer)."
lab,neural_networks.ipynb,markdown,1,In addition we should also utilize a non-linear activation function such as ReLU (rectified linear unit) or the sigmoid to model our non-linear boundaries.
lab,neural_networks.ipynb,code,1,"def relu(x):
    """"""Applies the ReLU function element-wise on x. Returns x if x > 0, else 0.""""""
    return np.where(x > 0, x, 0)

def relu_derivative(x):
    return (x > 0).astype(float)

def sigmoid(x):
    """"""Applies the sigmoid function element-wise on x. Returns value between 0 and 1.""""""
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)"
lab,neural_networks.ipynb,code,1,"activations = [relu(x) for x in x_axis]

plt.plot(x_axis, activations)
plt.title(""ReLU Function"")
plt.xlabel(""Perceptron Output"")
plt.ylabel(""Activation Function Output"")
plt.show()"
lab,neural_networks.ipynb,code,1,"x_axis = np.linspace(-8, 8, 100)
activations = [sigmoid(x) for x in x_axis]

plt.plot(x_axis, activations)
plt.title(""Sigmoid Function"")
plt.xlabel(""Perceptron Output"")
plt.ylabel(""Activation Function Output"")
plt.show()"
lab,neural_networks.ipynb,markdown,1,"While the details of implementing the back-prop algorithm are beyond the scope of this fellowship, details explained within the section titled ""How does backpropagation work?"" in the following link: https://www.ibm.com/think/topics/backpropagation 

Instead, we will utilize the `Keras` package to implement this mechanism."
lab,neural_networks.ipynb,markdown,1,"## Feed-Forward Neural Network using Keras

In this section we use Keras to implement a simple feed-forward neural network on the Olivetti faces dataset. The dataset contains 400 grayscale images (64x64 pixels) across 40 classes.

If this is your first time running this notebook, be sure to run the code block below before any other code."
lab,neural_networks.ipynb,code,1,"!pip install tensorflow
!pip install pydot
!pip install graphviz"
lab,neural_networks.ipynb,code,1,"import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Flatten

from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split"
lab,neural_networks.ipynb,markdown,1,"The cell above imports necessary modules from TensorFlow and scikit-learn. We will use Keras (via TensorFlow) to define and train our neural network, and scikit-learn to load and split the dataset."
lab,neural_networks.ipynb,code,1,"faces = fetch_olivetti_faces()

X = faces.images
y = faces.target

X_flat = faces.images.reshape((len(faces.images), -1))  # flatten to (n_samples, n_features)

X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.2, random_state=42)"
lab,neural_networks.ipynb,code,1,X_train.shape
lab,neural_networks.ipynb,markdown,1,This cell loads the Olivetti faces dataset. The data is then split into training and testing sets.
lab,neural_networks.ipynb,markdown,1,"Before we go further, let's go over the basics of the Keras package. 

Think of `Keras` as the `sklearn` of deep learning. This package allows us to quickly create deep learning models without manually coding the components we've discussed in class."
lab,neural_networks.ipynb,markdown,1,"### Sequential

The `Sequential` object is a linear stack of layers. This allows us to connect layers of neurons to one another, which will subsequently flow information from the input layer towards the final output layer."
lab,neural_networks.ipynb,code,1,model = Sequential()
lab,neural_networks.ipynb,markdown,1,"While a `Sequential` object contains no parameters, we instead pass in layers of neurons via the `Input` & `Dense` objects.

For example, in the code-block below, we create a simple neural network."
lab,neural_networks.ipynb,code,1,"model = Sequential([
    Input(shape=(4,)),
    Dense(1, activation='sigmoid')
])"
lab,neural_networks.ipynb,markdown,1,"This network contains:
* `Input`: an input layer which takes in the shape of each sample in the `shape` param.
* `Dense`: an output layer of 1 neuron.

Note that we specify that this dataset contains 4 columns by specifying the `(4,)` shape. 

In the `Sequential` object, each layer feeds into the next. We will utilize this object to train on our dataset."
lab,neural_networks.ipynb,markdown,1,"### Dense

The `Dense` object is a fully connected layer of neurons that receive input from all neurons in the previous layers.

This object contains the following parameters:
* `units`: The number of neurons in this layer
* `activation`: the type of activation function (`relu`, `sigmoid`, `softmax`, etc)

Note that for our first input layer, we specify an `Input` object which takes in our samples of data."
lab,neural_networks.ipynb,markdown,1,"### Activation Functions

While there is no hard & fast rule for choosing activation functions, we generally utilize the problem space we are attempting to predict to narrow down our options. 

Generally speaking, we use...
* `ReLU` for image recognition in our hidden layer
* `Sigmoid` or `Tanh` for time series data in our hidden layer
* `Sigmoid` for binary or multilabel classification in our output layer
* `Softmax` for multiclass classification in our output layer

For more details, check out the following guide: https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/

Let's use what we've learned so far to build a network that will classify our olivetti datase"
lab,neural_networks.ipynb,markdown,2,t...
lab,neural_networks.ipynb,code,1,"oli_model = Sequential([
    Input(shape=(4096, )),  # input layer
    Dense(64, activation='relu'), # hidden layer
    Dense(40, activation='softmax') # output layer
])"
lab,neural_networks.ipynb,markdown,1,"### Compile

Now that we have our neural network prepared, we can call the `compile` method which will prepare our model for training.

During this step we define the following parameters:
* `optimizer`: This defines an algorithm which optimizes the training of the model's internal weights. This is where we define the ""type"" of gradient descent that we apply in our model training. We could specify stochastic-gradient-descent `SGD`, however at this point we have a number of advanced optimizers we could utilize in our training: https://keras.io/api/optimizers/ 
* `loss`: This parameter defines the metric our model attempts to optimize. This could be mean-squared error (`mse`) or any other meas"
lab,neural_networks.ipynb,markdown,2,"ure of error that we've discussed before (`f1-score`, `accuracy`, etc). In this case, we will utilize a metric called `sparse categorical cross entropy` which is similar to our `gini index`. This measure calculates the difference between our predicted probability distributions and our actual distributions: https://keras.io/api/losses/#loss-class
* `metrics`: Lastly, we define the list of metrics we evaluate during training and testing. This is similar to our `loss` parameter, however we only evaluate these measures by ""consequence"" and do not utilize these metrics to optimize our model. 

Note however that we have many more hyperparameters to choose: https://keras.io/api/models/model_trainin"
lab,neural_networks.ipynb,markdown,3,g_apis/
lab,neural_networks.ipynb,markdown,1,"A simple feed-forward neural network is defined here. The network has an input layer matching the flattened image size, two hidden layers with ReLU activation, and an output layer with softmax activation to classify the 40 classes."
lab,neural_networks.ipynb,code,1,"oli_model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])"
lab,neural_networks.ipynb,markdown,1,"### Fit

Much like `sklearn`, we also have a `.fit()` method which takes in our training data in order to learn the best weights for our neural network. Our parameters for this method include:

* `epochs`: the number of times to train our model. Like we saw in the above toy example, we want this to be sufficiently long, however at some point we reach a minimum.
* `validation_split`: The fraction of data used for validation. We will evaluate our model at the end of each epoch on this dataset. Note that unlike `sklearn` we do not take random samples for our validation set!
* `verbose`: How ""talkative"" is our training process. `0` indicates complete silence, `1` expresses a progress bar, and `2"
lab,neural_networks.ipynb,markdown,2,"` prints out information for each epoch.

Again, note that we have way more parameters to choose from: https://keras.io/api/models/model_training_apis/"
lab,neural_networks.ipynb,code,1,"oli_model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)"
lab,neural_networks.ipynb,markdown,1,"In summary, the model is compiled using the Adam optimizer and trained using sparse categorical crossentropy loss. The training is run for 100 epochs with a validation split of 20%. 

As we can see however, our accuracy is quite abysmal! 

When evaluating our model on our testing set we get an accuracy of 0%."
lab,neural_networks.ipynb,code,1,"loss, accuracy = oli_model.evaluate(X_test, y_test, verbose=0)
print(f""Loss: {loss}, Test Accuracy: {accuracy}"")"
lab,neural_networks.ipynb,markdown,1,"While we shouldn't expect anything interesting, let's see which predictions our model generated for our faces."
lab,neural_networks.ipynb,code,1,"yhat = oli_model.predict(X_test)

yhat[0]"
lab,neural_networks.ipynb,markdown,1,"Note that we get a vector of probabilities that sum to 100% when creating our predictions. If we want to associate these probabilities with predictions, we must utilize the `argmax` method."
lab,neural_networks.ipynb,code,1,sum(yhat[0])
lab,neural_networks.ipynb,code,1,"predicted_classes = np.argmax(yhat, axis=1)

predicted_classes"
lab,neural_networks.ipynb,code,1,"X_images = faces.images


_, _, _, _, _, idx_test = train_test_split(
    X_flat, faces.target, np.arange(len(faces.images)),
    test_size=0.3, random_state=42
)

# Step 2: Visualize predictions with true labels
n_samples_to_plot = 16
sample_idxs = np.random.choice(len(predicted_classes), n_samples_to_plot, replace=False)

plt.figure(figsize=(15, 6))
for i, idx in enumerate(sample_idxs):
    img = X_images[idx_test[idx]]
    true_label = y_test[idx]
    pred_label = predicted_classes[idx]

    plt.subplot(4, 4, i + 1)
    plt.imshow(img, cmap='gray')
    title_color = 'green' if true_label == pred_label else 'red'
    plt.title(f""True: {true_label}\nPred: {pred_label}"", color=title_color, f"
lab,neural_networks.ipynb,code,2,"ontsize=10)
    plt.axis('off')

plt.suptitle(""Olivetti Faces – True vs Predicted (kNN)"", fontsize=16)
plt.tight_layout()
plt.show()"
lab,neural_networks.ipynb,markdown,1,"Welp, it looks like every face was predicted as class `6`. Obviously we need to try something different.

Let's try bumping the number of nodes in our hidden layer on this model to see if we get better performance."
lab,neural_networks.ipynb,markdown,1,"## Intermediate Outputs using Keras

Here we experiment with different numbers of intermediate nodes. We also extract outputs from intermediate layers to study what features the network is learning."
lab,neural_networks.ipynb,code,1,"model_alt = Sequential([
    Input(shape=(4096, )),  # input layer
    Dense(128, activation='relu'), # hidden layer
    Dense(40, activation='softmax') # 
])"
lab,neural_networks.ipynb,code,1,"model_alt.compile(optimizer='adam', 
                  loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy'])
model_alt.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)"
lab,neural_networks.ipynb,markdown,1,"Amazing! Just by doubling the number of nodes in my intermediate layer & the epochs, we now have a training accuracy of 84%

Adding more neurons increases the models capacity to ""learn"", allowing it to more deeply analyze the complex patterns that make up a face in our dataset. 

When we have a highly dimensional dataset we also need highly dimensional neurons to extract useful features out of our flattened pixels.

But, before we claim victory, let's evaluate our model on our testing dataset."
lab,neural_networks.ipynb,code,1,"loss, accuracy = model_alt.evaluate(X_test, y_test, verbose=0)
print(f""Loss: {loss}, Test Accuracy: {accuracy}"")"
lab,neural_networks.ipynb,markdown,1,Darn! Better than 0% but still pretty poor. Let's continue experimenting with our deep learning architecture to see if we can extract better predictive capabilities. Let's try for another layer.
lab,neural_networks.ipynb,code,1,"model_3 = Sequential([
    Input(shape=(4096, )),  # input layer
    Dense(128, activation='relu'), # hidden layer 1
    Dense(64, activation='tanh'), # hidden layer 2
    Dense(40, activation='softmax') # output layer
])

model_3.compile(optimizer='adam', 
                  loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy'])

model_3.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)"
lab,neural_networks.ipynb,code,1,"loss, accuracy = model_3.evaluate(X_test, y_test, verbose=0)
print(f""Loss: {loss}, Test Accuracy: {accuracy}"")"
lab,neural_networks.ipynb,markdown,1,"Great! An 86% testing accuracy!

**Challenge**: Can you beat my testing accuracy? Try implementing variations on my neural network architecture. Or better yet, search up what previous architectures have worked for image recognition."
lab,neural_networks.ipynb,code,1,"yhat = model_3.predict(X_test)

predicted_classes = np.argmax(yhat, axis=1)

predicted_classes"
lab,neural_networks.ipynb,code,1,"X_images = faces.images

_, _, _, _, _, idx_test = train_test_split(
    X_flat, faces.target, np.arange(len(faces.images)),
    test_size=0.3, random_state=42
)

# Step 2: Visualize predictions with true labels
n_samples_to_plot = 16
sample_idxs = np.random.choice(len(predicted_classes), n_samples_to_plot, replace=False)

plt.figure(figsize=(15, 6))
for i, idx in enumerate(sample_idxs):
    img = X_images[idx_test[idx]]
    true_label = y_test[idx]
    pred_label = predicted_classes[idx]

    plt.subplot(4, 4, i + 1)
    plt.imshow(img, cmap='gray')
    title_color = 'green' if true_label == pred_label else 'red'
    plt.title(f""True: {true_label}\nPred: {pred_label}"", color=title_color, fo"
lab,neural_networks.ipynb,code,2,"ntsize=10)
    plt.axis('off')

plt.suptitle(""Olivetti Faces – True vs Predicted (kNN)"", fontsize=16)
plt.tight_layout()
plt.show()"
lab,neural_networks.ipynb,markdown,1,"The cool thing about this dataset is that we can ""peel back"" the neural network to view which features our network has picked up.

We do this by extracting the weights of the connections between our first and second layer. Using these weights, we can visualize what the network ""sees."""
lab,neural_networks.ipynb,code,1,"weights, biases = model_3.layers[0].get_weights()

print(""weights shape:"", weights.shape)  # (4096, 128)
print(""biases shape:"", biases.shape)    # (128,)"
lab,neural_networks.ipynb,code,1,"import matplotlib.pyplot as plt

# Visualize the first 20 neurons' weight vectors as 64x64 images
fig, axes = plt.subplots(4, 5, figsize=(15, 6))

for i, ax in enumerate(axes.flat):
    weight_vector = weights[:, i]
    weight_image = weight_vector.reshape((64, 64))  # reshape from 4096 to 64x64

    ax.imshow(weight_image, cmap='gray')
    ax.set_title(f'Neuron {i}')
    ax.axis('off')

plt.suptitle(""Visualizations of First Layer Weights"")
plt.tight_layout()
plt.show()"
lab,neural_networks.ipynb,markdown,1,"Note that while most neurons are ""dead' or do not pick up on any meaningful features, we have around 5 neurons which sufficiently ""fired"" and detected the features of a face (much like our PCA algorithm yesterday).

This could be related to the problem of [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).

For this reason, we often utilize a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) for image detection."
lab,pca.ipynb,markdown,1,"# Dimensionality Reduction & PCA Lab Notebook

By the end of this lab you will review:
- Linear Algebra, Cosine similarity, & Eigen-Vectors
- Principal Component Analysis
- Implementation of PCA through sklearn"
lab,pca.ipynb,markdown,1,"## Linear Algebra Refresher & Bottom-Up Concepts

In this section we review basic vector operations. We will create vectors representing basketball players’ statistics and compute their magnitudes, dot product, and cosine similarity."
lab,pca.ipynb,code,1,"import numpy as np

# Define vectors for two basketball players' statistics
# [pointspergame, assistspergame]
player1 = np.array([25, 20])
player2 = np.array([15.59, 1.17])"
lab,pca.ipynb,markdown,1,"The code above imports numpy and creates two NumPy arrays `player1` and `player2`, each representing statistics (points, assists) for two basketball players. This sets up the data for subsequent vector operations."
lab,pca.ipynb,code,1,"import matplotlib.pyplot as plt

# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

#pPlot the origin
origin = [0], [0]  

# plot vectors
ax.quiver(*origin, player1[0], player1[1], color='blue', scale=1, scale_units='xy', angles='xy', label='Player 1')
ax.quiver(*origin, player2[0], player2[1], color='green', scale=1, scale_units='xy', angles='xy', label='Player 2')

# set limits and grid
plt.xlim(0, 40)
plt.ylim(0, 40)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Vector Visualization: Player 1 vs Player 2')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# show plot
plt.show()"
lab,pca.ipynb,code,1,"# we can utilize vector addition
player1 + player2"
lab,pca.ipynb,code,1,"sum_vect = player1 + player2

# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

# plot vectors
ax.quiver(*origin, player1[0], player1[1], color='blue', scale=1, scale_units='xy', angles='xy', label='Player 1')
ax.quiver(*origin, player2[0], player2[1], color='green', scale=1, scale_units='xy', angles='xy', label='Player 2')
ax.quiver(*origin, sum_vect[0], sum_vect[1], color='red', scale=1, scale_units='xy', angles='xy', label='Added Vectors')

# set limits and grid
plt.xlim(0, 40)
plt.ylim(0, 40)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Player 1 + Player 2')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# show plot
plt.show()"
lab,pca.ipynb,code,1,"# as well as vector subtraction
player1 - player2"
lab,pca.ipynb,code,1,"sub_vect = player1 - player2

# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

#pPlot the origin
origin = [0], [0]  

# plot vectors
ax.quiver(*origin, player1[0], player1[1], color='blue', scale=1, scale_units='xy', angles='xy', label='Player 1')
ax.quiver(*origin, player2[0], player2[1], color='green', scale=1, scale_units='xy', angles='xy', label='Player 2')
ax.quiver(*origin, sub_vect[0], sub_vect[1], color='red', scale=1, scale_units='xy', angles='xy', label='Subtracted Vectors')

# set limits and grid
plt.xlim(0, 40)
plt.ylim(0, 40)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Player 1 - Player 2')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

"
lab,pca.ipynb,code,2,"# show plot
plt.show()"
lab,pca.ipynb,code,1,"# and scalar multiplication
2 * player2"
lab,pca.ipynb,code,1,"scaled_vector = 2 * player2

# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

#pPlot the origin
origin = [0], [0]  

# plot vectors
ax.quiver(*origin, player1[0], player1[1], color='blue', scale=1, scale_units='xy', angles='xy', label='Player 1')
ax.quiver(*origin, player2[0], player2[1], color='green', scale=1, scale_units='xy', angles='xy', label='Player 2')
ax.quiver(*origin, scaled_vector[0], scaled_vector[1], color='red', scale=1, scale_units='xy', angles='xy', label='Scaled Vector')

# set limits and grid
plt.xlim(0, 40)
plt.ylim(0, 40)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('2 * Player 2')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# s"
lab,pca.ipynb,code,2,"how plot
plt.show()"
lab,pca.ipynb,code,1,"# we can also calculate the Euclidean norms (magnitudes) of each vector
norm_player1 = np.sqrt(player1[0] ** 2 + player1[1] ** 2)

norm_player1"
lab,pca.ipynb,code,1,"# while it's good to understand the underlying components of equations, we can also simply use numpy methods
np.linalg.norm(player1)"
lab,pca.ipynb,code,1,"# and as we identified before, we can calculate the dot product of the two vectors
dot_product = player1[0] * player2[0] + player1[1] * player2[1]

dot_product"
lab,pca.ipynb,code,1,"np.dot(player1, player2)"
lab,pca.ipynb,markdown,1,"Let's identify how we can calculate the similarity between two vectors (aka players) by calculating their cosine similarity. This will entail the following steps:

1. Compute the dot product of `player1` and `player2` using `np.dot`.
2. Calculate the Euclidean norm (or magnitude) of each vector using `np.linalg.norm`.
3. Divide the dot product by the product of the norms.

The printed outputs will display the cosine similarity value."
lab,pca.ipynb,code,1,"# 1. compute the dot product of the two vectors
dot_product = np.dot(player1, player2)

# 2. compute the Euclidean norms (magnitudes) of each vector
norm_player1 = np.linalg.norm(player1)
norm_player2 = np.linalg.norm(player2)

# 3. calculate cosine similarity between the two players
cosine_similarity = dot_product / (norm_player1 * norm_player2)

print('Cosine Similarity:', cosine_similarity)"
lab,pca.ipynb,markdown,1,"This gives us a measure between -1 and 1. We consider similar and opposite vectors (1 & -1 respectively) to be ""similar"", even if they travel in opposite directions. However we consider cosine similarity close to 0 to indicate ""opposite"" vectors. We can convert these values into radians by using `arrccos`."
lab,pca.ipynb,code,1,np.arccos(cosine_similarity)
lab,pca.ipynb,markdown,1,Let's formalize these steps into a function so that we can re-use this similarity metric for a variety of different vectors (aka players).
lab,pca.ipynb,code,1,"# implement the cosine-similarity function
def cosine_sim(vector1, vector2):
    dot_product = np.dot(vector1, vector2)

    # 2. compute the Euclidean norms (magnitudes) of each vector
    norm_player1 = np.linalg.norm(vector1)
    norm_player2 = np.linalg.norm(vector2)

    # 3. calculate cosine similarity between the two players
    return dot_product / (norm_player1 * norm_player2)"
lab,pca.ipynb,code,1,"# let's see what the cosine similarity is between player1 & player3
player3 = np.array([18.62, 6.12])
cos_sim = cosine_sim(player1, player3)
cos_sim"
lab,pca.ipynb,code,1,"# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

#pPlot the origin
origin = [0], [0]  

# plot vectors
ax.quiver(*origin, player1[0], player1[1], color='blue', scale=1, scale_units='xy', angles='xy', label='Player 1')
ax.quiver(*origin, player2[0], player2[1], color='green', scale=1, scale_units='xy', angles='xy', label='Player 2')
ax.quiver(*origin, player3[0], player3[1], color='red', scale=1, scale_units='xy', angles='xy', label='Player 3')

# set limits and grid
plt.xlim(0, 40)
plt.ylim(0, 40)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Player1, Player2, Player3')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# show plot
plt.show()"
lab,pca.ipynb,code,1,np.arccos(cos_sim)
lab,pca.ipynb,markdown,1,Notice that `player3` & `player1` have a cosine similarity close to `1`. This indicates that these players are more similar in performance. Let's explore the other types of cosine similarities.
lab,pca.ipynb,code,1,"# let's imagine we have a player who is the exact opposite of player2. They make roughly 1 point per game, but 15 assists per game (notice this is the reverse of player2's stats)
player4 = np.array([1.17, 15])
cos_sim = cosine_sim(player2, player4)
cos_sim"
lab,pca.ipynb,code,1,np.arccos(cos_sim)
lab,pca.ipynb,code,1,"# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

#pPlot the origin
origin = [0], [0]  

# plot vectors
ax.quiver(*origin, player4[0], player4[1], color='blue', scale=1, scale_units='xy', angles='xy', label='Player 4')
ax.quiver(*origin, player2[0], player2[1], color='green', scale=1, scale_units='xy', angles='xy', label='Player 2')

# set limits and grid
plt.xlim(0, 20)
plt.ylim(0, 20)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Player4 vs Player2')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# show plot
plt.show()"
lab,pca.ipynb,markdown,1,"Notice that orthoganlity better describes opposite performance as opposed to vectors that go in the complete opposite direction. Think of it this way, while player2 has greatest variance in the X direction, player4 has the greatest variance in the Y direction."
lab,pca.ipynb,markdown,1,"## PCA Calculation - Data Standardization and Covariance Matrix

This section focuses on centering the data through standardization and computing the covariance matrix. As outlined in the slides, this step is crucial before applying PCA."
lab,pca.ipynb,code,1,"import pandas as pd

# load the basketball dataset into a DataFrame
team1 = [
    [18.72, 3.38],
    [22.88, 7.97],
    [20.07, 5.11],
    [25.0, 20.0],
    [18.62, 6.12],
    [15.59, 1.17]
]

# convert to DataFrame
df = pd.DataFrame(team1, columns=['PointsPerGame', 'AssistsPerGame'])

df.head()"
lab,pca.ipynb,code,1,"import matplotlib.cm as cm

plt.figure(figsize=(12, 8))
ax = plt.gca()
colors = cm.get_cmap('tab10', len(df))

# plot each vector from origin
for idx, row in df.iterrows():
    ax.quiver(0, 0, row['PointsPerGame'], row['AssistsPerGame'], angles='xy', scale_units='xy', scale=1, color=colors(idx), alpha=0.8, label=f'Player #{idx+1}')

# Formatting
plt.xlim(0, df['PointsPerGame'].max() + 5)
plt.ylim(0, df['AssistsPerGame'].max() + 5)
plt.xlabel(""PointsPerGame"")
plt.ylabel(""AssistsPerGame"")
plt.title(""Team1 Vectors from Origin"")
plt.grid(True)
plt.legend()
ax.set_aspect('equal')

# Show plot
plt.show()"
lab,pca.ipynb,markdown,1,"Step 1: Before moving forward, we must standardize this dataset around 0 by subtracting the average off of each sample."
lab,pca.ipynb,code,1,"# standardize the numeric columns of the dataset
standardized_df = df - df.mean()
standardized_df.head()"
lab,pca.ipynb,code,1,"plt.figure(figsize=(12, 8))
ax = plt.gca()
colors = cm.get_cmap('tab10', len(df))

# plot each vector from origin
for idx, row in standardized_df.iterrows():
    ax.quiver(0, 0, row['PointsPerGame'], row['AssistsPerGame'], angles='xy', scale_units='xy', scale=1, color=colors(idx), alpha=0.8, label=f'Player #{idx+1}')

# Formatting
plt.xlim(-15, 15)
plt.ylim(-15, 15)
plt.xlabel(""PointsPerGame"")
plt.ylabel(""AssistsPerGame"")
plt.title(""Team1 Standardized Vectors from Origin"")
plt.grid(True)
plt.legend()
ax.set_aspect('equal')

# Show plot
plt.show()"
lab,pca.ipynb,markdown,1,"The above cell standardizes the numeric features in the dataset. For each column, it subtracts the mean. This re-centers the data around zero. The result is displayed using `head()`."
lab,pca.ipynb,markdown,1,Step 2: We then calculate the covariance matrix.
lab,pca.ipynb,code,1,"# Compute the covariance matrix of the standardized data
cov_matrix = standardized_df.cov()
cov_matrix"
lab,pca.ipynb,code,1,"cov_matrix.iloc[0, 0]"
lab,pca.ipynb,code,1,"# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

#pPlot the origin
origin = [0], [0]  

# plot vectors
ax.quiver(*origin, cov_matrix.iloc[0, 0], cov_matrix.iloc[0, 1], color='purple', scale=1, scale_units='xy', angles='xy', label='Cov Row 1')
ax.quiver(*origin, cov_matrix.iloc[1, 0], cov_matrix.iloc[1, 1], color='purple', scale=1, scale_units='xy', angles='xy', label='Cov Row 2')

# set limits and grid
plt.xlim(0, 50)
plt.ylim(0, 50)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Covariance Matrix')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# show plot
plt.show()"
lab,pca.ipynb,markdown,1,"This code calculates the covariance matrix for the standardized numerical features using the pandas `cov()` function. The covariance matrix shows how different features vary together, which is a key preparatory step for PCA."
lab,pca.ipynb,markdown,1,"## PCA Calculation - Getting Eigenvectors

In this section we manually implement PCA using numpy. We will compute the eigenvalues and eigenvectors of the covariance matrix, sort them, and project the data onto the principal component."
lab,pca.ipynb,code,1,"# Compute eigenvalues and eigenvectors of the covariance matrix
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix.values)

print('Eigenvalues:', eigenvalues)
print('Eigenvectors:', eigenvectors)"
lab,pca.ipynb,markdown,1,"This cell uses numpy's `linalg.eig` function to perform eigen decomposition on the covariance matrix. The resulting eigenvalues indicate the amount of variance captured by each principal component, while the eigenvectors provide the direction of these components."
lab,pca.ipynb,code,1,"sorted_indices = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[sorted_indices]
sorted_eigenvectors = eigenvectors[:, sorted_indices]

print(""Sorted eigenvectors"", sorted_eigenvectors)"
lab,pca.ipynb,code,1,"# create the plot
plt.figure(figsize=(12, 8))
ax = plt.gca()

# plot eigen-vectors
ax.quiver(*origin, sorted_eigenvectors[0][0], sorted_eigenvectors[0][1], color='purple', scale=1, scale_units='xy', angles='xy', label='Cov Row 1')
ax.quiver(*origin, sorted_eigenvectors[1][0], sorted_eigenvectors[1][1], color='purple', scale=1, scale_units='xy', angles='xy', label='Cov Row 2')

# plot original points
for idx, row in standardized_df.iterrows():
    plt.scatter(row['PointsPerGame'], row['AssistsPerGame'], color=colors(idx), s=100, label=f'Player #{idx+1}')

# Formatting
plt.xlim(-5, 5)
plt.ylim(-5, 5)

# set limits and grid
plt.xlabel('PointsPerGame')
plt.ylabel('AssistsPerGame')
plt.title('Eig"
lab,pca.ipynb,code,2,"enVectors of Dataset')
plt.grid(True)
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')

# show plot
plt.show()"
lab,pca.ipynb,markdown,1,"We consider the eigenvector with the largest eigenvalue to explain the most variance in our dataset. In this case, we can state that the first eigenvector explains 96% of variance in our dataset! Therefore, if we take the dot product of our first eigenvector & the dataset, we can express our data in just one dimension and still retain 96% of information."
lab,pca.ipynb,code,1,"# Project the standardized data onto the first principal component
pc1 = np.dot(standardized_df.values, sorted_eigenvectors[:, 0])

print('Projection onto first principal component:', pc1)"
lab,pca.ipynb,code,1,"plt.figure(figsize=(8, 1.5))
plt.scatter(pc1, np.zeros_like(pc1), c='blue', s=100) 
plt.yticks([])
plt.xlabel(""PCA Component 1"")
plt.title(""1D PCA Projection"")
plt.grid(True)
plt.show()"
lab,pca.ipynb,markdown,1,"This cell projects the standardized data onto the first principal component by taking the dot product of the data matrix with the eigenvector associated with the largest eigenvalue. The output shows the projected values, illustrating how the original multidimensional data is reduced onto a single axis representing maximum variance."
lab,pca.ipynb,markdown,1,"## PCA with Scikit-learn and Data Visualization

In this section we transition to using sklearn's PCA for dimensionality reduction and visualize the results. Let's see how we can extract better predictive capabilities on a highly dimensional dataset that contains 400 images of 40 different faces."
lab,pca.ipynb,code,1,"import seaborn as sns

from sklearn import decomposition
from sklearn import datasets

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

from sklearn.model_selection import train_test_split, RandomizedSearchCV"
lab,pca.ipynb,code,1,"faces = datasets.fetch_olivetti_faces()

X = faces.images
y = faces.target"
lab,pca.ipynb,code,1,X.shape
lab,pca.ipynb,markdown,1,"Each image is a 64 x 64 pixel grid. Each value expresses a normalized [gray-level value](https://en.wikipedia.org/wiki/Grayscale). Let's visualize what the first image looks like, and view its' respective class value."
lab,pca.ipynb,code,1,X[0].shape
lab,pca.ipynb,code,1,"plt.imshow(X[0], cmap='gray')
plt.title(f""Subject: {y[0]}"")
plt.axis('off')
plt.show()"
lab,pca.ipynb,markdown,1,Let's take a look at a few more faces.
lab,pca.ipynb,code,1,"plt.figure(figsize=(16, 6))

n_row, n_col = 3, 3
n_components = n_row * n_col
image_shape = (64, 64)

random_indices = np.random.choice(X.shape[0], n_components, replace=False)
sampled_faces = X[random_indices]

def plot_gallery(title, images, n_col=n_col, n_row=n_row):
    fig, axs = plt.subplots(
        nrows=n_row,
        ncols=n_col,
        figsize=(2.0 * n_col, 2.3 * n_row),
        constrained_layout=True,
    )
    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)
    fig.set_edgecolor(""black"")
    fig.suptitle(title, size=16)

    for ax, vec in zip(axs.flat, images):
        im = ax.imshow(
            vec.reshape(image_shape),
            cmap='gray',
 "
lab,pca.ipynb,code,2,"           interpolation=""nearest""
        )
        ax.axis(""off"")

    fig.colorbar(im, ax=axs, orientation=""horizontal"", shrink=0.99, aspect=40, pad=0.01)
    plt.show()

plot_gallery(""Faces from dataset"", sampled_faces)"
lab,pca.ipynb,markdown,1,"Let's see how a basic knn-classifier functions on this raw dataset. First we need to ""flatten"" this dataset by expreesing each 64 x 64 grid as an array of 4096 predictors."
lab,pca.ipynb,code,1,"# flatten the file
X_flat = faces.images.reshape((len(faces.images), -1))  # flatten to (n_samples, n_features)

X_flat.shape"
lab,pca.ipynb,markdown,1,Now we can take our typical steps of creating a train/test split and tuning a model.
lab,pca.ipynb,code,1,"y = faces.target

# train a knn classifier
X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.3, random_state=42)
X_train.shape"
lab,pca.ipynb,code,1,"# implement random-search on the knn model to find best hyperparams
params = {
    ""n_neighbors"": range(5, 100, 5),
    ""weights"": [""uniform"", ""distance""],
    ""metric"": [""cityblock"", ""cosine"", ""euclidean"", ""minkowski""]
}

knn = KNeighborsClassifier()

# set up GridSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(knn, param_distributions=params, cv=5, random_state=5)

# fit this model on your training data
random_search.fit(X_train, y_train)"
lab,pca.ipynb,code,1,"# extract the best estimator
best_knn = random_search.best_estimator_

# predict on testing data
yhat = best_knn.predict(X_test)

# evaluate its accuracy
confusion_mat = confusion_matrix(yhat, y_test)
class_report = classification_report(yhat, y_test)
accuracy = accuracy_score(yhat, y_test)

print(""Accuracy Score\n"",accuracy)
print(""Confusion Matrix\n"", confusion_mat)
print(""Classification Report\n"", class_report)"
lab,pca.ipynb,markdown,1,"Without any dimension decomposition, we achieve an accuracy of ~80%. Let's visualize which faces we predict accurately and which faces we're misclassifying."
lab,pca.ipynb,code,1,"X_images = faces.images

_, _, _, _, _, idx_test = train_test_split(
    X_flat, faces.target, np.arange(len(faces.images)),
    test_size=0.3, random_state=42
)

# Step 2: Visualize predictions with true labels
n_samples_to_plot = 16
sample_idxs = np.random.choice(len(yhat), n_samples_to_plot, replace=False)

plt.figure(figsize=(15, 6))
for i, idx in enumerate(sample_idxs):
    img = X_images[idx_test[idx]]
    true_label = y_test[idx]
    pred_label = yhat[idx]

    plt.subplot(4, 4, i + 1)
    plt.imshow(img, cmap='gray')
    title_color = 'green' if true_label == pred_label else 'red'
    plt.title(f""True: {true_label}\nPred: {pred_label}"", color=title_color, fontsize=10)
    plt.axis('o"
lab,pca.ipynb,code,2,"ff')

plt.suptitle(""Olivetti Faces – True vs Predicted (kNN)"", fontsize=16)
plt.tight_layout()
plt.show()"
lab,pca.ipynb,markdown,1,"While we cannot visualize 4096 dimensions, we can peek into the bivariate relationship of two different pixels to check if data is ""resting"" at some lower-dimensional manifold."
lab,pca.ipynb,code,1,"x1 = X_flat[:, 50]   # 50th pixel (starting from top-left corner)
x2 = X_flat[:, 51]   # pixel to the immediate right of the 50th

# Scatter plot
plt.figure(figsize=(7, 6))
scatter = plt.scatter(x1, x2, c=y, cmap='tab20', s=40, alpha=0.7)
plt.xlabel(""Pixel 0 Intensity"")
plt.ylabel(""Pixel 1 Intensity"")
plt.title(""Scatter Plot of Two Raw Pixel Dimensions"")
plt.grid(True)
plt.colorbar(scatter, label=""Person ID"")
plt.tight_layout()
plt.show()"
lab,pca.ipynb,markdown,1,"Amazing! It appears that we could potentially decompose these dimensions into a lower-dimensional space that minimizes variance and potentially provides better predictive capabilites. Let's try PCA out.

First, let's visualize the most ""important"" components of our faces."
lab,pca.ipynb,code,1,"from sklearn.decomposition import PCA

# initialize PCA to reduce data to 4 components for visualization
pca_estimator = PCA(n_components=4, svd_solver=""randomized"", whiten=True)

pca_estimator.fit(X_flat)
plot_gallery(""Eigenfaces - PCA using randomized SVD"", pca_estimator.components_, 2, 2)"
lab,pca.ipynb,markdown,1,"It appears that our PCA components are honing in on on the presence of glasses, a nose, eyes, and a mouth. We call this an [eigenface](https://en.wikipedia.org/wiki/Eigenface). Let's see if reducing our faces to these basic components result in better prediction."
lab,pca.ipynb,code,1,"# create 100 components using PCA
pca = PCA(n_components=100)

# convert our predictors variables to 2 basic components
X_pca = pca.fit_transform(X_flat)

# split into train & testing data
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)
X_train.shape"
lab,pca.ipynb,markdown,1,Amazing! We've decomposed 4096 dimensions into 100 predictors.
lab,pca.ipynb,code,1,"# implement random-search on the knn model to find best hyperparams
params = {
    ""n_neighbors"": range(5, 100, 5),
    ""weights"": [""uniform"", ""distance""],
    ""metric"": [""cityblock"", ""cosine"", ""euclidean"", ""minkowski""]
}

knn = KNeighborsClassifier()

# set up GridSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(knn, param_distributions=params, cv=5, random_state=5)

# fit this model on your training data
random_search.fit(X_train, y_train)"
lab,pca.ipynb,code,1,"# extract the best estimator
best_knn = random_search.best_estimator_

# predict on testing data
yhat = best_knn.predict(X_test)

# evaluate its accuracy
confusion_mat = confusion_matrix(yhat, y_test)
class_report = classification_report(yhat, y_test)
accuracy = accuracy_score(yhat, y_test)

print(""Accuracy Score\n"",accuracy)
print(""Confusion Matrix\n"", confusion_mat)
print(""Classification Report\n"", class_report)"
lab,pca.ipynb,markdown,1,"With dimension decomposition, we get an accuracy of ~88%. A clear sign of improvement!"
lab,pca.ipynb,code,1,"X_images = faces.images

_, _, _, _, _, idx_test = train_test_split(
    X_flat, faces.target, np.arange(len(faces.images)),
    test_size=0.3, random_state=42
)

# Step 2: Visualize predictions with true labels
n_samples_to_plot = 16
sample_idxs = np.random.choice(len(yhat), n_samples_to_plot, replace=False)

plt.figure(figsize=(15, 6))
for i, idx in enumerate(sample_idxs):
    img = X_images[idx_test[idx]]
    true_label = y_test[idx]
    pred_label = yhat[idx]

    plt.subplot(4, 4, i + 1)
    plt.imshow(img, cmap='gray')
    title_color = 'green' if true_label == pred_label else 'red'
    plt.title(f""True: {true_label}\nPred: {pred_label}"", color=title_color, fontsize=10)
    plt.axis('o"
lab,pca.ipynb,code,2,"ff')

plt.suptitle(""Olivetti Faces – True vs Predicted (kNN)"", fontsize=16)
plt.tight_layout()
plt.show()"
lab,pca.ipynb,markdown,1,Let's visualize what our new principal components look like in our dataset.
lab,pca.ipynb,code,1,"plt.figure(figsize=(7, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab20', s=40, alpha=0.7)
plt.xlabel(""PCA Component 1"")
plt.ylabel(""PCA Component 2"")
plt.title(""PCA Projection of Olivetti Faces (2D)"")
plt.grid(True)
plt.colorbar(scatter, label=""Person ID"")
plt.tight_layout()
plt.show()"
lab,pca.ipynb,markdown,1,"Notice that this scatter plot displays a dataset that utilizes the full ""breadth"" of the scatterplot, whereas the previous scatter plot displayed data that fell onto a lower manifold."
lab,pca.ipynb,markdown,1,"## Exploring Alternative Dimensionality Reduction Techniques

Keep in mind PCA is not the only dimension decomposition technique! There is also..
* t-SNE: useful for visualizing high-dimensional data in a non-linear way: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding 
* ICA: useful for capturing non-gaussian signals, and used in time series analysis: https://en.wikipedia.org/wiki/Independent_component_analysis
* MiniBatchKMeans: computationally efficient algorithm which can be used for online machine-learning: https://en.wikipedia.org/wiki/K-means_clustering"
lab,pca.ipynb,markdown,1,### T-SNE
lab,pca.ipynb,code,1,"from sklearn.manifold import TSNE

# Initialize t-SNE to reduce data to 2 dimensions
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)

X_tsne = tsne.fit_transform(X_flat)
X_train, X_test, y_train, y_test = train_test_split(X_tsne, y, test_size=0.3, random_state=42)

knn = KNeighborsClassifier()

# set up GridSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(knn, param_distributions=params, cv=5, random_state=5)
# fit this model on your training data
random_search.fit(X_train, y_train)
# extract the best estimator
best_knn = random_search.best_estimator_
# predict on testing data
yhat = best_knn.predict(X_test)
# evaluate its accuracy
accuracy = "
lab,pca.ipynb,code,2,"accuracy_score(yhat, y_test)

print(""Accuracy Score\n"",accuracy)"
lab,pca.ipynb,code,1,"plt.figure(figsize=(7, 6))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab20', s=40, alpha=0.7)
plt.xlabel(""T-SNE Embedding 1"")
plt.ylabel(""T-SNE Embedding 2"")
plt.title(""T-SNE Projection of Olivetti Faces (2D)"")
plt.grid(True)
plt.colorbar(scatter, label=""Person ID"")
plt.tight_layout()
plt.show()"
lab,pca.ipynb,markdown,1,### ICA
lab,pca.ipynb,code,1,"from sklearn.decomposition import FastICA

# Initialize t-SNE to reduce data to 2 dimensions
ica = FastICA(n_components=100, random_state=42)

X_ica  = tsne.fit_transform(X_flat)
X_train, X_test, y_train, y_test = train_test_split(X_ica , y, test_size=0.3, random_state=42)

knn = KNeighborsClassifier()

# set up GridSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(knn, param_distributions=params, cv=5, random_state=5)
# fit this model on your training data
random_search.fit(X_train, y_train)
# extract the best estimator
best_knn = random_search.best_estimator_
# predict on testing data
yhat = best_knn.predict(X_test)
# evaluate its accuracy
accuracy = accuracy_score(y"
lab,pca.ipynb,code,2,"hat, y_test)

print(""Accuracy Score\n"",accuracy)"
lab,pca.ipynb,code,1,"plt.figure(figsize=(7, 6))
scatter = plt.scatter(X_ica[:, 0], X_ica[:, 1], c=y, cmap='tab20', s=40, alpha=0.7)
plt.xlabel(""ICA Embedding 1"")
plt.ylabel(""ICA Embedding 2"")
plt.title(""ICA Projection of Olivetti Faces (2D)"")
plt.grid(True)
plt.colorbar(scatter, label=""Person ID"")
plt.tight_layout()
plt.show()"
lab,pca.ipynb,markdown,1,"### MiniBatchKmeans

Technically KMeans is **not** a dimensionality reduction algorithm. However, we can use the centroids of our unsupervised algorithm to detect cluster centers."
lab,pca.ipynb,code,1,"from sklearn.cluster import MiniBatchKMeans

# Cluster into 40 clusters (matches number of people in Olivetti)
kmeans = MiniBatchKMeans(n_clusters=10,
    tol=1e-3,
    batch_size=20,
    max_iter=50,
    random_state=42
)

# Fit
kmeans.fit(X_train)

# predict on testing data
yhat = kmeans.predict(X_test)
# evaluate its accuracy
accuracy = accuracy_score(yhat, y_test)

print(""Accuracy Score\n"",accuracy)"
lab,pca.ipynb,code,1,"plt.figure(figsize=(7, 6))
scatter = plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1])
plt.xlabel(""Cluster Center 1"")
plt.ylabel(""Cluster Center 2"")
plt.title(""KMeans Projection of Olivetti Faces (2D)"")
plt.grid(True)
plt.tight_layout()
plt.show()"
lab,nlp_vectors.ipynb,markdown,1,"# Sentiment Lab

Within the following lab, you will practice a few regex & word2vec concepts. Follow along with the guided code.

To get started, install the following packages."
lab,nlp_vectors.ipynb,code,1,"!pip install gensim
!pip install nltk"
lab,nlp_vectors.ipynb,code,1,"import pandas as pd

import matplotlib.pyplot as plt

# nltk download words
import nltk
from nltk.corpus import stopwords
from  nltk.stem import SnowballStemmer, WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Regex
import re

# Word2vec
from gensim.models import Word2Vec

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer"
lab,nlp_vectors.ipynb,markdown,1,"## EDA

First, let's explore some basic components of the twitter dataset regarding its text content and sentiment. Recall that we saw this dataset in phase 1!"
lab,nlp_vectors.ipynb,code,1,"# TODO: import your tweets dataset
..."
lab,nlp_vectors.ipynb,code,1,"# TODO: observe the first 5 rows
..."
lab,nlp_vectors.ipynb,code,1,"# TODO: view frequency of positive `1` and negative `0` tweets as a bar chart
..."
lab,nlp_vectors.ipynb,code,1,"# TODO: Using regex, find all rows that contain the word 'hate'
tweets[tweets[""tweet""].str.contains('hate')]"
lab,nlp_vectors.ipynb,code,1,"# TODO: Using regex, find all rows that contain the word 'love'
..."
lab,nlp_vectors.ipynb,markdown,1,"## BOW

Let's apply the bag of words technique to note similarities between tweets. Note that this entails no machine learning, but rather a simple vector operation on our pandas dataframe."
lab,nlp_vectors.ipynb,code,1,"from collections import Counter

bagged_words = tweets[""tweet""].apply(lambda x: Counter(x.lower().split("" "")))

bagged_words"
lab,nlp_vectors.ipynb,code,1,"# let's look at tweet 58 & 64 and check if our BOW transformation reveals any insights between these two tweets
bagged_words[58]"
lab,nlp_vectors.ipynb,code,1,bagged_words[64]
lab,nlp_vectors.ipynb,markdown,1,"## TF-IDF

Let's explore how the `tf-idf` algorithm iterates upon the simple BOW model."
lab,nlp_vectors.ipynb,code,1,"# since this is a heavy operation, sample a small subset of tweets
sample_tweets = tweets.sample(50, random_state=42).reset_index(drop=True)

sample_tweets.head()"
lab,nlp_vectors.ipynb,code,1,"# lowercase and remove punctuation (basic clean)
cleaned_tweets = sample_tweets[""tweet""].str.lower().str.replace(""[^\w\s]"", """", regex=True)

cleaned_tweets"
lab,nlp_vectors.ipynb,code,1,"from sklearn.feature_extraction.text import TfidfVectorizer

# initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# fit and transform
tfidf_matrix = vectorizer.fit_transform(cleaned_tweets)

# convert to DataFrame for display
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

tfidf_df.head()"
lab,nlp_vectors.ipynb,code,1,"# view top 3 TF-IDF terms per tweet
for idx, row in tfidf_df.iterrows():
    top_words = row.sort_values(ascending=False).head(3)
    print(f""\nTweet {idx+1}: \""{sample_tweets.loc[idx, 'tweet']}\"""")
    print(""Top TF-IDF words:"")
    print(top_words)"
lab,nlp_vectors.ipynb,markdown,1,"## Word2Vec

Finally let's utilize a word2vec model to analyze the content of our tweets. 

First, we'll go through a small toy-example to show off the power of word-2-vec. Next, we'll apply it to our dataset for its predictive capabilities."
lab,nlp_vectors.ipynb,code,1,"# TODO: word2vec example. Notice each step that we take and the output that it makes. 
...

# view the text data you are working with
frankenstein"
lab,nlp_vectors.ipynb,code,1,"# create a word lemmatizer (get the lemma of each word)
lemma = WordNetLemmatizer()

# break data into tokens and remove stop-words
data = []
for sent in frankenstein:
  new_sent = []
  for word in sent.split():
    new_word = word.lower()
    if (new_word[0] not in string.punctuation) and (new_word[0] not in stopwords.words('english')):
      new_sent.append(lemma.lemmatize(new_word))
  if len(new_sent) > 0:
    data.append(new_sent)

for sentence in data:
  print(sentence)"
lab,nlp_vectors.ipynb,code,1,"# Create a word2vec model
w2v_model = Word2Vec(sentences=data)

# train the model on your dataa
w2v_model.train(data, total_examples=len(data), epochs=50)"
lab,nlp_vectors.ipynb,code,1,"# note that each unique word gets its own unique key pairing. Let's see what features were learned from these words!
w2v_model.wv.key_to_index"
lab,nlp_vectors.ipynb,code,1,"# Let's the view output of one word in our word2vec model...
w2v_model.wv[""frankenstein""]

# doesn't make too much sense at the moment"
lab,nlp_vectors.ipynb,code,1,"# let's contextualize this! What are the most similar words to ""frankenstein""
w2v_model.wv.most_similar(""frankenstein"")"
lab,nlp_vectors.ipynb,code,1,"# now let's try something fun
# we will get the vector embedding of creation
creation = w2v_model.wv[""creation""]

# as well as the vector embedding of orrible
horrible = w2v_model.wv[""horrible""]

creation + horrible"
lab,nlp_vectors.ipynb,code,1,"# again, nothing too interesting happening yet, but let's see what are the most similar vectors to this addition
w2v_model.wv.most_similar(creation + horrible)"
lab,nlp_vectors.ipynb,markdown,1,"Now, let's apply this model to our 200,000 row dataset."
lab,nlp_vectors.ipynb,code,1,"# Preprocess your tweet column

stop_words = stopwords.words(""english"")
stemmer = SnowballStemmer(""english"")

TEXT_CLEANING_RE = ""@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+""

def preprocess(text, stem=False):
    # Remove link,user and special characters
    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            if stem:
                tokens.append(stemmer.stem(token))
            else:
                tokens.append(token)
    return "" "".join(tokens)"
lab,nlp_vectors.ipynb,code,1,"# Apply this preprocessing to the tweet column
tweets[""lemma_tweet""] = tweets[""tweet""].apply(lambda x: preprocess(x))"
lab,nlp_vectors.ipynb,code,1,"# get a sample of 5 random tweets
tweets[""lemma_tweet""].sample(5)"
lab,nlp_vectors.ipynb,code,1,"# create a train,test split
X_train, X_test = train_test_split(tweets[[""lemma_tweet""]], test_size=0.2, random_state=42)
print(""TRAIN size:"", len(X_train))
print(""TEST size:"", len(X_test))"
lab,nlp_vectors.ipynb,code,1,"# tokenize your data
sentences = [t.split() for t in X_train.lemma_tweet]"
lab,nlp_vectors.ipynb,code,1,sentences
lab,nlp_vectors.ipynb,code,1,"# create a word2vec model
w2v_model = Word2Vec()
w2v_model.build_vocab(sentences)"
lab,nlp_vectors.ipynb,code,1,"# take a look at the INITIAL vocab words that it discovered
w2v_model.wv.key_to_index"
lab,nlp_vectors.ipynb,code,1,"# let's train our model
w2v_model.train(sentences, total_examples=len(sentences), epochs=20)"
lab,nlp_vectors.ipynb,code,1,"# let's take a look at the vector we trained for the word 'lol'
w2v_model.wv[""lol""]"
lab,nlp_vectors.ipynb,code,1,"# as we've established already, this does not give us too much info, let's take a look at the MOST similair words
w2v_model.wv.most_similar(""lol"")

# AMAZING! Our word2vec model ""understands"" what 'lol' means
# feel free to play around with this word to see what other words come up"
lab,nlp_vectors.ipynb,code,1,"# just like before, let's combine two vectors via addition and see if the semantic output makes sense
morning = w2v_model.wv[""morning""]
breakfast = w2v_model.wv[""breakfast""]

w2v_model.wv.most_similar(morning + breakfast)"
lab,nlp_vectors.ipynb,code,1,"# finally, let's apply some PCA to each word in our model in order to reduce these values into a comprehensible visualization
# NOTE: we limit this to 100 words so that this doesn't take all night to run
words = list(w2v_model.wv.key_to_index.keys())[:100] 
word_vectors = [w2v_model.wv[word] for word in words]

word_vectors"
lab,nlp_vectors.ipynb,code,1,"from sklearn.decomposition import PCA

pca = PCA(n_components=2)

pca_vectors = pca.fit_transform(word_vectors)

plt.figure(figsize=(10, 10))
for i, word in enumerate(words):
    plt.scatter(pca_vectors[i, 0], pca_vectors[i, 1])
    plt.text(pca_vectors[i, 0]+0.03, pca_vectors[i, 1]+0.03, word, fontsize=9)
plt.show()"
lab,nlp_vectors.ipynb,markdown,1,"Notice that the model has sufficiently captured the semantic relationships of words! For example, `sleep` and `bed` are next to one another.

We will see how we can apply this to the understanding of LLM's by tomorrow. For now, let's consider a few reflection questions."
lab,nlp_vectors.ipynb,markdown,1,# Self-Reflective Questions
lab,nlp_vectors.ipynb,markdown,1,"## Q1

Which vectors (words) do you notice have high cosine similarity? Feel free to use `most_similar` to answer this?"
lab,nlp_vectors.ipynb,markdown,1,write answer here
lab,nlp_vectors.ipynb,markdown,1,"## Q2

Why do we apply PCA to this dataset?"
lab,nlp_vectors.ipynb,markdown,1,write answer here
lab,nlp_vectors.ipynb,markdown,1,"## Q3

Now that we've reduced our vectors to two PCA components, what unsupervised learning algorithm can we apply in order to find clusters of words?"
lab,nlp_vectors.ipynb,markdown,1,write answer here
lab,rag_case_study.ipynb,markdown,1,"# RAG Case Study

You are a data scientist working at an edutech startup. Your team is interested in developing a personalized data-science tutor which consults with expertly-crafted material in order to provide accurate & relevant information.

Your task is to:
* identify and download a subset of lecture slides as pdf's
* embed the text content from your downloaded pdf's to `openai`
* utilize the newly created embeddings to augment `gpt-4o-mini`'s responses

You will test your models response on the following set of prompts."
lab,rag_case_study.ipynb,code,1,"test_prompts = [ 
    ""Looking back to the Shopping Dataset Case Study slides, what do we increase in order to better observe the visual distributions that are apparent in our dataset?"",
    ""According to structured databases I, why is SQL a 'good idea'?""
    ""According to Introduction to Data Analytics II, what are the two techniques we have available to get around the problem of modification in place for pandas?""
]"
lab,rag_case_study.ipynb,code,1,"from openai import OpenAI

# access the specific OpenAI project
client = OpenAI(project=""proj_fHRnVJY0Oyfm1ufG1sffxa6W"")
# specify vector store id
vec_id = ""vs_689a4317fee48191b7cbc8df7089d9f5"""
lab,rag_case_study.ipynb,code,1,"import os

# TODO: import all relevant pdf's (hint: if you have more than 1 pdf, you should utilize a for-loop to loop through your folder of pdf's)
files = os.listdir(""."")

print(files)"
lab,rag_case_study.ipynb,code,1,"# TODO: test out your vector store by iteratively prompting test_prompts to `gpt-4-turbo` (hint: you should also use a for-loop here)
..."
lab,rag_case_study.ipynb,code,1,"# TODO: compare your outputs to non vector-store answers
..."
lab,vectors_rags.ipynb,markdown,1,"# Vector Databases & RAGs Code-Along

While the pre-trained weights of most LLMs provide a great baseline for natural language, and even ""intelligent"" behavior, requests for contextual/granular information often fall flat on their face.

For this reason, we must utilize vector databases & RAGs to imbue contextual information in our models. Follow along with the code below to find out more."
lab,vectors_rags.ipynb,code,1,!pip install faiss-cpu sentence-transformers transformers
lab,vectors_rags.ipynb,markdown,1,"## Vector Database

A vector database is a specialized system for storing and searching numerical vectors efficiently. Each vector usually represents a piece of unstructured data after being processed by an embedding model (think back to `word2vec` or the `audio2vec` models we worked with).

Instead of searching by keywords, a vector DB lets you search by semantic similarity. That is, we extract information that is not only **directly** related to our input prompt, but also information that **might** be related to our prompt (think back to cosine similarity).

We create a vector database by storing embeddings of the information we want our model to be aware of. This could be:
* legal documen"
lab,vectors_rags.ipynb,markdown,2,"ts
* copyrighted music
* or even pdf's of lecture slides

And one of the best parts is that if we use a private implementation of a vector database, we also don't have to share information with `Meta`, `Amazon`, `Microsoft` or any other large organization that would want to sniff around our data."
lab,vectors_rags.ipynb,markdown,1,"## Retrieval-Augmented-Generation (RAG)

Retrieval-Augmented Generation is a *technique* to improve an LLM's responses by:
* Retrieving relevant documents from a knowledge store (such a vector DB).
* Augmenting the model's prompt with those documents.
* Generating an answer using the model with this extra context.

With RAG, the model is handed the right documents at generation time. That is, the model does not respond to a user's queries until it refers to a specified set of documents. 

The vector DB acts as the ""memory"" that stores your domain knowledge, whereast the RAG pipeline is the ""brain"" that searches the vector DB for relevant content, combines the top result into the user prompt,"
lab,vectors_rags.ipynb,markdown,2," and feeds all information into the main LLM.

For example, let's explore our `distilgpt2` model again. As we recall, its' basic functionalities are comparable to a hamsters' at the moment."
lab,vectors_rags.ipynb,code,1,"from transformers import pipeline

model_id = ""distilgpt2""

pipe = pipeline(
    ""text-generation"",
    model=model_id,
    torch_dtype=""auto"",
    device_map=""auto""
)"
lab,vectors_rags.ipynb,code,1,"prompt = ""The first president of the United States was ""

out = pipe(prompt, max_new_tokens=10, do_sample=True, temperature=1.0)
print(out[0][""generated_text""])"
lab,vectors_rags.ipynb,markdown,1,"However, by giving the model a set of information which it can call back to using vector databases, we can improve its response.

First, we specify a list of relevant historical information."
lab,vectors_rags.ipynb,code,1,"facts = [
    ""First president: George Washington served 1789–1797; no formal party."",
    ""Second president: John Adams served 1797–1801; Federalist."",
    ""Third president: Thomas Jefferson served 1801–1809; Democratic-Republican."",
    ""Sixteenth president: Abraham Lincoln served 1861–1865; Republican; led the Union during the Civil War."",
    ""Twenty-sixth: Theodore Roosevelt served 1901–1909; Republican."",
    ""Thirty-second: Franklin D. Roosevelt served 1933–1945; Democratic; led during the Great Depression and WWII."",
    ""Thirty-fourth: Dwight D. Eisenhower served 1953–1961; Republican."",
    ""Thirty-fifth: John F. Kennedy served 1961–1963; Democratic; succeeded by Lyndon B. Johnson "
lab,vectors_rags.ipynb,code,2,"after his assassination."",
    ""Thirty-sixth: Lyndon B. Johnson served 1963–1969; Democratic."",
    ""Thirty-seventh: Richard Nixon served 1969–1974; Republican."",
    ""Thirty-eigth: Gerald Ford served 1974–1977; Republican."",
    ""Thirty-ninth: Jimmy Carter served 1977–1981; Democratic."",
    ""Fortieth: Ronald Reagan served 1981–1989; Republican."",
    ""Forty-first: George H. W. Bush served 1989–1993; Republican."",
    ""Forty-second: Bill Clinton served 1993–2001; Democratic."",
    ""Forty-third: George W. Bush served 2001–2009; Republican."",
    ""Forty-fourth: Barack Obama served 2009–2017; Democratic."",
    ""Forty-fifth: Donald Trump served 2017–2021; Republican."",
    ""Forty-sixth: Joe Bid"
lab,vectors_rags.ipynb,code,3,"en began 2021; Democratic.""
]"
lab,vectors_rags.ipynb,markdown,1,"Now that we have a prepared set of facts that we want the model to ""remember"", we convert all of these sentences into embeddings which can then be used to inform the model. We use the [SentenceTransformer](https://sbert.net/) object to convert all these sentences into vector embeddings which will then be stored on the [Facebook AI Similarity Search](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) (`FAISS`) which provides efficient similarity search & clustering of dense vectors."
lab,vectors_rags.ipynb,code,1,"from sentence_transformers import SentenceTransformer
import faiss     

embedder = SentenceTransformer('bert-base-nli-mean-tokens')
embeddings = embedder.encode(facts, normalize_embeddings=True) 

embeddings.shape"
lab,vectors_rags.ipynb,markdown,1,"Note that we retain the length of our original 'facts' list (19). However this time, we also have 768 additional dimensions which express some sort of semantic meaning of the sentence.

We could utilize this list of vector embeddings to find similar/disimilar facts. Let's envision a user trying to find more information on Ronald Reagan. They begin their prompt with ""Ronald Reagan was a..."" which we could express as a vector of 768 dimensions using our transformer model."
lab,vectors_rags.ipynb,code,1,"test_prompt = ""Ronald Reagan was a""

emb_prompt = embedder.encode([test_prompt], normalize_embeddings=True)
emb_prompt"
lab,vectors_rags.ipynb,markdown,1,"While this vector does not provide much information to us humans, we could calculate the cosine similarity of this embedding to all other sentences in our `facts` list and extract the fact that has the **most** similarity."
lab,vectors_rags.ipynb,code,1,"reagan_fact = facts[12]
washington_fact = facts[0]

print(""Reagan fact,"", reagan_fact)
print(""Washington fact,"", washington_fact)"
lab,vectors_rags.ipynb,code,1,"from numpy import dot
from numpy.linalg import norm

# get embeddings of the two facts
emb_reagan = embedder.encode([reagan_fact], normalize_embeddings=True)
emb_washington = embedder.encode([washington_fact], normalize_embeddings=True)

# calculate cosine similarity
sim_reagan = dot(emb_prompt, emb_reagan.T)/(norm(emb_prompt) * norm(emb_reagan))
sim_washington = dot(emb_prompt, emb_washington.T)/(norm(emb_prompt) * norm(emb_washington))

print(""Similarity of original prompt to Reagan fact"", sim_reagan)
print(""Similarity of original prompt to Washington fact"", sim_washington)"
lab,vectors_rags.ipynb,markdown,1,"To speed up the extraction of similar strings (also known as documents), we utilize the `FAISS` library. This provides us a database (similar to a SQL database) that specializes in searching highly dimensional vector embeddings."
lab,vectors_rags.ipynb,code,1,"index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)"
lab,vectors_rags.ipynb,markdown,1,"Now that we've added our embeddings to our vector database, we can perform operations such as the extraction of the top 3 similar facts. Let's evaluate our test prompt and see which documents are extracted."
lab,vectors_rags.ipynb,code,1,"# use FAISS to get top 3 similar vectors
top_k = 3

scores, ids = index.search(emb_prompt, top_k)

print(scores)
print(ids)"
lab,vectors_rags.ipynb,code,1,"print(""Top k results for prompt:"", test_prompt)

top_facts = []
for i in range(top_k):
    score = float(scores[0][i])
    ident = int(ids[0][i])
    fact = facts[int(ids[0][i])]
    top_facts.append(fact)

    print(""\nfact"", fact, ""score"", score, ""\nid"", ident)"
lab,vectors_rags.ipynb,markdown,1,"We can attach these top 3 facts to our original prompt (along with some system-level instructions) to help our LLM provide a more accurate answer. First let's generate our ""context"", which provides relevant information to our original prompt."
lab,vectors_rags.ipynb,code,1,"context = """"
for f in top_facts:
    context += ""\n"" + f

print(context)"
lab,vectors_rags.ipynb,markdown,1,"Next, let's generate our system-level instructions to ""point"" our model towards the context we constructed for it."
lab,vectors_rags.ipynb,code,1,"rag_prompt = (
    ""Use ONLY these facts to answer the question. If unknown, say 'Not in facts.'\n""
    f""Facts:\n{context}\n\n""
    f""{test_prompt}""
)

rag_prompt"
lab,vectors_rags.ipynb,markdown,1,"And now we can test out how are minimal ""RAG"" operates!"
lab,vectors_rags.ipynb,code,1,"out = pipe(rag_prompt, max_new_tokens=10, do_sample=True, temperature=0.5)
print(out[0][""generated_text""])"
lab,vectors_rags.ipynb,markdown,1,Notice that this vastly differs from what the models response would look like if we did not provide this contextual information.
lab,vectors_rags.ipynb,code,1,"out = pipe(test_prompt, max_new_tokens=10, do_sample=True, temperature=0.5)
print(out[0][""generated_text""])"
lab,vectors_rags.ipynb,markdown,1,"To confirm the validity of RAGs & Vector DB, let's try out this technique on our original prompt about the first US president. Except this time, let's make these techniques concrete by formalizing them into functions."
lab,vectors_rags.ipynb,code,1,"def get_top_k(emb_prompt, k=3):
    """"""Get top 3 similar facts to original prompt embedding
    """"""
    _, ids = index.search(emb_prompt, k)

    top_facts = []
    for i in ids[0]:
        fact = facts[i]
        top_facts.append(fact)
    
    return top_facts"
lab,vectors_rags.ipynb,code,1,"def construct_prompt(facts, og_prompt):
    """"""Create a system prompt using the top 3 facts to inform answers
    """"""
    context = """"
    for f in facts:
        context += ""\n"" + f

    prompt = (
        ""Use ONLY these facts to answer the question. If unknown, say 'Not in facts.'\n""
        f""Facts:\n{context}\n\n""
        f""{og_prompt}""
    )

    return prompt"
lab,vectors_rags.ipynb,code,1,"# generate embedding of original prompt
emb_prompt = embedder.encode([prompt], normalize_embeddings=True)

# get top 3 facts from original prompt
top_facts = get_top_k(emb_prompt, 3)

print(""Top k results for prompt:"", prompt)
top_facts"
lab,vectors_rags.ipynb,code,1,"# construct prompt using newly found context
rag_prompt = construct_prompt(top_facts, prompt)

print(""RAG Prompt"")
rag_prompt"
lab,vectors_rags.ipynb,code,1,"out = pipe(rag_prompt, max_new_tokens=10, do_sample=True, temperature=0.5)
print(out[0][""generated_text""])"
lab,vectors_rags.ipynb,markdown,1,"## Vector Databases & RAGS in OpenAI

As we can see from the above example however, it's going to take a lot more work to get `distilgpt2` to a capacity where it is able to assist in everyday business automation. Therefore, for the remainder of this exercise we will utilize the `openai` API. 

Particularly, we will explore how we could pass `pdfs` to the openai API. In this example, we will pass a pdf of ""presidential facts.""

Therefore, we will instead utilize the `openai` API to interact with a vector store. This will then be used to inform `gpt-4o-mini`'s answers. While a vector store is not as comprehensive as a [vector database](https://myscale.com/blog/vector-store-vs-vector-database-c"
lab,vectors_rags.ipynb,markdown,2,"omparison-guide/), this tool still allows us to efficiently search the semantic space to augment our responses.

Particularly we will interact with the vector store with the `id` ""proj_fHRnVJY0Oyfm1ufG1sffxa6W.""

To begin running the code below, copy & paste the provided API key in the code-block below.

**DO NOT PUSH THIS KEY TO GITHUB**  
**DO NOT PUSH THIS KEY TO GITHUB**  
**DO NOT PUSH THIS KEY TO GITHUB**"
lab,vectors_rags.ipynb,code,1,"api_key = ""..."""
lab,vectors_rags.ipynb,code,1,"from openai import OpenAI

# access the specific OpenAI project
client = OpenAI(project=""proj_fHRnVJY0Oyfm1ufG1sffxa6W"")
# specify vector store id
vec_id = ""vs_689a38d0704081919c7e463d0efd9dfb"""
lab,vectors_rags.ipynb,markdown,1,"Here we initialize our `openai` client and specify which vector store we will use.

We opent this file the same way we would for basic `I/O` operations. We can then create this file in our remote vector store."
lab,vectors_rags.ipynb,code,1,"# open the pdf file and create an object which could be interpreted by openai
with open(""pres_facts.pdf"", ""rb"") as file_obj:
    f = client.files.create(file=file_obj, purpose=""assistants"")

    # push pdf to vector store
    client.vector_stores.files.create(
        vector_store_id=vec_id,
        file_id=f.id,
    )"
lab,vectors_rags.ipynb,markdown,1,"Now that we have our file within our vector store, we can utilize it to augment our responses. Note that the only thing we need to specify are our model ""tools."" This allows our model to search a specific vector store for more information.

```
[
    {
        ""type"": ""file_search"",
        ""vector_store_ids"": (vector store id that contains our file info)
    }
]
```"
lab,vectors_rags.ipynb,code,1,"resp = client.responses.create(
    model='gpt-4-turbo',
    input='How much did James Madison weigh?',
    tools=[{""type"": ""file_search"", ""vector_store_ids"": [vec_id]}],
)

print(resp.output_text)"
lab,vectors_rags.ipynb,markdown,1,"Again, let's see how our model works without the addition of our vector files."
lab,vectors_rags.ipynb,code,1,"resp = client.responses.create(
    model='gpt-4-turbo',
    input='How much did James Madison weigh?'
)

print(resp.output_text)"
lab,vectors_rags.ipynb,markdown,1,"Now that we know how to augment our responses, let's try out the `rag_case_study.ipynb` exercise!"
