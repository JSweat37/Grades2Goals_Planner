{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54366a32",
   "metadata": {},
   "source": [
    "# Linear Algebra Foundations\n",
    "\n",
    "Understanding linear algebra is essential for grasping how machine learning especially neural networks operate. Here’s a concise review of key concepts:\n",
    "\n",
    "--\n",
    "\n",
    "## Vectors and Matrices\n",
    "\n",
    "A **vector** is an ordered list of numbers, often written as a column or row. Formally, an $n$-dimensional vector is:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- 2D vector:  \n",
    "    $$\n",
    "    \\mathbf{a} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}\n",
    "    $$\n",
    "- 3D vector:  \n",
    "    $$\n",
    "    \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 4 \\end{bmatrix}\n",
    "    $$\n",
    "- 4D vector:  \n",
    "    $$\n",
    "    \\mathbf{c} = \\begin{bmatrix} 0 \\\\ 3 \\\\ 8 \\\\ -2 \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "Vectors can represent features, inputs, or outputs in neural networks, with the dimension corresponding to the number of features or neurons. Vectors and matrices are the building blocks of linear algebra and are fundamental to machine learning and neural networks.\n",
    "\n",
    "- **Vectors** are ordered lists of numbers, often representing features, data points, or weights. For example, a vector can represent the input features to a neural network layer:\n",
    "  \n",
    "  $$\n",
    "  \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n",
    "  $$\n",
    "  \n",
    "  In code, a vector might look like `np.array([5, 7])` for a 2D vector, or `np.array([2, -1, 4])` for a 3D vector.\n",
    "\n",
    "- **Matrices** are rectangular arrays of numbers arranged in rows and columns. They are used to organize data and perform transformations. In neural networks, the weights connecting layers are typically stored in matrices. For example, a matrix $A$ with shape $2 \\times 3$:\n",
    "  \n",
    "  $$\n",
    "  A = \\begin{bmatrix}\n",
    "  1 & 2 & 3 \\\\\n",
    "  4 & 5 & 6\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  \n",
    "  In code, this is represented as `np.array([[1, 2, 3], [4, 5, 6]])`.\n",
    "\n",
    "**Types of matrices commonly used:**\n",
    "- **Square matrix:** Same number of rows and columns (e.g., $2 \\times 2$).\n",
    "  \n",
    "  $$\n",
    "  S = \\begin{bmatrix}\n",
    "  7 & 8 \\\\\n",
    "  9 & 10\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Identity matrix:** Diagonal elements are 1, others are 0. Acts as a multiplicative identity.\n",
    "  \n",
    "  $$\n",
    "  I = \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Zero matrix:** All elements are zero.\n",
    "  \n",
    "  $$\n",
    "  Z = \\begin{bmatrix}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Diagonal matrix:** Only diagonal elements are nonzero.\n",
    "  \n",
    "  $$\n",
    "  D = \\begin{bmatrix}\n",
    "  3 & 0 \\\\\n",
    "  0 & 5\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "**Why they matter:**\n",
    "- Vectors represent data, weights, and activations.\n",
    "- Matrices organize weights and data batches, and enable efficient computation through matrix multiplication.\n",
    "- Understanding their properties (such as shape, transpose, and special types) is crucial for implementing and debugging neural networks.\n",
    "\n",
    "- **Vectors** represent data points or features. In neural networks, inputs, weights, and outputs are often expressed as vectors.\n",
    "- **Matrices** are used to organize collections of vectors and perform transformations. For example, the weights connecting layers in a neural network are typically stored in matrices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8933f4",
   "metadata": {},
   "source": [
    "\n",
    "## Norm\n",
    "\n",
    "### Norm\n",
    "\n",
    "The norm of a vector or matrix is a measure of its \"size\" or \"length.\" The most common is the Euclidean (L2) norm, which is the square root of the sum of the squares of the elements.\n",
    "\n",
    "#### Example: Vector Norm\n",
    "\n",
    "For a vector $\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, the L2 norm is:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{u}\\|_2 = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14} \\approx 3.74\n",
    "$$\n",
    "\n",
    "#### Example: Matrix Norm\n",
    "\n",
    "For a matrix $A$, the Frobenius norm is:\n",
    "\n",
    "$$\n",
    "\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}\n",
    "$$\n",
    "\n",
    "For\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "3 & 4 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the Frobenius norm is:\n",
    "\n",
    "$$\n",
    "\\|A\\|_F = \\sqrt{1^2 + 2^2 + 3^2 + 3^2 + 4^2 + 5^2} = \\sqrt{1 + 4 + 9 + 9 + 16 + 25} = \\sqrt{64} = 8\n",
    "$$\n",
    "\n",
    "**Interpretation:**  \n",
    "- Norms are used to measure distances, regularize weights (prevent overfitting), and quantify errors in neural networks.  \n",
    "- L1 norm (sum of absolute values) and L2 norm (Euclidean) are common in machine learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63010f",
   "metadata": {},
   "source": [
    "## Element-wise Operations\n",
    "\n",
    "In linear algebra and neural networks, **element-wise operations** refer to performing a specific operation on each individual element of a vector or matrix. These operations are fundamental for data transformation and are heavily used in tasks like normalization and activation functions.\n",
    "\n",
    "### Scalars and Broadcasting\n",
    "\n",
    "A **scalar** is a single numerical value. When a scalar is added to, subtracted from, multiplied by, or divided with a vector or matrix, the operation is applied **individually to each element** — a behavior known as **broadcasting**.\n",
    "\n",
    "For example, multiplying a vector by a scalar:\n",
    "\n",
    "$$\n",
    "2 \\cdot \\begin{bmatrix} 1 \\\\\\\\ 2 \\\\\\\\ 3 \\end{bmatrix} =\n",
    "\\begin{bmatrix} 2 \\cdot 1 \\\\\\\\ 2 \\cdot 2 \\\\\\\\ 2 \\cdot 3 \\end{bmatrix} =\n",
    "\\begin{bmatrix} 2 \\\\\\\\ 4 \\\\\\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Broadcasting is commonly used to **scale** or **shift** data, such as during normalization or when applying a bias term.\n",
    "\n",
    "#### Example: Element-wise Vector Operations\n",
    "\n",
    "Let the vectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \\begin{bmatrix} 1 \\\\\\\\ 2 \\\\\\\\ 3 \\end{bmatrix}, \\quad\n",
    "\\mathbf{v} = \\begin{bmatrix} 4 \\\\\\\\ 5 \\\\\\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 1. **Addition**\n",
    "\n",
    "Each corresponding pair of elements is added:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} + \\mathbf{v} =\n",
    "\\begin{bmatrix} 1 + 4 \\\\\\\\ 2 + 5 \\\\\\\\ 3 + 6 \\end{bmatrix} =\n",
    "\\begin{bmatrix} 5 \\\\\\\\ 7 \\\\\\\\ 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. **Element-wise Multiplication** (Hadamard Product)\n",
    "\n",
    "Each element is multiplied with the corresponding element of the other vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\circ \\mathbf{v} =\n",
    "\\begin{bmatrix} 1 \\cdot 4 \\\\\\\\ 2 \\cdot 5 \\\\\\\\ 3 \\cdot 6 \\end{bmatrix} =\n",
    "\\begin{bmatrix} 4 \\\\\\\\ 10 \\\\\\\\ 18 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 3. **Applying a Function (e.g., Squaring)**\n",
    "\n",
    "Functions such as square, square root, ReLU, sigmoid, or tanh are often applied element-wise:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}^2 =\n",
    "\\begin{bmatrix} 1^2 \\\\\\\\ 2^2 \\\\\\\\ 3^2 \\end{bmatrix} =\n",
    "\\begin{bmatrix} 1 \\\\\\\\ 4 \\\\\\\\ 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This principle applies to **activation functions** in neural networks, where functions like ReLU, sigmoid, or tanh are applied to each neuron output individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8e7b4",
   "metadata": {},
   "source": [
    "## Vector-on-Vector Element-wise Operations\n",
    "\n",
    "Element-wise operations between vectors apply a binary operation (such as addition or multiplication) to each corresponding pair of elements from two vectors of the same size. These are also called **Hadamard operations** when referring to multiplication.\n",
    "\n",
    "Let the vectors be:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \\begin{bmatrix} 1 \\\\\\\\ 2 \\\\\\\\ 3 \\end{bmatrix}, \\quad\n",
    "\\mathbf{v} = \\begin{bmatrix} 4 \\\\\\\\ 5 \\\\\\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "1. **Element-wise Addition**\n",
    "\n",
    "\n",
    "Each corresponding pair of elements is added:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} + \\mathbf{v} =\n",
    "\\begin{bmatrix}\n",
    "1 + 4 \\\\\\\\\n",
    "2 + 5 \\\\\\\\\n",
    "3 + 6\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "5 \\\\\\\\\n",
    "7 \\\\\\\\\n",
    "9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "2. **Element-wise Multiplication** (Hadamard Product which is NOT the dot product)\n",
    "\n",
    "\n",
    "Each element of $\\mathbf{u}$ is multiplied by the corresponding element of $\\mathbf{v}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\circ \\mathbf{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\cdot 4 \\\\\\\\\n",
    "2 \\cdot 5 \\\\\\\\\n",
    "3 \\cdot 6\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\\\\\n",
    "10 \\\\\\\\\n",
    "18\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. **Element-wise Function Application (e.g., Squaring $\\mathbf{u}$)**\n",
    "\n",
    "\n",
    "You can also apply a function to each element of a single vector independently:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}^2 =\n",
    "\\begin{bmatrix}\n",
    "1^2 \\\\\\\\\n",
    "2^2 \\\\\\\\\n",
    "3^2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\\\\\n",
    "4 \\\\\\\\\n",
    "9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "These operations are fundamental in machine learning and neural networks, where transformations are often applied element-wise to vectors or matrices (e.g., during activation, loss computation, or feature scaling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84b3b",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "The **dot product** (also called the **inner product**) of two vectors produces a **scalar** — a single number — by multiplying corresponding elements and summing the results. It is written using a **dot** notation: $\\mathbf{a} \\cdot \\mathbf{b}$.\n",
    "\n",
    "### Dot Product Formula\n",
    "\n",
    "Given two vectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\begin{bmatrix} a_1 \\\\\\\\ a_2 \\\\\\\\ a_3 \\end{bmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{bmatrix} b_1 \\\\\\\\ b_2 \\\\\\\\ b_3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The dot product is:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} =\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\\\\\n",
    "a_2 \\\\\\\\\n",
    "a_3\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\\\\\n",
    "b_2 \\\\\\\\\n",
    "b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3\n",
    "$$\n",
    "\n",
    "In general:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i\n",
    "$$\n",
    "\n",
    "### Example: Dot Product Calculation\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\begin{bmatrix} 2 \\\\\\\\ 5 \\\\\\\\ 7 \\end{bmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{bmatrix} 3 \\\\\\\\ 4 \\\\\\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = (2 \\cdot 3) + (5 \\cdot 4) + (7 \\cdot 1) = 6 + 20 + 7 = 33\n",
    "$$\n",
    "\n",
    "**Result:**\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = 33\n",
    "$$\n",
    "\n",
    "This is a **scalar**, not a vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65811d",
   "metadata": {},
   "source": [
    "### Matrix Multiplication (Dot Product Generalized)\n",
    "\n",
    "The **dot product** of vectors produces a scalar. When generalized to matrices, this becomes **matrix multiplication**, where each entry of the result is a dot product between a **row** of the first matrix and a **column** of the second matrix.\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\\\\\n",
    "4 & 5 & 6 \\\\\\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix}\n",
    "9 & 8 & 7 \\\\\\\\\n",
    "6 & 5 & 4 \\\\\\\\\n",
    "3 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To compute the matrix product $C = AB$, each element $C_{ij}$ is calculated as the **dot product** of the $i$-th row of $A$ and the $j$-th column of $B$:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{3} A_{ik} \\cdot B_{kj}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Example\n",
    "\n",
    "We compute each element of $C = AB$:\n",
    "\n",
    "- **First row of A** $\\cdot$ **First column of B**:\n",
    "  $$\n",
    "  C_{11} = (1 \\cdot 9) + (2 \\cdot 6) + (3 \\cdot 3) = 9 + 12 + 9 = 30\n",
    "  $$\n",
    "\n",
    "- **First row of A** $\\cdot$ **Second column of B**:\n",
    "  $$\n",
    "  C_{12} = (1 \\cdot 8) + (2 \\cdot 5) + (3 \\cdot 2) = 8 + 10 + 6 = 24\n",
    "  $$\n",
    "\n",
    "- **First row of A** $\\cdot$ **Third column of B**:\n",
    "  $$\n",
    "  C_{13} = (1 \\cdot 7) + (2 \\cdot 4) + (3 \\cdot 1) = 7 + 8 + 3 = 18\n",
    "  $$\n",
    "\n",
    "- **Second row of A** $\\cdot$ **First column of B**:\n",
    "  $$\n",
    "  C_{21} = (4 \\cdot 9) + (5 \\cdot 6) + (6 \\cdot 3) = 36 + 30 + 18 = 84\n",
    "  $$\n",
    "\n",
    "- **Second row of A** $\\cdot$ **Second column of B**:\n",
    "  $$\n",
    "  C_{22} = (4 \\cdot 8) + (5 \\cdot 5) + (6 \\cdot 2) = 32 + 25 + 12 = 69\n",
    "  $$\n",
    "\n",
    "- **Second row of A** $\\cdot$ **Third column of B**:\n",
    "  $$\n",
    "  C_{23} = (4 \\cdot 7) + (5 \\cdot 4) + (6 \\cdot 1) = 28 + 20 + 6 = 54\n",
    "  $$\n",
    "\n",
    "- **Third row of A** $\\cdot$ **First column of B**:\n",
    "  $$\n",
    "  C_{31} = (7 \\cdot 9) + (8 \\cdot 6) + (9 \\cdot 3) = 63 + 48 + 27 = 138\n",
    "  $$\n",
    "\n",
    "- **Third row of A** $\\cdot$ **Second column of B**:\n",
    "  $$\n",
    "  C_{32} = (7 \\cdot 8) + (8 \\cdot 5) + (9 \\cdot 2) = 56 + 40 + 18 = 114\n",
    "  $$\n",
    "\n",
    "- **Third row of A** $\\cdot$ **Third column of B**:\n",
    "  $$\n",
    "  C_{33} = (7 \\cdot 7) + (8 \\cdot 4) + (9 \\cdot 1) = 49 + 32 + 9 = 90\n",
    "  $$\n",
    "\n",
    "#### Final Result:\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "C = AB = \\begin{bmatrix}\n",
    "30 & 24 & 18 \\\\\\\\\n",
    "84 & 69 & 54 \\\\\\\\\n",
    "138 & 114 & 90\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Key Point\n",
    "\n",
    "Matrix multiplication is **not** element-wise. It involves **dot products** between rows and columns, resulting in new combinations of values — not just simple multiplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "299c671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: [[ 30  24  18]\n",
      " [ 84  69  54]\n",
      " [138 114  90]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#a = np.array([2, 5, 7])\n",
    "a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "#b = np.array([3, 4, 1])\n",
    "b = np.array([[9,8,7], [6,5,4], [3,2,1]])\n",
    "dot_product = np.dot(a, b)\n",
    "print(\"Dot product:\", dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce1af7",
   "metadata": {},
   "source": [
    "### Summary: Dot Product vs. Matrix Multiplication\n",
    "\n",
    "- The **dot product** takes two vectors and returns a scalar.\n",
    "- **Matrix multiplication** generalizes this idea: each element in the result is a dot product between a row of the first matrix and a column of the second.\n",
    "\n",
    "\n",
    "\n",
    "This is how the \"dot product\" generalizes to matrices: each entry in the result is a dot product of a row from $A$ and a column from $B$.\n",
    "\n",
    "### Dot Product vs. Element-wise Multiplication\n",
    "\n",
    "| Operation                  | Symbol     | Output Type | Description                                         |\n",
    "|---------------------------|------------|---------------|-----------------------------------------------------|\n",
    "| Dot product               | $\\cdot$    | Scalar/Matrix | Multiply and sum: $a_1b_1 + a_2b_2 + \\dots$         |\n",
    "| Element-wise multiplication | $\\circ$    | Vector      | Multiply each element separately: $[a_1b_1, a_2b_2, \\dots]$ |\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- The **dot product** is used in neural networks to compute the **weighted sum** of inputs before applying an activation function — this is the basic operation of a neuron.\n",
    "- **Element-wise operations**, on the other hand, are used for scaling, normalization, and activation functions that apply independently to each element.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418072c7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Transpose\n",
    "\n",
    "The transpose of a matrix flips its rows and columns. It is often used to align dimensions for multiplication.\n",
    "\n",
    "### Example: Matrix Transpose\n",
    "\n",
    "Suppose we have a matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The transpose of $A$, denoted $A^\\top$, is:\n",
    "\n",
    "$$\n",
    "A^\\top = \\begin{bmatrix}\n",
    "1 & 4 \\\\\n",
    "2 & 5 \\\\\n",
    "3 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Result:**  \n",
    "Rows become columns and columns become rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff730c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[1 2 3]\n",
      " [3 4 5]]\n",
      "A.reshape(6, 1):\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "A.reshape(1, 6):\n",
      "[[1 2 3 3 4 5]]\n",
      "A.reshape(2, 3):\n",
      "[[1 2 3]\n",
      " [3 4 5]]\n",
      "A.reshape(3, 2):\n",
      "[[1 2]\n",
      " [3 3]\n",
      " [4 5]]\n",
      "A:\n",
      "[[1 2 3]\n",
      " [3 4 5]]\n",
      "transpose:\n",
      "[[1 3]\n",
      " [2 4]\n",
      " [3 5]]\n"
     ]
    }
   ],
   "source": [
    "# try changing the shape of the array\n",
    "A = np.array([[1, 2, 3], [3, 4, 5]])\n",
    "print(f\"A\\n{A}\")\n",
    "B = A.reshape(6, 1)\n",
    "print(f\"A.reshape(6, 1):\\n{B}\")\n",
    "C = A.reshape(1, 6)\n",
    "print(f\"A.reshape(1, 6):\\n{C}\")\n",
    "D = A.reshape(2, 3)\n",
    "print(f\"A.reshape(2, 3):\\n{D}\")\n",
    "E = A.reshape(3, 2)\n",
    "print(f\"A.reshape(3, 2):\\n{E}\")\n",
    "print(f\"A:\\n{A}\")\n",
    "transpose = A.T\n",
    "print(f\"transpose:\\n{transpose}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6a426",
   "metadata": {},
   "source": [
    "\n",
    "## Determinant\n",
    "\n",
    "The determinant is a scalar value that can be computed from a square matrix. It provides important information about the matrix, such as whether it is invertible and how it scales space.\n",
    "\n",
    "The determinant tells us important things about a matrix. In general, it helps us know if a matrix can be inverted (reversed), how it changes the size or orientation of shapes when used for transformations, and whether a system of linear equations has a unique solution. If the determinant is zero, the matrix cannot be inverted and the system may not have a unique solution.\n",
    "\n",
    "The general formula for the determinant of an $n \\times n$ matrix $A$ is:\n",
    "\n",
    "$$\n",
    "\\det(A) = \\sum_{\\sigma \\in S_n} \\mathrm{sgn}(\\sigma) \\prod_{i=1}^n a_{i, \\sigma(i)}\n",
    "$$\n",
    "\n",
    "where $S_n$ is the set of all permutations of $\\{1, 2, \\dots, n\\}$ and $\\mathrm{sgn}(\\sigma)$ is the sign of the permutation $\\sigma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af015e0b",
   "metadata": {},
   "source": [
    "\n",
    "### The determinant of a $2 \\times 2$ matrix is calculated as:\n",
    "\n",
    "$$\n",
    "\\det\\left(\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\\right) = ad - bc\n",
    "$$\n",
    "\n",
    "**Example** Let's compute the determinant of a $2 \\times 2$ matrix using an example:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The determinant is:\n",
    "\n",
    "$$\n",
    "\\det(A) = (1 \\times 4) - (2 \\times 3) = 4 - 6 = -2\n",
    "$$\n",
    "\n",
    "\n",
    "### For a $3 \\times 3$ matrix the general formula is:\n",
    "\n",
    "$$\n",
    "\\det\\left(\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix}\\right) = aei + bfg + cdh - ceg - bdi - afh\n",
    "$$\n",
    "\n",
    "### However, it is often easier to use the Cofactor Expansion formula:\n",
    "\n",
    "For a $3 \\times 3$ matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a & b & c \\\\\\\\\n",
    "d & e & f \\\\\\\\\n",
    "g & h & i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We compute the determinant by expanding along the **first row**:\n",
    "\n",
    "$$\n",
    "\\det(A) =\n",
    "a \\cdot \\det\\begin{bmatrix} e & f \\\\\\\\ h & i \\end{bmatrix}\n",
    "- b \\cdot \\det\\begin{bmatrix} d & f \\\\\\\\ g & i \\end{bmatrix}\n",
    "+ c \\cdot \\det\\begin{bmatrix} d & e \\\\\\\\ g & h \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each $2 \\times 2$ determinant is computed using the rule: $ad - bc$.\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "3 & 4 & 5 \\\\\n",
    "6 & 7 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The determinant of $C$ is:\n",
    "To derive the $3 \\times 3$ determinant formula from the definition, expand along the first row:\n",
    "\n",
    "\n",
    "$$\n",
    "\\det(C) = 1 \\cdot\n",
    "\\begin{vmatrix}\n",
    "4 & 5 \\\\\n",
    "7 & 8\n",
    "\\end{vmatrix}\n",
    "- 2 \\cdot\n",
    "\\begin{vmatrix}\n",
    "3 & 5 \\\\\n",
    "6 & 8\n",
    "\\end{vmatrix}\n",
    "+ 3 \\cdot\n",
    "\\begin{vmatrix}\n",
    "3 & 4 \\\\\n",
    "6 & 7\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Each $2 \\times 2$ determinant is:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{vmatrix}\n",
    "= ad - bc\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\det(C) = 1 \\cdot (4 \\times 8 - 5 \\times 7)\n",
    "- 2 \\cdot (3 \\times 8 - 5 \\times 6)\n",
    "+ 3 \\cdot (3 \\times 7 - 4 \\times 6) \n",
    "$$\n",
    "\n",
    "So, the determinant is:\n",
    "\n",
    "$$\n",
    "\\det(C) = 1 \\cdot (4 \\times 8 - 5 \\times 7)\n",
    "- 2 \\cdot (3 \\times 8 - 5 \\times 6)\n",
    "+ 3 \\cdot (3 \\times 7 - 4 \\times 6) \n",
    "= 1 \\cdot (32 - 35) - 2 \\cdot (24 - 30) + 3 \\cdot (21 - 24)\n",
    "= 1 \\cdot (-3) - 2 \\cdot (-6) + 3 \\cdot (-3)\n",
    "= -3 + 12 - 9 = 0\n",
    "$$\n",
    "\n",
    "Therefore, $\\det(C) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc67bdd",
   "metadata": {},
   "source": [
    "\n",
    "#### Interpretation\n",
    "\n",
    "- If the determinant is zero, the matrix is singular (not invertible).\n",
    "- If the determinant is nonzero, the matrix is invertible.\n",
    "\n",
    "Determinants are important for understanding the properties of transformations in neural networks and for solving systems of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382bf1c",
   "metadata": {},
   "source": [
    "\n",
    "#### Using NumPy\n",
    "\n",
    "You can compute determinants in code using NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d9924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of C: (should be 0)\n",
      "-0.0000000000000009516197353929940532026773740116004442025297296522956536080073419725522398948669433593750000000000000000000000000000\n",
      "is close to zero? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "C = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "det_C = np.linalg.det(C)\n",
    "# Print the determinant with higher precision\n",
    "print(f\"Determinant of C: (should be 0)\\n{det_C:.130f}\")  # Show 130 decimal places\n",
    "print(f\"is close to zero? {np.isclose(det_C, 0)}\") # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a83edcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pure python determinant of C: 0\n"
     ]
    }
   ],
   "source": [
    "def determinant(matrix):\n",
    "    # Base case for 2x2 matrix\n",
    "    if len(matrix) == 2 and len(matrix[0]) == 2:\n",
    "        # Calculate determinant of 2x2 matrix\n",
    "        # return the determinant using the formula ad - bc\n",
    "        # ad - bc for matrix [[a, b], [c, d]]\n",
    "        # matrix\n",
    "        # ad = matrix[0][0] * matrix[1][1]\n",
    "        # bc = matrix[0][1] * matrix[1][0]\n",
    "        # return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n",
    "        return matrix[0][0]*matrix[1][1] - matrix[0][1]*matrix[1][0]\n",
    "    # Recursive case for larger matrices\n",
    "    det = 0\n",
    "    for col in range(len(matrix)):\n",
    "        # Build minor matrix\n",
    "        # minor = [row[:col] + row[col+1:] for row in matrix[1:]]\n",
    "        # This creates a new matrix excluding the first row and the current column\n",
    "        # The minor is the matrix obtained by removing the first row and the current column\n",
    "        # Calculate the cofactor\n",
    "        # cofactor = ((-1) ** col) * matrix[0][col]\n",
    "        # Multiply the cofactor by the determinant of the minor\n",
    "        # det += cofactor * determinant(minor)\n",
    "        # The determinant is the sum of the cofactors\n",
    "        # det += ((-1) ** col) * matrix[0][col] * determinant([row[:col] + row[col+1:] for row in matrix[1:]])\n",
    "        # We iterate over each column in the first row (matrix[0]) because the determinant is expanded along the first row.\n",
    "        # For each column index 'col', we exclude that column to form the minor matrix,\n",
    "        # and recursively compute its determinant. This follows the Laplace expansion (cofactor expansion) formula.\n",
    "        minor = [row[:col] + row[col+1:] for row in matrix[1:]]\n",
    "        cofactor = ((-1) ** col) * matrix[0][col] * determinant(minor)\n",
    "        det += cofactor\n",
    "    return det\n",
    "\n",
    "# Example usage:\n",
    "det_C_py = determinant(C.tolist())\n",
    "print(f\"pure python determinant of C: {det_C_py}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef96db",
   "metadata": {},
   "source": [
    "## Invertibility of Matrices\n",
    "\n",
    "A matrix is **invertible** (also called **nonsingular**) if and only if its determinant is **nonzero**. If the determinant is zero, the matrix is **singular** and does **not** have an inverse.\n",
    "\n",
    "## Why does the determinant matter?\n",
    "\n",
    "- **If $\\det(A) \\neq 0$:** $A$ is invertible (there exists $A^{-1}$ such that $A A^{-1} = I$).\n",
    "- **If $\\det(A) = 0$:** $A$ is not invertible (no $A^{-1}$ exists).\n",
    "\n",
    "## Example 1: Invertible $2 \\times 2$ Matrix\n",
    "\n",
    "Let\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute the determinant:\n",
    "$$\n",
    "\\det(M) = (1 \\times 4) - (2 \\times 3) = 4 - 6 = -2\n",
    "$$\n",
    "\n",
    "Since $\\det(M) \\neq 0$, $M$ is invertible.\n",
    "\n",
    "## Example 2: Singular $3 \\times 3$ Matrix\n",
    "\n",
    "Let\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute the determinant (as shown above):\n",
    "$$\n",
    "\\det(C) = 0\n",
    "$$\n",
    "\n",
    "Since $\\det(C) = 0$, $C$ is **not** invertible.\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Matrix | Determinant | Invertible? |\n",
    "|--------|-------------|-------------|\n",
    "| $M$    | $-2$        | Yes         |\n",
    "| $C$    | $0$         | No          |\n",
    "\n",
    "**In practice:**  \n",
    "- Always check the determinant before trying to compute a matrix inverse.\n",
    "- In neural networks and data science, invertibility is important for solving systems of equations and for certain algorithms (e.g., finding unique solutions, matrix decompositions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f206e0",
   "metadata": {},
   "source": [
    "## Inverse of a Matrix\n",
    "\n",
    "The **inverse** of a square matrix $ A $, denoted $ A^{-1} $, is a matrix such that:\n",
    "\n",
    "$$\n",
    "A A^{-1} = A^{-1} A = I\n",
    "$$\n",
    "\n",
    "where $ I $ is the identity matrix. A matrix is **invertible if and only if** its determinant is nonzero.\n",
    "\n",
    "### Formula for the Inverse (2×2 Matrix)\n",
    "\n",
    "For a $ 2 \\times 2 $ matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If \\( \\det(A) = ad - bc \\neq 0 \\), then:\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{ad - bc}\n",
    "\\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example: Inverse of a 2×2 Matrix\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\det(M) = 1 \\cdot 4 - 2 \\cdot 3 = -2 \\\\\n",
    "M^{-1} = \\frac{1}{-2} \\begin{bmatrix} 4 & -2 \\\\ -3 & 1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} -2 & 1 \\\\ 1.5 & -0.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Verify:\n",
    "\n",
    "$$\n",
    "M^{-1}M = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "= I\n",
    "$$\n",
    "\n",
    "### Inverse of a 3×3 Matrix\n",
    "\n",
    "For larger matrices, the inverse is found using:\n",
    "\n",
    "1. **Minors** – Determinants of $ 2 \\times 2 $ submatrices  \n",
    "2. **Cofactors** – Apply a checkerboard of signs to the minors  \n",
    "3. **Adjugate** – Transpose the cofactor matrix  \n",
    "4. **Inverse** – Divide the adjugate by $ \\det(A) $\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "6 & 1 & 1 \\\\\n",
    "4 & -2 & 5 \\\\\n",
    "2 & 8 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 1: Compute Determinant\n",
    "\n",
    "$$\n",
    "\\det(A) = 6 \\cdot (-54) - 1 \\cdot 18 + 1 \\cdot 36 = -306\n",
    "$$\n",
    "\n",
    "#### Step 2: Compute Minors and Cofactors\n",
    "\n",
    "Minors matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-54 & 18 & 36 \\\\\n",
    "-1 & 40 & -16 \\\\\n",
    "7 & 26 & -16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Apply checkerboard signs (Cofactors matrix):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-54 & -18 & 36 \\\\\n",
    "1 & 40 & 16 \\\\\n",
    "7 & -26 & -16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 3: Transpose to Get Adjugate\n",
    "\n",
    "$$\n",
    "\\text{adj}(A) = \n",
    "\\begin{bmatrix}\n",
    "-54 & 1 & 7 \\\\\n",
    "-18 & 40 & -26 \\\\\n",
    "36 & 16 & -16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 4: Compute Inverse\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{-306}\n",
    "\\begin{bmatrix}\n",
    "-54 & 1 & 7 \\\\\n",
    "-18 & 40 & -26 \\\\\n",
    "36 & 16 & -16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Verification: \\( A^{-1} A = I \\)\n",
    "\n",
    "To verify:\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{-306}\n",
    "\\begin{bmatrix}\n",
    "-54 & 1 & 7 \\\\\n",
    "-18 & 40 & -26 \\\\\n",
    "36 & 16 & -16\n",
    "\\end{bmatrix}, \\quad\n",
    "A = \\begin{bmatrix}\n",
    "6 & 1 & 1 \\\\\n",
    "4 & -2 & 5 \\\\\n",
    "2 & 8 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now compute the matrix product \\( A^{-1} A \\) (only showing final result):\n",
    "\n",
    "$$\n",
    "A^{-1} A =\n",
    "\\frac{1}{-306}\n",
    "\\begin{bmatrix}\n",
    "-306 & 0 & 0 \\\\\n",
    "0 & -306 & 0 \\\\\n",
    "0 & 0 & -306\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "= I\n",
    "$$\n",
    "\n",
    "✅ This confirms that the computed inverse is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "febbfc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[ 6  1  1]\n",
      " [ 4 -2  5]\n",
      " [ 2  8  7]]\n",
      "Inverse of A:\n",
      " [[ 0.17647059 -0.00326797 -0.02287582]\n",
      " [ 0.05882353 -0.13071895  0.08496732]\n",
      " [-0.11764706  0.1503268   0.05228758]]\n",
      "A * A^-1:\n",
      " [[ 1.00000000e+00  0.00000000e+00 -1.38777878e-17]\n",
      " [-8.32667268e-17  1.00000000e+00  8.32667268e-17]\n",
      " [-2.77555756e-17  1.11022302e-16  1.00000000e+00]]\n",
      "Is A * A^-1 close to identity matrix? True\n"
     ]
    }
   ],
   "source": [
    "# Compute the inverse of A using numpy\n",
    "A = np.array([[1,2], [3,4]])\n",
    "A = np.array([[6, 1, 1], [4, -2, 5], [2, 8, 7]])\n",
    "\n",
    "try:\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "    inv_A = np.linalg.inv(A)\n",
    "    print(\"Inverse of A:\\n\", inv_A)\n",
    "    # Verify the inverse by multiplying A and its inverse\n",
    "    identity = np.dot(A, inv_A)\n",
    "    print(\"A * A^-1:\\n\", identity)\n",
    "    print(\"Is A * A^-1 close to identity matrix?\", np.allclose(identity, np.eye(A.shape[0])))\n",
    "\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(\"Matrix A is not invertible:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99636b8a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "While not always directly used, understanding eigenvalues and eigenvectors helps in analyzing the stability and dynamics of learning algorithms.\n",
    "\n",
    "Imagine you have a special kind of transformation—like stretching, squishing, or rotating—applied to vectors (arrows) in space. Most vectors will change direction and length when you do this. But some rare, special vectors only get stretched or squished (their direction stays the same). These are called **eigenvectors**.\n",
    "\n",
    "The amount by which these special vectors are stretched or squished is called the **eigenvalue**.\n",
    "\n",
    "- **Eigenvector:** A direction that stays the same after the transformation (except for getting longer or shorter).\n",
    "- **Eigenvalue:** How much the eigenvector is stretched or squished.\n",
    "\n",
    "**Example:**  \n",
    "If you imagine pushing on a door, the axis the door rotates around doesn’t move—it’s like an eigenvector. The amount the door moves (how far it swings open) is like the eigenvalue.\n",
    "\n",
    "In math, for a matrix $A$, an eigenvector $\\mathbf{v}$ and eigenvalue $\\lambda$ satisfy:\n",
    "$$\n",
    "A\\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "This means applying $A$ to $\\mathbf{v}$ just scales it by $\\lambda$—the direction doesn’t change. Eigenvalues and eigenvectors help us understand what a matrix (or transformation) really does at its core.\n",
    "\n",
    "To find the **eigenvalues** $\\lambda$ of a square matrix $A$, solve the **characteristic equation**:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix of the same size as $A$.\n",
    "\n",
    "Once you have an eigenvalue $\\lambda$, the corresponding **eigenvectors** $\\mathbf{v}$ are the nonzero solutions to:\n",
    "\n",
    "$$\n",
    "(A - \\lambda I)\\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "That is, solve the homogeneous system above for $\\mathbf{v}$ for each eigenvalue $\\lambda$.\n",
    "\n",
    "\n",
    "### Example: Eigenvalues and Eigenvectors\n",
    "\n",
    "Suppose we have a square matrix:\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To find the eigenvalues $\\lambda$, solve the characteristic equation:\n",
    "\n",
    "$$\n",
    "\\det(M - \\lambda I) = 0\n",
    "$$\n",
    "Where $I$ is the identity matrix:\n",
    "First, write the matrix $M$ and the identity matrix $I$:\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}, \\quad\n",
    "I = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiply the identity matrix $I$ by the scalar $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\lambda I = \\begin{bmatrix}\n",
    "\\lambda & 0 \\\\\n",
    "0 & \\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Subtract $\\lambda I$ from $M$:\n",
    "\n",
    "$$\n",
    "M - \\lambda I = \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 0 \\\\\n",
    "0 & \\lambda\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 - \\lambda & 1 \\\\\n",
    "1 & 2 - \\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The determinant is:\n",
    "\n",
    "$$\n",
    "(2 - \\lambda)(2 - \\lambda) - (1 \\times 1) = (2 - \\lambda)^2 - 1 = 0\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "(2 - \\lambda)^2 = 1 \\\\\n",
    "2 - \\lambda = \\pm 1 \\\\\n",
    "\\lambda_1 = 2 - 1 = 1 \\\\\n",
    "\\lambda_2 = 2 + 1 = 3\n",
    "$$\n",
    "\n",
    "So, the eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 3$.\n",
    "\n",
    "To find the eigenvector for $\\lambda = 3$:\n",
    "\n",
    "$$\n",
    "(M - 3I)\\mathbf{v} = 0 \\\\\n",
    "\\begin{bmatrix}\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This gives $-v_1 + v_2 = 0$ and $v_1 - v_2 = 0$, so $v_1 = v_2$.\n",
    "\n",
    "An eigenvector for $\\lambda = 3$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, for $\\lambda = 1$:\n",
    "\n",
    "$$\n",
    "(M - I)\\mathbf{v} = 0 \\\\\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This gives $v_1 + v_2 = 0$, so $v_1 = -v_2$.\n",
    "\n",
    "An eigenvector for $\\lambda = 1$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Interpretation of Eigenvalues and Eigenvectors\n",
    "\n",
    "- **Eigenvalues ($\\lambda_1 = 1$, $\\lambda_2 = 3$):**\n",
    "    - The eigenvalues represent the amount of variance or \"stretching\" along their corresponding eigenvector directions.\n",
    "    - The larger eigenvalue ($3$) indicates the direction along which the data (or transformation) has the greatest effect or variance.\n",
    "    - The smaller eigenvalue ($1$) indicates a direction with less effect or variance.\n",
    "\n",
    "- **Eigenvectors ($\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$):**\n",
    "    - The eigenvector $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ (for $\\lambda = 3$) points along the line where both variables increase together. In the sensor example, this means both temperature and humidity rise or fall together—this is the main trend in the data.\n",
    "    - The eigenvector $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ (for $\\lambda = 1$) points along the line where one variable increases as the other decreases. This direction captures the contrast between temperature and humidity.\n",
    "\n",
    "- **Geometric Meaning:**\n",
    "    - If you transform a set of points using matrix $M$, points along the $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ direction will be stretched by a factor of $3$, while points along the $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ direction will be stretched by a factor of $1$ (i.e., unchanged in length).\n",
    "    - In PCA, projecting data onto the eigenvector with the largest eigenvalue gives the principal component—showing the direction of maximum variance.\n",
    "\n",
    "- **Practical Implication:**\n",
    "    - In neural networks and data analysis, understanding these directions helps in reducing dimensionality, denoising data, and interpreting the underlying structure of datasets.\n",
    "    - For the given matrix $M$, most of the \"action\" or information is along the $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ direction, so focusing on this can simplify analysis without losing much information.\n",
    "\n",
    "**Summary:**  \n",
    "- Eigenvalues: $1$ and $3$  \n",
    "- Eigenvectors: $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "## Real-Life Example: Using These Eigenvalues and Eigenvectors\n",
    "\n",
    "Suppose you are analyzing a simple network of two sensors measuring temperature and humidity in a room. The readings are correlated, and you want to understand the main directions of variation in your data—this is a classic use case for Principal Component Analysis (PCA).\n",
    "\n",
    "Given the covariance matrix:\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The eigenvalues ($1$ and $3$) tell you the amount of variance captured along each principal direction.\n",
    "- The eigenvectors ($\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$) give you the directions in feature space.\n",
    "\n",
    "**How to use them:**\n",
    "- The direction $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ (largest eigenvalue $3$) is the axis along which the data varies most—combining temperature and humidity together.\n",
    "- The direction $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ (smaller eigenvalue $1$) is the axis along which the data varies least—contrasting temperature and humidity.\n",
    "\n",
    "You can project your sensor data onto these axes to reduce dimensionality or to visualize the main trends, helping you identify patterns or anomalies in the room’s climate.\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Concept         | Neural Network Role                      |\n",
    "|-----------------|------------------------------------------|\n",
    "| Vector          | Input, output, weights                   |\n",
    "| Matrix          | Layer weights, data batches              |\n",
    "| Dot Product     | Weighted sum in neurons                  |\n",
    "| Matrix Multiply | Layer transformations                    |\n",
    "| Transpose       | Aligning dimensions for operations       |\n",
    "| Determinant     | Invertibility, transformations           |\n",
    "| Eigenvalues/Eigenvectors | Stability, dynamics analysis         |\n",
    "| Element-wise    | Activation functions                     |\n",
    "\n",
    "A solid grasp of these linear algebra concepts will make it easier to understand and implement neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0dba6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
